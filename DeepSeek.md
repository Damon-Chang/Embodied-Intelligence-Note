# DeekSeek系列

- [DeepSeek-V3](#deepseek-v3)
  - [介绍](#介绍)
  - [精读](#精读)
    - [结构](#1结构)
    - [设备](#2设备)
    - [预训练](#3预训练)
    - [后训练](#4后训练)
- [DeepSeek R1](#deepseek-r1)

## DeepSeek-V3 
<a href="#deekseek系列">🔝</a>

---
### 介绍 
<a href="#deekseek系列">🔝</a>
- **名称**：DeepSeek-V3
- **类型**：Mixture-of-Experts（MoE）大语言模型。Mixture-of-Experts (MoE) language model，即混合专家语言模型，是一种先进的语言模型架构，通过整合多个专家模块提升性能，近年来在自然语言处理领域备受关注。
- **参数量**：671B，每个token激活37B的参数。
- **组件**
  - **MLA**：Multi-head Latend Attention；
  - **DeepSeekMoE**：DeepSeek专门开发的混合专家模型；
- **创新**
  - 首创一种**无辅助损失策略**（auxiliary-loss-free strateg）来应对负载均衡问题（load balancing）；
  - 设定**多词元训练目标**（ multi-token prediction training
objective），以实现更好的性能
- **训练资源**
  - **数据**：1.48万亿（14.8trillion）个多样且高质量的词元上对DeepSeek-V3进行预训练；
  - 随后历经**监督微调**与**强化学习**阶段，以充分发挥其性能。
- **表现**
  - 全面评估显示，DeepSeek-V3的表现超越了其他开源模型，达到了与领先的闭源模型相当的水平。
  - DeepSeek-V3完成全部训练仅需**278.8万个H800 GPU小时**；
  - 其训练过程极为稳定。在整个训练过程中，没有遇到任何无法挽回的**损失激增**（loss spikes）情况，也未进行任何**回滚**（rollbacks）操作。
![alt text](<截屏2025-02-09 12.07.46.png>)

#### 1.Model Summary
---
**结构：创新了负载均衡策略（Load Balancing Strategy）和训练目标（Training Objective）**
- 在DeepSeek-V2的高效架构之上，V3开创了负载均衡的无辅助损失策略，该策略最大限度地减少了因鼓励负载平衡而导致的性能下降；
- V3研究了一个多词元预测（Multi-TokenPrediction，MTP）目标并证明它有利于模型性能。它也可以用于推理加速的推测解码。

---
**预训练：迈向终极训练效率**
- V3设计了一个FP8混合精度训练框架，并首次在超大规模模型上验证了FP8训练的可行性和有效性；
- 通过算法、框架和硬件的协同设计，V3克服了跨节点MoE训练中的通信瓶颈，几乎实现了全computation-communication重叠。这显着提高了模型训练效率并降低了训练成本，使得能够在没有额外开销的情况下进一步扩展模型大小；
- 以仅2.664M H800 GPU小时的经济成本，我们在14.8T tokens上完成了DeepSeek-V3的预训练，产生了目前最强的开源基础模型。预训练后的后续训练阶段只需要0.1M GPU小时。

---
**后训练：DeepSeek-R1的知识提炼**
- V3引入了一种创新的方法论，将long-Chain-of-Thought（CoT）模型，特别是DeepSeek R1系列模型之一的推理能力提取到标准LLM中，尤其是DeepSeek-V3。我们的管道将R1的验证和反射模式优雅地整合到DeepSeek-V3中，并显着提高了其推理性能。同时，我们还保持对DeepSeek-V3输出样式和长度的控制。

---
#### 2.模型下载
<div align="center">

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |
| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |

</div>

> [!NOTE]
> Hugging Face上DeepSeek-V3模型的总大小685B，包括主模型权重的671B和多词元预测（MTP）模块权重的14B。

#### 3.交互网站 & API平台
与 DeepSeek-V3 聊天在DeepSeek的[官方网址](chat.deepseek.com)

提供了OpenAI-Compatible API 在 DeepSeek 的[平台](platform.deepseek.com)

### 精读 <a href="#deekseek系列">🔝</a>
考虑到强大的模型性能和经济成本，DeepSeek-V3模型以前瞻性的视角，沿用**Multi-head Latend Attention（MLA）**实现高效推理，使用**DeepSeekMoE**实现经济高效的训练。这两种架构已在[DeepSeek-V2](https://github.com/deepseek-ai/DeepSeek-V2?utm_source=catalyzex.com)中得到验证，展示了它们在保持稳健模型性能的同时实现高效训练和推理的能力。
除了这两个基本结构，两个额外的策略被用于增强模型性能。DeepSeek-V3为负载平衡开创了一种**无辅助损失策略**，目的是最大限度地减少因鼓励负载平衡而对模型性能产生的不利影响。其次，DeepSeek-V3采用了**多token预测训练目标**，我们已经观察到它可以提高评估基准的整体性能。
为了高效训练，V3使用FP8混合精度训练，并全面优化训练框架。
> **低精度**训练已经成为高效训练的有希望的解决方案，其演变与硬件能力的进步密切相关。V3引入了FP8混合精读训练框架，并首次在超大规模模型上验证了其有效性。
> - 支持FP8计算和存储，实现了**加速训练**和**减少GPU内存使用**；
> - 对于训练框架，设计了**DualPipe算法**以实现高效的管道并行性，该算法具有更少的管道气泡，并通过**computation-communication重叠**隐藏了训练期间的大部分通信。
> 
这种重叠确保了随着模型的扩大，只要我们保持恒定的computation-to-communication比，我们仍然可以跨节点使用细粒度的专家（fine-grained experts），同时实现接近零的all-to-all全通信开销。
此外，V3还开发了高效的**跨节点all-to-all通信内核**，以充分利用InfiniBand（IB）和NVLink带宽。
此外，V3精心优化了内存赵勇，使得在不使用昂贵的张量并行性的情况下训练DeepSeek-V3成为了可能，结合这些努力，V3实现了高效训练效率。
> [!NOTE]
> 看下来DeepSeek确实花了很多心思在如何用有限的算力实现大模型训练上，无论是模型的结构上还是训练架构上，都做了很多优化，还有创新。这样的模型感觉起来确实更加智能。好厉害！

---

**1.预训练阶段：**
- 14.8T高质量多样的词元上进行训练，训练过程十分稳定，没有损失激增和回滚操作。

**2.两阶段的DeepSeek-V3上下文长度扩展**
- 第一阶段：最大上下文长度扩展到32K；
- 第二阶段：最大上下文长度扩展到128K。

**3.后训练阶段：**
- **有监督微调（Supervised Fine-Tuning，SFT）**；
- **强化学习（Reinforcement Learning，RL）**；
- 将DeepSeek-V3与人类喜好对齐，并进一步释放其潜力。

在后训练阶段，从DeepSeek-R1系列模型中提取推理能力，同时小心保持模型精度和生成长度之间的平衡。

![alt text](<截屏2025-02-10 12.32.16.png>)

在预训练阶段，在每万亿个令牌上训练 DeepSeek-V3 只需要 180KH800 GPU 小时，即在我们拥有 2048 个 H800 GPU 的集群上训练 3.7 天。因此，我们的预训练阶段在**不到两个月**的时间内完成，花费 2664KGPU 小时。加上上下文长度扩展的 119KGPU 小时和训练后的 5KGPU 小时，DeepSeek-V3 的完整训练仅花费 2.788MGPU 小时。假设 H800 GPU 的租赁价格为每 GPU 小时 2 美元，我们的总训练成本仅为 557.60 万美元。请注意，上述成本仅包括 DeepSeek-V3 的官方训练，不包括与架构、算法或数据的先前研究和消融实验相关的成本。
> 也蛮烧钱的，不过如此大模型在两个月的时间内训练完成，非常了不起！👍

---
#### 主要贡献
**结构：创新负载均衡策略和训练目标**
- **负载均衡策略**：在DeepSeek-V2的高效架构之上，开创了负载平衡的无辅助损失策略，该策略最大限度地减少了因鼓励负载平衡而导致的性能下降。
- **训练目标**：研究了多令牌预测（MTP）目标，并证明它有利于模型性能。它也可用于推理加速的推测解码。
- 
**预培训：迈向终极培训效率**
- V3设计了一个FP8混合精度训练框架，并首次在超大规模模型上验证了FP8训练的可行性和有效性；
- 通过算法、框架和硬件的协同设计，我们克服了跨节点MoE训练中的通信瓶颈，实现了近乎完全的计算-通信重叠。这显着提高了我们的训练效率，降低了训练成本，使我们能够在没有额外开销的情况下进一步扩大模型大小；
- 我们以仅2.664M H800 GPU小时的经济成本完成了14.8T令牌上DeepSeek-V3的预训练，产生了目前最强大的开源基础模型。预训练后的后续训练阶段只需要0.1MGPU小时。

**后训练：DeepSeek-R1的知识提炼**
- 引入了一种创新的方法论，将长思维链（long-Chain-of-Thought，CoT）模型，特别是DeepSeek R1系列模型之一的推理能力提炼到标准LLM中，尤其是DeepSeek-V3。我们的管道优雅地将R1的验证和反射模型结合到的V3中，显着提高其推理性能。同时，我们还保持对DeepSeek-V3输出样式和长度的控制。

**核心评估结果摘要**
- **知识：**
  1. 在MMLU、MMLU-Pro和GPQA等教育基准上，DeepSeek-V3的表现优于所有其他开源模型，在MMLU上达到88.5，在MMLU-Pro上达到75.9，在GPQA上达到59.1。它的性能可与GPT-4o和Claude-Sonnet-3.5等领先的闭源模型相媲美，缩小了该领域的开源和闭源模型之间的差距；
  2. 对于事实性基准，DeepSeek-V3在SimpleQA和中国SimpleQA的开源模型中都表现出卓越的性能。虽然它在英语事实知识（SimpleQA）方面落后于GPT-4o和Claude-Sonnet-3.5，但它在中文事实知识（中文SimpleQA）方面超越了这些模型，突出了其在中文事实知识方面的优势。

- **代码、数学和推理：**
  1. 在所有非长CoT开源和闭源模型中，DeepSeek-V3在数学相关基准测试上实现了最先进的性能。值得注意的是，它甚至在特定基准测试（如MATH-500）上的性能优于o1-preview，展示了其强大的数学推理能力；
  2. 在编码相关任务上，DeepSeek-V3成为编码竞争基准测试（如LiveCodeBench）的最佳模型，巩固了其在该领域的领先模型地位。对于工程相关任务，尽管DeepSeek-V3的性能略低于Claude-Sonnet-3.5，但它仍然远远超过所有其他模型，展示了其在各种技术基准测试中的竞争力。<a href="#deekseek系列">🔝</a>

---
#### 1.结构
DeepSeek-V3基本的结构特点是：多头潜在注意力（Multi-head Attention，MLA）用于高效推理，DeepSeekMoE用于高效经济训练。多令牌预测（Multi-Token Prediction，MTP）训练目标，我们观察到它可以提高评估基准的整体性能。对于其他未明确提及的小细节，DeepSeek-V3遵循DeepSeek-V2（DeepSeek-AI，2024c）的设置。
##### 1.1 基础结构
DeepSeel-V3的基础结构仍在Transformer结构内。为了高效推理和经济训练采用了V2中的MLA和DeepSeekMoE。与V2相比为DeepSeek-V3的DeepSeekMoE新增了无辅助损失的负载均衡（auxiliary-loss-free load balancing）策略，减轻由确保负载平衡的努力引起的性能下降。
![img](https://github.com/Damon-Chang/Embodied-Intelligence-Note/blob/main/figures/%E6%88%AA%E5%B1%8F2025-02-10%2014.19.30.png)
![alt text](<截屏2025-02-10 14.19.30.png>)

###### 1.1.1 Multi-Head Latend Attention，MLA

对于注意力，DeepSeek-V3采用了MLA架构。$d$让表示嵌入维度，$n_h$表示注意力头的数量，$d_h$表示每个头的维度，$h_t\in \mathbb{R}^d$表示给定注意力层第个标记的注意力输入。MLA的核心是**对注意力键和值进行低秩联合压缩，以减少推理过程中的Key-Value（KV）缓存**：

$$
\begin{align*}
\boxed{\mathbf{c}_t^{KV}} &= W^{DKV}\mathbf{h}_t, \\
[\mathbf{k}_ {t,1}^{C}; \mathbf{k}_{t,2}^{C};\cdots;\mathbf{k}_{t,n_h}^{C}] &= \mathbf{k}_t^{C} = W^{UK}\mathbf{c}_t^{KV}, \\
\boxed{\mathbf{k}_ t^{R}} &= \text{RoPE}(W^{KR}\mathbf{h}_ t), \\ 
\mathbf{k}_{t,i} &= [\mathbf{k}_{t,i}^{C};\mathbf{k}_t^{R}], \\
[\mathbf{v}_{t,1}^{C};\mathbf{v}_{t,2}^{C};\cdots;\mathbf{v}_{t,n_h}^{C}] &= \mathbf{v}_t^{C} = W^{UV}\mathbf{c}_t^{KV},
\end{align*}
$$

$$
\begin{align*}
\mathbf{c}t^{KV} &= W^{DKV}\mathbf{h}t, \
[\mathbf{k}{t,1}^{C}; \mathbf{k}{t,2}^{C};\cdots;\mathbf{k}_{t,n_h}^{C}] &= \mathbf{k}t^{C} = W^{UK}\mathbf{c}t^{KV}, \
\mathbf{k}t^{R} &= \text{RoPE}(W^{KR}\mathbf{h}t), \
\mathbf{k}{t,i} &= [\mathbf{k}{t,i}^{C};\mathbf{k}t^{R}], \
[\mathbf{v}{t,1}^{C};\mathbf{v}{t,2}^{C};\cdots;\mathbf{v}{t,n_h}^{C}] &= \mathbf{v}_t^{C} = W^{UV}\mathbf{c}_t^{KV},
\end{align*}
$$

$c_t^{KV}\in \mathbb{R}^{d_c}$是为键和值准备的压缩潜在向量，$d_c(\ll d_h n_h)$表示KV压缩维度；$W^{DKV}\in \mathbb{R}^{d_c\times d}$表示下投影矩阵;$W^{UK},W^{UV}\in \mathbb{R}^{d_hn_h\times d_c}$分别表示键和值的上投影矩阵；$W^{KR}\in \mathbb{R}^{d_h^R\times d}$表示用于产生携带旋转位置潜入（ Rotary Positional Embedding，RoPE）解藕键的旋转矩阵，$RoPE(\cdot)$表示应用RoPE矩阵的操作；$[\cdot;\cdot]$表示拼接。

> [!NOTE]
> 对于MLA，只有框住的向量（$c_t^{KV},k_t^R$）在迭代时需要缓存，这使得MLA在保持和标准的多头注意力（Multi-Head Attention，MHA）相同表现的同时极大地降低了KV缓存。

对于注意力查询，也执行低秩压缩，这可以减少训练期间的激活记忆：

$$
\begin{align*}
\mathbf{c}_t^Q &= W^{DQ}\mathbf{h}_t, \\
[\mathbf{q}_{t,1}^C;\mathbf{q}_{t,2}^C;\cdots;\mathbf{q}_{t,n_h}^C] &= \mathbf{q}_t^C = W^{UQ}\mathbf{c}_t^Q, \\
[\mathbf{q}_{t,1}^R;\mathbf{q}_{t,2}^R;\cdots;\mathbf{q}_{t,n_h}^R] &= \mathbf{q}_t^R = \text{RoPE}(W^{QR}\mathbf{c}_t^Q), \\
\mathbf{q}_{t,i} &= [\mathbf{q}_{t,i}^C;\mathbf{q}_{t,i}^R],
\end{align*}
$$
其中$c_t^Q\in \mathbb{R}^{d_c'}$表示查询的压缩潜在向量；$d_c'(\ll d_hn_h)$表示查询压缩维度；$W^{DQ}\in \mathbb{R}^{d_c'\times d},W^{UQ}\in\mathbb{R}^{d_hn_h\times d_c'}$分别是查询的下投影和上投影；$W^{RQ}\in\mathbb{R}^{d_h^Rn_h\times d_c'}$是产生携带RoPE的解藕查询的矩阵。

最后，注意力查询（$q_{t,i}$），键（$k_{t,i}$），值（$v_{t,i}$）结合产出最终的注意力输出$u_t$:
$$
\begin{align*}
\mathbf{o}_{t, i} &=\sum_{j = 1}^{t} \text{Softmax}_{j}\left(\frac{\mathbf{q}_{t, i}^{T} \mathbf{k}_{j, i}}{\sqrt{d_{h}+d_{h}^{R}}}\right) \mathbf{v}_{j, i}^{C}\\
\mathbf{u}_{t} &=W^{O}\left[\mathbf{o}_{t, 1} ; \mathbf{o}_{t, 2} ; \ldots ; \mathbf{o}_{t, n_{h}}\right]
\end{align*}
$$
其中$W^O\in\mathbb{R}^{d\times d_hn_h}$表示输出投影矩阵。

###### 1.1.2 无辅助损失负载均衡的DeepSeekMoE

**DeepSeekMoE的基本结构**。V3使用DeepSeekMoE作为前馈神经网络（Feed-Forward Networks，FNNs）。与传统MoE相比，DeepSeekMoE结构使用更细粒度（finer-grained）的专家，并将一些专家隔离为共享专家。令$u_t$表示FFN第t个输入，FFN输出$h_t'$为：
 $$
\begin{align*}
\mathbf{h}_{t}^{\prime}&=\mathbf{u}_{t}+\sum_{i = 1}^{N_{s}} \mathrm{FFN}_{i}^{(s)}\left(\mathbf{u}_{t}\right)+\sum_{i = 1}^{N_{r}} g_{i, t} \mathrm{FFN}_{i}^{(r)}\left(\mathbf{u}_{t}\right),\\
g_{i, t}&=\frac{g_{i, t}^{\prime}}{\sum_{j = 1}^{N_{r}} g_{j, t}^{\prime}},\\
g_{i, t}^{\prime}&= \begin{cases}s_{i, t}, & s_{i, t} \in \mathrm{Topk}(\{s_{j, t}|1 \leqslant j \leqslant N_{r}\}, K_{r}), \\ 0, & \text { otherwise, }\end{cases}\\
s_{i, t}&=\mathrm{Sigmoid}\left(\mathbf{u}_{t}^{T} \mathbf{e}_{i}\right),
\end{align*}
$$
其中$N_s,N_r$分别表示共享专家和路由专家的数量；$FFN_i^{(s)}(\cdot),FFN_i^{(r)}(\cdot)$分别表示第i个共享专家和路由专家。$K_r$表示激活的路由专家数量；$g_{i,t}$表示第i歌专家的门控值；$s_{i,t}$表示词元到专家（token-to-expert）的亲和度（affinity）；$e_i$表示第i个路由专家的质心向量；$TopK(\cdot,K)$表示第t个词元和所有路由专家计算的亲和力分数中最高分的集合。与DeepSeek-V2略有不同，DeepSeek-V3使用sigmoid函数来计算亲和力分数，并在所有选定的亲和力分数中应用规范化来生成门控值。

**无辅助损失负载均衡**。对于MoE模型，不平衡的专家加载将导致路由崩溃并降低专家并行性场景中的计算效率。常规解决方案通常依赖辅助损失来避免不平衡负载。但太大的辅助损失会影响模型性能。为了在负载平衡和模型性能之间实现更好的权衡，V3开创了一种无辅助损失的负载均衡策略。具体来说，为每个专家引进了一个偏执项$b_i$，并将其添加到相应的亲和力分数$s_{i,t}$中，以确定top-K路由。

$$
g_{i,t}'=
\begin{cases}
s_{i,t}, & s_{i,t} + b_i \in \mathrm{Topk}(\{s_{j,t} + b_j|1 \leqslant j \leqslant N_r\}, K_r), \\
0, & \text{otherwise}.
\end{cases}
$$

注意，偏执项仅应用在路由。门控值将与FFN输出相乘，仍然来自原始亲和力分数$s_{i,t}$。在训练期间不断监控每个训练步骤的整个批次上的专家负载。在每个步骤结束时，如果其对应专家负载过重，将偏执项减少$\gamma$；如果对应的专家负载不足，将偏执项增加$\gamma$，其中$\gamma$是一个被称为偏执更新速度的超参数。通过上述的动态调整，DeepSeek-V3在训练中保持了平衡的专家负载，并且比只使用额外损失的模型实现了更好的性能。

**互补时序辅助损失**。Complementary Sequence-Wise Auxiliary Loss。尽管DeepSeek-V3的负载均衡依靠无辅助损失策略。为了防止任何单个序列中的极端不平衡，V3还采用了互补的序列平衡损失。

$$
\begin{align*}
\mathcal{L}_{\text{Bal}}&=\alpha\sum_{i = 1}^{N_r}f_iP_i,\\
f_i&=\frac{N_r}{K_rT}\sum_{t = 1}^{T}\mathbb{1}(s_{i,t}\in\mathrm{Topk}(\{s_{j,t}|1\leqslant j\leqslant N_r\},K_r)),\\
s_{i,t}'&=\frac{s_{i,t}}{\sum_{j = 1}^{N_r}s_{j,t}},\\
P_i&=\frac{1}{T}\sum_{t = 1}^{T}s_{i,t}'
\end{align*}
$$

其中平衡因子$\alpha$是超参数，在V3中将被分配给一个很小的值；$\mathbb{1}(\cdot)$表示指标函数；$T$表示一个序列中的token数量。序列平衡损失鼓励平衡每个序列上的专家加载平衡。

**节点限制路由**。Node-limited Routing。与 DeepSeek-V2 所使用的设备受限路由类似，DeepSeek-V3 也采用受限路由机制来限制训练期间的通信成本。简而言之，确保每个token最多被发送到M个节点，这些节点是根据每个节点上分布的专家的最高 $\frac{K_r}{M}$ 亲和度得分之和来选择的。在这个约束下，我们的 MoE 训练框架可以几乎实现完全的计算-通信重叠。

**无token丢弃**。No Token-Dropping。由于有效的负载平衡策略，DeepSeek-V3在其充分训练期间保持了良好的负载平衡，因此，DeepSeek-V3在训练期间不会丢弃任何token，此外，我们还实施了特定的部署策略来确保推理负载平衡，因此DeepSeek-V3在推理期间也不会丢弃token。

##### 1.2 多词元预测
Multi-Token Prediction。受Gloeckle等人（2024）的启发，V3研究并为DeepSeek-V3设置了一个多令牌预测（MTP）目标，该目标将预测范围扩展到每个位置的多个未来令牌。一方面，MTP目标使训练信号致密化，并可能提高数据效率。另一方面，MTP可能使模型能够预先规划其表示，以便更好地预测未来令牌。下图说明了我们对MTP的实现。与Gloeckle等人（2024）不同，后者使用独立的输出头并行预测附加令牌，我们顺序预测附加令牌，并在每个预测深度保持完整的因果链。我们在本节中介绍我们的MTP实现的细节。


![我们的多令牌预测（MTP）实施的插图。我们为每个深度的每个令牌的预测保留完整的因果链](<截屏2025-02-12 09.12.45.png>)

**MTP模块**。具体来说，MTP实现使用D个序列模块预测D个额外的词元。第k个MTP模块包括一个共享的的嵌入层$Emb(\cdot)$，一个共享的输出头$OutHead(\cdot)$，一个Transformer块$TRM_k(\cdot)$，一个投影矩阵$M_k\in \mathbb{R}^{d\times 2d}$。对于第i个输入词元$t_i$，在第k个预测深度，首先将第k-1深度的第i个token的表示连接$ h_i^{k-1}\in\mathbb{R}^d $与第i+k个token的嵌入$Emb(t_{i+k})\in\mathbb{R}^d$通过线性投影进行组合。

$$ h_{i}^{\prime k}=M_{k}\left[RMSNorm\left(h_{i}^{k - 1}\right); RMSNorm\left(Emb\left(t_{i + k}\right)\right)\right] $$

> 其中，$h_{k-1}$是一个d维向量，$Emb(t_{i+k})$也是一个d维向量，d表示向量的维度，i和k是整数。这种组合方式可能是为了在不同的预测深度下更好利用输入标记的信息，从而提高预测的准确性。

其中$ [\cdot;\cdot] $表示向量拼接，$RMSNorm(\cdot)$表示根均方根归一化，$ M_k $是一个d×2d的矩阵，表示线性投影。特别地，k=1时，$h_i^{k-1}$表示由主模型得到的表示。请注意，对于每个MTP模块，其嵌入层与主模型共享。$h_i^{'k}$作为第𝑘深度的 Transformer 模块的输入，用于生成当前深度$h_i^{k}$的输出表示：

$$h^k_{1:T;k}=TRM_k(h^{'k}_{1:T-k})$$

其中T表示输入序列的长度，$i;j$表示切片操作（包含左右边界）。最后，以$h^k_i$作为输入，共享输出头将计算第k个额外预测的词元的概率分布$P_{i+1+k}^k\in\mathbb{R}^V$，其中V是词表大小：

$$P_{i+1+k}=OutHead(h_i^k).$$

输出头 OutHead(·) 将表示线性映射为对数几率，然后应用 Softmax(·) 函数来计算第k个附加词元的预测概率。此外，对于每个 MTP 模块，其输出头与主模型共享。

**MTP Training Objective**。对于每个预测深度，计算交叉熵损失$\mathcal{L}^k_{MTP}$:

$$\mathcal{L}_{\mathrm{MTP}}^{k}=\mathrm{CrossEntropy}(P_{2 + k:T+1}^{k}, t_{2 + k:T+1})=-\frac{1}{T}\sum_{i = 2 + k}^{T + 1}\log P_{i}^{k}[t_{i}],
$$

其中T表示输入序列向量，$t_i$表示第i个位置的真实值词元，$P^k_i[t_i]$表示给定MTP模块后$t_i$相应的预测概率。最后得到求所有深度的MTP目标之和，再乘一个系数得到DeepSeek-V3额外的损失：

$$\mathcal{L}_{MTP} = \frac{\lambda}{D}\sum_{k=1}^D\mathcal{L}_{\mathrm{MTP}}^{k}$$

**MTP在推理**。MTP主要是提高了主模型的性能，因此在推理阶段可以直接将这一模块丢弃。主模块可以独立正常运行。此外，我们还可以将这些MTP模块重新用于推测解码，以进一步提高生成延迟。

#### 2.设备

> [!NOTE] 📖
> 大模型的成功与否很大程度上取决于训练设备的性能和配置。以及如何利用计算机群分配资源高效训练。这是我完全不了解的，高校里似乎很难有这样的条件进行GPU集群的使用吧。 

##### 2.1.计算集群

DeepSeek-V3在配备2048个NVIDIA H800 GPU的集群上进行训练。H800集群中的每个节点都包含8个GPU，由节点内的NVLink和NVSwitch连接。跨不同节点，InfiniBand（IB）互连用于促进通信。

##### 2.2 训练框架

DeepSeek-V3的训练由**HAI-LLM**框架支持，这是一个由我们的工程师从头开始构建的高效轻量级训练框架。总的来说，DeepSeek-V3应用了16路**管道并行（Pipeline Parallelism,PP）**（Qi et al.，2023a）、跨越8个节点的64路**专家并行（Experts Parallelism， EP）**（Lepikhin et al.，2021）和ZeRO-1**数据并行（Data Parallelism，DP）**（Rajb-handari et al.，2020）。
为了促进DeepSeek-V3的高效训练，我们实施了细致的工程优化。首先，我们设计了**DualPipe算法**以实现高效的管道并行性。与现有的PP（pipeline parallelism）方法相比，DualPipe的管道气泡更少。更重要的是，它在前向和后向进程之间重叠了计算和通信阶段，从而解决了跨节点专家并行性引入的沉重通信开销的挑战。其次，我们开发了高效的跨节点所有对所有通信内核，以充分利用IB和NVLink带宽，并节省专用于通信的**流式多处理器（Streaming Multiprocessors，SM）**。最后，我们在训练期间精心优化了内存占用，从而使我们能够在不使用昂贵的**张量并行性（Tensor Parallelism，TP）**的情况下训练DeepSeek-V3。

###### 2.2.1 DualPipe和Computation-Communication重叠
对于DeepSeek-V3，跨节点专家并行引入的通信开销导致约1:1的低效computation-to-communication比。为了应对这一挑战，我们设计了一种名为DualPipe的创新流水线并行算法，它不仅通过有效重叠前向和后向计算-通信阶段来加速模型训练，还减少了流水线气泡。
DualPipe的核心思想是将计算和通信重叠在一对单独的向前和向后块中。具体来说，我们将每个块分为**四个组成部分**：注意力、所有对所有调度、MLP和所有对所有组合（attention, all-to-all dispatch, MLP, and all-to-all combine）。特别是，对于向后块，注意力和MLP进一步分为**两部分**，向后用于输入，向后用于权重（backward for input and backward for weights），如ZeroBubble（Qi et al.，2023b）。此外，我们还有一个PP通信组件，如下图所示，对于一对向前和向后的块，我们重新排列这些组件，并手动调整专用于通信与计算的GPU SM的比例。

![一对单独的向前和向后块的重叠策略（转换器块的边界不对齐）。橙色表示向前，绿色表示“向后输入”，蓝色表示“向后权重”，紫色表示PP通信，红色表示障碍。所有对所有和PP通信都可以完全隐藏。](<截屏2025-02-13 19.58.18.png>)

在这种重叠策略中，我们可以确保all-to-all和PP通信在执行期间都可以完全隐藏。给定有效的重叠策略，完整的DualPipe调度如下图所示。
![示例DualPipe调度用于两个方向的8个PP等级和20个微批次。反向方向的微批次与正向方向的微批次是对称的，因此为了说明简单起见，我们省略了它们的批次ID。由共享黑边包围的两个单元相互重叠计算和通信。](<截屏2025-02-13 20.26.30.png>)
它采用双向流水线调度，从流水线两端同时输送微批次，并且很大一部分通信可以完全重叠。这种重叠还确保了，随着模型进一步扩展，只要我们保持恒定的computation-to-communication比，我们仍然可以跨节点雇用细粒度的专家，同时实现近乎零的所有对所有通信开销。
此外，即使在没有沉重通信负担的更一般场景中，DualPipe 仍然展现出效率优势。在表下表中，我们总结了不同并行处理（PP）方法的流水线气泡和内存使用情况。如下表所示，与 ZB1P（Qi 等人，2023b）和 1F1B（Harlap 等人，2018）相比，DualPipe 显著减少了流水线气泡，同时仅将峰值激活内存增加了$\frac{1}{pp}$倍。
![不同流水线并行方法中流水线气泡和内存使用情况的比较。𝐹表示前向块的执行时间，𝐵表示完整反向块的执行时间，𝑊表示“用于权重的反向”块的执行时间，𝐹&𝐵表示两个相互重叠的前向和反向块的执行时间。](<截屏2025-02-13 20.32.07.png>)

虽然DualPipe需要保留模型参数的两个副本，这不会显著增加内存消耗，因为我们在训练时使用了大EP大小。与Chimera（Li和Hoefler，2021）相比，DualPipe只要求管道级和微批次可被2整除，而不要求微批次可被管道级整除。此外，对于DualPipe，气泡和激活内存都不会随着微批次数量的增长而增加。

###### 2.2.2 跨节点全对全通信的高效实现
为了保证DualPipe足够的计算性能，我们定制了高效的跨节点全对全通信内核（包括调度和合并），以节省专用于通信的SM数量。内核的实现与我们集群的MoE门控算法和网络拓扑共同设计。具体来说，在我们的集群中，跨节点GPU与IB完全互联，节点内通信通过NVLink处理。NVLink 提供 160GB/s 的带宽，大约是 IB（50GB/s）的 3.2 倍。为了有效地利用 IB 和 NVLink 不同的带宽，我们将每个令牌限制为最多分发到 4 个节点，从而减少 IB 流量。为了有效地利用无限带宽（InfiniBand，IB）和英伟达高速互联（NVLink）的不同带宽，我们将每个token限制为最多分发到 4 个节点，从而减少 IB 流量。对于每个令牌，当确定其路由决策时，它将首先通过 IB 传输到目标节点上具有相同节点内索引的图形处理器（GPU）。一旦到达目标节点，我们将努力确保它立即通过 NVLink 转发到承载其目标专家的特定 GPU，而不会被随后到达的token阻塞。通过这种方式，通过IB和NVLink的通信完全重叠，每个令牌可以有效地选择每个节点平均3.2个专家，而不会产生NVLink的额外开销。这意味着，尽管DeepSeek-V3在实践中只选择8个路由专家，它可以在保持相同通信成本的同时将这个数量扩展到**最多13个专家**（4个节点×3.2个专家/节点）。总体而言，在这样的通信策略下，仅需20个SM就足以充分利用IB和NVLink的带宽。
详细来说，我们采用warp专业化技术（Bauer等人，2014），将20个SM划分为10个通信通道。在调度过程中，（1）IB发送、（2）IB-to-NVLink转发和（3）NVLink接收由各自的warp处理。分配给每个通信任务的warp数量根据跨所有SM的实际工作负载进行动态调整。同样，在合并过程中，（1）NVLink发送、（2）NVLink to-IB转发和累积以及（3）IB接收和累积也由动态调整的warp处理。此外，调度和合并内核都与计算流重叠，因此我们也考虑了它们对其他SM计算内核的影响。具体来说，我们采用定制的PTX（并行线程执行）指令并自动调整通信块大小，从而显着减少了L2缓存的使用和对其他SM的干扰。

> ❕上面这些就是GPU集群训练中的术语了，我基本是看不懂的，不过看起来是在使用一些策略来尽可能的减少GPU的时间消耗更加高效地训练目标🎯。


###### 2.2.3 极大地节省内存，开销最小

为了减少训练期间的内存占用，V3采用以下技术。

**RMSNorm和MLA上投影的重新计算**。我们在反向传播期间重新计算所有RMSNorm操作和MLA上投影，从而消除了持续存储其输出激活的需要。这种策略的开销很小，大大降低了存储激活的内存需求。

**CPU中的指数移动平均线**。在训练期间，我们保留模型参数的指数移动平均线（Exponential Moving Average，EMA），以便在学习率衰减后早期估计模型性能。EMA参数存储在CPU内存中，并在每个训练步骤后异步更新。这种方法允许我们在不产生额外内存或时间开销的情况下维护EMA参数。

**用于多令牌预测的共享嵌入和输出头**。使用DualPipe策略，我们将模型的最浅层（包括嵌入层）和最深层（包括输出头）部署在同一PP等级上。这种安排使得MTP模块和主模型之间能够物理共享参数和梯度，共享嵌入和输出头。这种物理共享机制进一步提高了我们的内存效率。

##### 2.3 FP8训练

受低精度训练最新进展的启发（Dettmers et al.，2022； Noune et al.，2022；彭et al.，2023b），我们提出了一种利用**FP8数据格式**训练DeepSeek-V3的细粒度混合精度框架。虽然低精度训练有很大的希望，但它通常受到激活、权重和梯度中异常值的存在的限制（Fishman et al.，2024；He et al.；Sun et al.，2024）。尽管在推理量化方面取得了重大进展（Frantar et al.，2022；肖et al.，2023），但证明低精度技术在大规模语言模型预训练中成功应用的研究相对较少。为应对这一挑战并有效扩展FP8格式的动态范围，我们引入了一种细粒度量化策略：按1×$N_c$个元素进行分块分组，或按$N_c$×$N_c$个元素进行块式分组。

> 在深度学习模型训练中，低精度训练面临诸多挑战，FP8格式虽有优势，但因动态范围受限，对激活值、权重和梯度中的异常值敏感。为解决该问题，提升FP8训练的有效性，文中提出了这种细粒度量化策略。按1×$N_c$个元素分块分组（即每个token的每128个通道一组）或按$N_c$×$N_c$个元素块式分组（即每128个输入通道和每128个输出通道为一组），能根据较小的元素组调整缩放比例，更好地适应异常值，在不损失太多精度的情况下扩展FP8格式的动态范围，进而提高低精度训练的准确性。 

在我们提高精度的累加过程中，相关的反量化开销在很大程度上得到了缓解，这是实现精确的 FP8 **通用矩阵乘法**（General Matrix Multiplication, GEMM）的一个关键方面。此外，为了进一步减少在混合专家（MoE）训练中的内存和通信开销，我们以 FP8 格式缓存并分发激活值，同时将低精度优化器状态存储在 BF16 格式中。我们在与 DeepSeek-V2-Lite 和 DeepSeek-V2 类似的两种模型规模上验证了所提出的 FP8 混合精度框架，对大约 1 万亿个token进行训练（更多细节见附录 B.1）。值得注意的是，与 BF16 基线相比，我们的 FP8 训练模型的相对损失误差始终保持在 0.25%以下，这一水平完全在训练随机性的可接受范围内。

###### 2.3.1 混合精读框架
基于低精度训练中广泛采用的技术（Kalamkar et al.，2019； Narang et al.，2017），我们提出了一种用于FP8训练的混合精度框架。在这个框架中，大多数计算密度操作在FP8中进行，而一些关键操作战略性地保持在原始数据格式中，以平衡训练效率和数值稳定性。整体框架如下图所示。
![具有FP8数据格式的整体混合精度框架。为了澄清，仅说明线性运算符。](<截屏2025-02-14 08.52.53.png>)

首先，为了加速模型训练，大多数核心计算内核，即 GEMM 操作，以 FP8 精度实现。这些 GEMM 操作接受 FP8 张量作为输入，并产生 BF16 或 FP32 的输出。如上图所示，**与线性算子相关的所有三个 GEMM，即前向传播（Fprop）、激活反向传播（Dgrad）和权重反向传播（Wgrad）**，都在 FP8 中执行。这种设计理论上比原始的 BF16 方法使计算速度加倍。此外，FP8 的 Wgrad GEMM 允许激活以 FP8 存储，以便在反向传播中使用。这显著减少了内存消耗。
尽管FP8格式具有效率优势，但由于某些算子对低精度计算的敏感性，它们仍然需要更高的精度。此外，一些低成本算子也可以利用更高的精度，而对整体训练成本的开销可以忽略不计。因此，经过仔细调查，我们**对以下组件保持原始精度（例如BF16或FP32）：嵌入模块（embedding modules）、输出头（output head）、MoE门控模块（MoE gating modules）、归一化算子（normalization operators）和注意力算子（attention operators）**。这些高精度的有针对性的保留确保了DeepSeek-V3的稳定训练动态。为了进一步保证数值稳定性，我们以更高的精度存储主权重、权重梯度和优化器状态。因为这些高精度组件会产生一些内存开销，它们的影响可以通过在我们的分布式训练系统中跨多个数据并行（DP）等级进行有效分片来最小化。

###### 2.3.2 从量化和乘法中提高精度
基于我们的混合精度FP8框架，我们介绍了几种提高低精度训练精度的策略，重点关注量化方法和乘法过程。
**细粒度量化**。Fine-Grained Quantization. 在低精度训练框架中，由于FP8格式的有限动态范围（受其缩减的指数位的约束），溢出和下溢是常见的挑战。作为标准实践，通过将输入张量的最大绝对值缩放到FP8的最大可表示值，将输入分布与FP8格式的可表示范围对齐（Narang et al.，2017）。这种方法使得低精度训练对激活异常值高度敏感，这会严重降低量化精度。为了解决这个问题，我们提出了一种细粒度量化方法，该方法在更细粒度的水平上应用缩放。
如下图所示，（1）对于激活值，我们以 1×128 的分块为基础对元素进行分组和缩放（即每个 token 的每 128 个通道一组）；（2）对于权重，我们以 128×128 的块为基础对元素进行分组和缩放（即每 128 个输入通道和每 128 个输出通道一组）。这种方法通过根据较小的元素组来调整缩放比例，确保量化过程能够更好地处理异常值。在附录 B.2 中，我们进一步讨论了如果像对权重进行量化那样以块为基础对激活值进行分组和缩放时，会出现的训练不稳定性问题。![（a）我们提出了一种细粒度量化方法，以减轻由特征异常值引起的量化误差；为简化说明，仅展示了前向传播（Fprop）过程。（b）结合我们的量化策略，我们通过每隔$N_{C}=128$个元素的矩阵乘法累加（MMA）操作，将中间结果提升至CUDA核心进行高精度累加，以此提高FP8通用矩阵乘法（GEMM）的精度。](<截屏2025-02-14 09.17.44.png>)

> 在深度学习模型训练中，**量化(quantization)** 是将数据表示为低精度格式（如 FP8）以提高计算效率的常用方法。然而，FP8 格式的动态范围有限，异常值可能导致量化误差显著增加。上述方法通过细粒度的分组和缩放策略，分别针对激活值和权重采用不同的分组方式。对于激活值，以 1×128 的分块处理，考虑到每个 token 在 128 个通道上的特征，能更细致地处理不同 token 间的差异，适应可能出现的异常值。对于权重，128×128 的块分组方式则基于输入和输出通道的维度进行，同样有助于在量化时更好地应对异常值的影响。
在附录 B.2 中讨论的，如果对激活值也采用像权重那样的块式分组缩放，会引发训练不稳定的问题。这是因为激活值的分布和特性与权重不同，块式分组可能无法有效处理激活值中的异常值，导致训练过程中梯度计算不准确，进而影响模型训练的稳定性，如可能出现模型训练发散等情况 。



V3的方法中的一个关键修改是沿着GEMM操作的内部维度引入**每组缩放因子（per-group scaling factors）**。标准FP8 GEMM不直接支持此功能。但是，结合我们**精确的FP32累积策略**，它可以有效实施。
> **解释**
>
> 在深度学习训练中，尤其是在使用FP8数据格式进行通用矩阵乘法（GEMM）运算时，这两句话阐述了一种关键的改进方法及其实现方式。具体解释如下：
> - **方法核心改进**：在进行GEMM操作时，沿着其内部维度引入了每组的缩放因子。这一改进意义重大，因为传统的标准FP8 GEMM并不直接支持这样的功能。在低精度训练场景下，FP8格式的动态范围有限，对异常值较为敏感，容易导致量化误差。引入每组缩放因子，可以针对不同组的数据，根据其具体特征调整缩放比例，使量化过程能更好地适应数据的变化，有效减少异常值对量化精度的影响，从而提升整体训练的准确性。
> - **实现方式**：虽然标准FP8 GEMM不支持每组缩放因子，但借助精确的FP32累加策略，该功能可以高效实现。在实际计算中，当进行GEMM运算时，先按照FP8格式进行计算，在累加阶段利用FP32的高精度特性进行累加操作。在这个过程中，将每组缩放因子融入到计算流程中，利用FP32累加的高精度优势，在不显著增加计算复杂度的情况下，实现了每组缩放因子的功能，既保证了计算精度，又提升了计算效率，确保了训练过程的稳定性和准确性。 

**提高累计精度**。低精度通用矩阵乘法（GEMM）操作经常遭受下溢问题，其准确性在很大程度上取决于高精度累加，通常以 FP32 精度执行（Kalamkar 等人，2019；Narang 等人，2017）。然而，我们观察到 NVIDIA H800 GPU 上 FP8 GEMM 的累加精度被限制为保留大约 14 位，这明显低于 FP32 的累加精度。当内维度 K 较大时，这个问题会变得更加明显（Wortsman 等人，2023），这是大规模模型训练中的典型场景，其中批量大小和模型宽度增加。以 K = 4096 的两个随机矩阵的 GEMM 操作为例，在我们的初步测试中，张量核心中的有限累加精度导致近 2%的最大相对误差。尽管存在这些问题，有限的累加精度仍然是一些 FP8 框架中的默认选项（NVIDIA，2024b），严重制约了训练精度。
为了解决这个问题，我们采用了提升到 CUDA 核心以获得更高精度的策略（Thakkar 等人，2023 年）。这个过程如上图（b）所示。具体来说，在张量核心上进行矩阵乘法累加（Matrix Multiply-Accumulate, MMA）执行时，中间结果使用有限的位宽进行累加。一旦达到间隔\(𝑁_𝐶\)，这些部分结果将被复制到 CUDA 核心上的 FP32 寄存器中，在那里进行全精度 FP32 累加。如前所述，我们的细粒度量化沿着内部维度 K 应用每组缩放因子。这些缩放因子可以在 CUDA 核心上高效地相乘，作为反量化过程，只需最小的额外计算成本。
需要注意的是，这种修改降低了单个线程组的线程组级矩阵乘积累加（Warpgroup-level Matrix Multiply-Accumulate，WGMMA）指令发布率。然而，在 H800 架构上，通常有两个 WGMMA 同时存在：当一个线程组执行提升操作时，另一个能够执行 MMA 操作。这种设计使得这两个操作能够重叠，保持张量核心的高利用率。根据我们的实验，设置\(N_C = 128\)个元素，相当于 4 个 WGMMA，代表了能够在不引入大量开销的情况下显著提高精度的最小积累间隔。

**尾数除以指数**。Mantissa over Exponents。与之前的工作（NVIDIA，2024b；彭等人，2023b；Sun等人，2019b）采用的混合FP8格式相比，在Fprop中使用E4M3（4位指数和3位尾数），在Dgrad和Wgrad中使用E5M2（5位指数和2位尾数），我们在所有张量上采用**E4M3**格式以获得更高的精度。我们将这种方法的可行性归因于我们的**细粒度量化策略，即平铺和按块缩放**。通过对较小的元素组进行操作，我们的方法论有效地在这些分组元素之间共享指数位，从而减轻了有限动态范围的影响。
> “Mantissa over Exponents”可翻译为“尾数与指数”。这段文本主要讨论了一种不同于以往工作采用的混合 FP8 格式的方法，即所有张量都采用 E4M3 格式以获得更高精度。文中提到的“Mantissa over Exponents”可能是在强调尾数和指数在这种新的量化策略中的重要性。这里提到的尾数和指数的组合方式对于数据的精度和动态范围有着重要影响。作者通过采用精细的量化策略，如按块和瓦片进行缩放，使得在较小的元素组上操作时，能够有效地在这些分组元素之间共享指数位，从而减轻有限动态范围的影响。

**Online Quantization（在线量化）**。在张量量化框架中采用**延迟量化**（如 NVIDIA 在 2024 年 b 版提到的以及 Peng 等人在 2023 年 b 版提到的），它会维护先前迭代中的最大绝对值的历史记录，以推断当前值。为了确保准确的尺度并简化框架，我们针对每个 1x128 的激活块或 128x128 的权重块在线计算最大绝对值。基于此，我们推导出缩放因子，然后将激活值或权重在线量化为 FP8 格式。

###### 2.3.3 低精度存储和通信
结合我们的FP8训练框架，我们通过将缓存激活和优化器状态压缩为低精度格式来进一步减少内存消耗和通信开销。

**低精度优化器状态**。我们采用BF16数据格式而不是FP32来跟踪AdamW（Loshchiov和Hutter，2017）优化器中的第一和第二矩，而不会导致可观察到的性能下降。然而，主权重（由优化器存储）和梯度（用于批量大小累积）仍然保留在FP32中，以确保整个训练过程中的数值稳定性。

**低精度激活**。如上上图的混合精读框架所示，Wgrad操作在FP8中执行。为了减少内存消耗，自然选择以FP8格式缓存Linear运算符的后向传递激活。但是，对于低成本高精度训练，需要对几个运算符进行特殊考虑：
1) **在注意力操作后的Linear层输入**。这些激活也用于注意力运算符的向后传递，这使得它对精度敏感。我们专门为这些激活采用定制的E5M6数据格式。此外，这些激活将在向后传递中从1x128量化瓦片转换为128x1瓦片。为了避免引入额外的量化误差，所有缩放因子都是圆形缩放的，即积分幂为2。
2) **MoE中的SwiGLU的输入**。为了进一步降低内存成本，我们缓存了SwiGLU运算符的输入，并在反向传递中重新计算其输出。这些激活也通过我们的细粒度量化方法存储在FP8中，在内存效率和计算精度之间取得平衡。

**低精度通信**。通信带宽是MoE模型训练中的关键瓶颈。为了缓解这一挑战，我们将 MoE 向上预测之前的激活量化到 FP8 中，然后应用调度组件，这与 MoE 向上预测中的 FP8 Fprop 兼容。就像注意力算子之后的线性输入一样，这种激活的缩放因子是 2 的整数幂。类似的策略应用于 MoE 向下预测之前的激活梯度。对于前向和后向组合组件，我们将它们保留在 BF16 中，以保持训练管道关键部分的训练精度。

##### 2.4 推理与部署
在H800集群上部署DeepSeek-V3，其中每个节点内的GPU使用NVLink互连，集群内的所有GPU通过IB完全互连。为了同时确保在线服务的服务水平目标（Service-Level Objective，SLO）和高吞吐量，我们采用以下部署方案，将预填充（prefilling）和解码（decoding）阶段分开。

###### 2.4.1 预填充
预填充阶段的最小部署单元由4个节点和32个GPU组成。注意力部分采用4路张量并行（TP4）和序列并行（SP），与8路数据并行（DP8）相结合。其4的小TP大小限制了TP通信的开销。对于MoE部分，我们使用32路专家并行（EP32），它确保每个专家处理足够大的批量大小，从而提高计算效率。对于MoE所有对所有通信，我们使用与训练中相同的方法：首先通过IB跨节点传输令牌，然后通过NVLink在节点内GPU之间转发。特别是，我们对浅层中的密集MLP使用1路张量并行，以节省TP通信。
为了在MoE部分实现不同专家之间的负载平衡，我们需要确保每个GPU处理大致相同数量的令牌。为此，我们引入了**冗余专家部署方案**，该方案将高负载专家复制并冗余部署。高负载专家根据在线部署期间收集的统计数据进行检测，并定期（例如，每10分钟）进行调整。在确定冗余专家集合后，我们根据观察到的负载在一个节点内的GPU之间仔细重新安排专家，努力在不增加跨节点所有对所有通信开销的情况下尽可能地平衡跨GPU的负载。对于DeepSeek-V3的部署，我们为预填充阶段设置了32个冗余专家。对于每个GPU，除了其托管的原始8个专家之外，还将托管一个额外的冗余专家。
此外，在预填充阶段，为了提高吞吐量并隐藏所有对所有和TP通信的开销，我们同时处理具有相似计算工作量的两个微批处理，将一个微批处理的注意力和MoE与另一个微批处理的调度和组合重叠。
最后，我们正在探索一种针对专家的**动态冗余策略**，其中每个GPU托管更多的专家（例如，16个专家），但在每个推理步骤中只有9个会被激活。在每一层的所有到所有操作开始之前，我们动态计算全局最优路由方案。鉴于预填充阶段涉及的大量计算，计算这种路由方案的开销几乎可以忽略不计。

###### 2.4.2 解码
在解码过程中，我们将共享专家视为路由专家。从这个角度来看，每个令牌将在路由过程中选择9个专家，其中共享专家被视为将始终被选中的重载专家。解码阶段的最小部署单元由40个节点组成，具有320个GPU。注意力部分使用带SP的TP4，结合DP80，而MoE部分使用EP320。对于MoE部分，每个GPU仅托管一个专家，64个GPU负责托管冗余专家和共享专家。调度和合并部分的所有对所有通信通过IB上的直接点对点传输进行，以实现低延迟。此外，我们利用IBGDA（NVIDIA，2022）技术进一步减少延迟并提高通信效率。
与预填充类似，我们根据在线服务的统计专家负载，在一定间隔内定期确定冗余专家集。然而，我们不需要重新安排专家，因为每个GPU只托管一个专家。我们还在探索解码的动态冗余策略。然而，这需要更仔细地优化计算全局最优路由方案的算法，并与调度内核融合以减少开销。
此外，为了增强吞吐量并隐藏所有对所有通信的开销，我们还在探索**在解码阶段同时处理两个计算工作负载**相似的微批处理。与预填充不同，注意力在解码阶段会消耗更大部分的时间。因此，我们将一个微批处理的注意力与另一个微批处理的dispatch+MoE+combine重叠。在解码阶段，每个专家的批处理大小相对较小（通常在256个token内），瓶颈是内存访问而不是计算。由于MoE部分只需要加载一个专家的参数，因此内存访问开销最小，因此使用更少的SM不会显着影响整体性能。因此，为了避免影响注意力部分的计算速度，我们可以只分配一小部分SM来dispatch+MoE+combine。

##### 2.5 硬件设计
基于我们对全对全通信和FP8训练方案的实施，我们向AI硬件厂商提出以下芯片设计建议。
###### 2.5.1 通信硬件
在DeepSeek-V3中，我们实现了计算与通信的重叠，以便在计算过程中隐藏通信延迟。与串行的计算和通信方式相比，这大大降低了对通信带宽的依赖。然而，当前的通信实现依赖于成本高昂的流多处理器（SM）（例如，我们从H800 GPU的132个可用SM中为此分配了20个），这会限制计算吞吐量。此外，使用SM进行通信会导致效率显著低下，因为张量核心会一直未得到充分利用。

目前，SM在全对全通信中主要执行以下任务：
- 在InfiniBand（IB）和NVLink域之间转发数据，同时将来自单个GPU、发往同一节点内多个GPU的IB流量进行聚合。
- 在RDMA缓冲区（已注册的GPU内存区域）和输入/输出缓冲区之间传输数据。
- 执行全对全合并的规约操作。
- 在通过IB和NVLink域将分块数据传输给多个专家的过程中，管理细粒度的内存布局。

我们期望未来的硬件供应商能够开发出硬件，将这些通信任务从宝贵的计算单元SM中卸载出来，使其作为GPU协处理器或网络协处理器，就像英伟达的SHARP（Graham等人，2016）那样。此外，为降低应用程序编程的复杂性，我们希望这种硬件能从计算单元的角度统一IB（横向扩展）和NVLink（纵向扩展）网络。有了这个统一的接口，计算单元可以通过基于简单原语提交通信请求，轻松地在整个IB-NVLink统一域中完成读取、写入、多播和规约等操作。 

###### 2.5.2 计算硬件
**提高张量核心中 FP8 通用矩阵乘法（GEMM）的累加精度。**
**支持按瓦片（tile）和块（block）进行量化。**
**支持在线量化。**


> 整个设备这一章是围绕着GPU设计与部署进行讨论的，主要还是对计算资源进行了很多讨论。这部分对我来说十分陌生。基本读不懂。

#### 3.预训练
[回顶部](#deepseek-v3)

##### 3.1 数据建设
与DeepSeek-V2相比，我们通过增强数学和编程样本的比例来优化预训练语料库，同时将多语言覆盖范围扩展到英语和中文之外。此外，我们的数据处理管道经过细化，以最大限度地减少冗余，同时保持语料库多样性。受丁等人（2024）的启发，我们实现了**文档打包**方法以实现数据完整性，但在训练期间不包含跨样本注意力掩蔽。最后，DeepSeek-V3的训练语料库由我们的tokenizer中的14.8T高质量和多样化的token组成。
在 DeepSeekCoder-V2（DeepSeek-AI，2024a）的训练过程中，我们观察到中间填充（Fill-in-Middle，FIM）策略在不损害下一个标记预测能力的同时，使模型能够根据上下文线索准确预测中间文本。与 DeepSeekCoder-V2 一致，我们在 DeepSeek-V3 的预训练中也采用了 FIM 策略。具体来说，我们使用前缀-后缀-中间（Prefix-Suffix-Middle，PSM）框架将数据组织如下：

<|fim_begin|>$f_{pre}$<|fim_hole|>$f_{suf}$<|fim_end|>$f_{middle}$<|eos_token|>

此结构作为预打包过程的一部分应用于文档级别。FIM策略以0.1的速率应用，与PSM框架一致。DeepSeek-V3的标记器采用字节级BPE（Shibata et al.，1999），扩展了128K个标记的词汇表。我们的标记器的预标记器和训练数据经过修改，以优化多语言压缩效率。此外，与DeepSeek-V2相比，新的预标记器引入了结合标点符号和换行符的标记。然而，当模型处理没有终端换行符的多行提示时，这种技巧可能会引入标记边界偏差（Lundberg，2023），特别是对于少拍评估提示。为了解决这个问题，我们在训练期间随机拆分了一定比例的此类组合标记，这将模型暴露在更广泛的特殊情况下，并减轻了这种偏差。

##### 3.2 超参数

**模型超参数**。我们将Transformer层数设置为61，隐层维度设置为7168. 所有可学习的参数都用 0.006 的标准差随机初始化。在MLA中，我们将注意力头的数量$n_{h}$设置为128，每个头的维度$d_{h}$设置为128。键值（KV）压缩维度$d_{c}$设置为512，查询压缩维度$d_{c}'$设置为1536。对于解耦的查询和键，我们将每个头的维度$d_{h}^{R}$设置为64。我们用混合专家（MoE）层替换除前三层之外的所有前馈神经网络（FFN）层。每个MoE层由1个共享专家和256个路由专家组成，其中每个专家的中间隐藏层维度为2048。在路由专家中，每个token将激活8个专家，并且确保每个标记最多被发送到4个节点。多词元预测深度D设置为1，即除了准确的下一个标记之外，每个标记还将预测一个额外的标记。与DeepSeek-V2一样，DeepSeek-V3也在压缩后的潜在向量之后采用了额外的RMSNorm层，并在宽度瓶颈处乘以额外的缩放因子。在这种配置下，DeepSeek-V3总共有6710亿个参数，其中每个标记激活370亿个参数。 

**训练超参数**。使用**AdamW优化器**，超参数设置为$\beta_1=0.9,\beta_2=0.95,weight\_decay=0.1.$ 在预训练期间将最大序列长度设置为4K，并在14.8T的token上训练DeepSeek-V3。对于学习率调度，首先在前2K个步骤线性将其从0增加到$2.2\times 10^{-4}$ 直到模型消耗10T的训练token。随后，我们在4.3万亿个标记的训练过程中，按照余弦衰减曲线将学习率逐渐衰减至2.2×10⁻⁵。在最后5000亿个标记的训练过程中，前3330亿个标记保持2.2×10⁻⁵的恒定学习率，在剩余的1670亿个标记中切换到7.3×10⁻⁶的另一个恒定学习率。梯度裁剪范数设置为1.0。我们采用批量大小调度策略，在训练前4690亿个标记时，批量大小从3072逐渐增加到15360，然后在剩余的训练中保持15360。我们利用流水线并行性将模型的不同层部署在不同的GPU上，对于每一层，路由专家将均匀部署在属于8个节点的64个GPU上。至于节点限制路由，每个标记最多将被发送到4个节点（即𝑀 = 4）。为实现无辅助损失的负载均衡，在前14.3万亿个标记的训练中，我们将偏差更新速度𝛾设置为0.001，在剩余的5000亿个标记训练中设置为0.0。对于平衡损失，我们将𝛼设置为0.0001，只是为了避免任何单个序列内出现极端的不平衡情况。在前10万亿个标记的训练中，多标记预测（MTP）损失权重𝜆设置为0.3，在剩余的4.8万亿个标记训练中设置为0.1。 

##### 3.3 长文本扩展
我们在DeepSeek - V3中采用了与DeepSeek-V2（DeepSeek-AI，2024c）类似的方法来实现长上下文能力。在预训练阶段之后，我们应用YaRN（Peng等人，2023a）进行上下文扩展，并执行两个额外的训练阶段，每个阶段包含1000步，以逐步将上下文窗口从4K扩展到32K，再扩展到128K。YaRN的配置与DeepSeek-V2中使用的一致，仅应用于解耦的共享键$k_{t}^{R}$ 。两个阶段的超参数保持相同，缩放比例$s = 40$ ，$\alpha = 1$ ，$\beta = 32$，缩放因子$\sqrt{t}=0.1\ln s + 1$ 。在第一阶段，序列长度设置为32K，批量大小为1920。在第二阶段，序列长度增加到128K，批量大小减小到480。两个阶段的学习率均设置为$7.3×10^{-6}$，与预训练阶段的最终学习率一致。 
通过这两个阶段的扩展训练，DeepSeek-V3能够处理长度高达128K的输入，同时保持强大的性能。下图表明，经过有监督微调后，DeepSeek-V3在“大海捞针”（NIAH）测试中取得了显著的成绩，在长达128K的上下文窗口长度范围内都展现出了稳定的稳健性。 

![“干草堆中的针”（Needle In A Haystack，NIAH）测试的评估结果。DeepSeek-V3在128K的所有上下文窗口长度上都表现良好。](<截屏2025-02-16 08.51.32.png>)

##### 3.4 评估

###### 3.4.1 评估基准
DeepSeek-V3的基础模型是在一个多语言语料库上进行预训练的，其中英语和中文占大多数。因此，我们主要在一系列以英语和中文为主的基准测试，以及一个多语言基准测试上评估其性能。我们的评估基于集成在我们HAI-LLM框架中的内部评估框架。所考虑的基准测试分类列举如下，其中带<u>下划线</u>的基准测试是中文的，<span style="text-decoration: underline double;">双下划线</span>的基准测试是多语言的： 

- **多主题多项选择题**数据集包括MMLU（Hendryck et al.，2020）、MMLU-Redux（Gema et al.，2024）、MMLU-Pro（Wang et al.，2024b）、<span style="text-decoration: underline double;">MMMLU</span>（OpenAI，2024b）、<u>C-Eval</u>（Huang et al.，2023）和<u>CMMLU</u>（Li et al.，2023）。
- **语言理解和推理**数据集包括HellaSwag（Zeller等人，2019年）、PIQA（Bisk等人，2020年）、ARC（Clark等人，2018年）和BigBench hard（BBH）（Suzgun等人，2022年）。
- **封闭式问答**数据集包括TriviaQA（Joshi et al.，2017）和Natu-ralks（Kwiatkowski et al.，2019）。
- **阅读理解**数据集包括RACE赖等人。（2017年）、DROP（Dua等人，2019年）、<u>C3</u>（Sun等人，2019a）和<u>CMRC</u>（崔等人，2019年）。
- **参考消歧**数据集包括<u>CLUEWSC</u>（徐等人，2020年）和WinoGrande Sakaguchi等人。（2019年）。
- **语言建模**数据集包括Pile（High et al.，2020）。
- **中文的理解和文化**数据集包括<u>CCPM</u>（Li et al.，2021）。
- **数学**数据集包括GSM8K（Cobbe et al.，2021）、MATH（Hendryks et al.，2021）、MGSM（shi et al.，2023）和<u>CMath</u>（Weiet al.，2023）。
- **代码**数据集包括HumEval（Chen et al.，2021）、LiveCodeBench-Base（0801-1101）（Jain et al.，2024）、MBPP（Austin et al.，2021）和CRUXEval（gu et al.，2024）。
- **标准化考试**包括<u>AGIEval</u>（钟等人，2023）。请注意，AGIEval包括英语和汉语子集。

延续我们之前的研究（DeepSeek-AI，2024b，c），对于包括HellaSwag、PIQA、WinoGrande、RACE - Middle、RACE - High、MMLU、MMLU - Redux、MMLU - Pro、MMMLU、ARC - Easy、ARC - Challenge、C - Eval、CMMLU、C3和CCPM在内的数据集，我们采用基于困惑度的评估方法；对于TriviaQA、NaturalQuestions、DROP、MATH、GSM8K、MGSM、HumanEval、MBPP、LiveCodeBench - Base、CRUXEval、BBH、AGIEval、CLUEWSC、CMRC和CMath等数据集，则采用基于生成的评估方法。此外，针对Pile - test数据集，我们执行基于语言建模的评估，并使用每字节比特数（BPB）作为指标，以确保在使用不同分词器的模型之间进行公平比较。 

###### 3.4.2 评估结果

![DeepSeek-V3-Base与其他具有代表性的开源基础模型之间的比较。所有模型都在我们的内部框架中进行评估，并共享相同的评估设置。差距不超过0.3的分数被认为处于同一水平。DeepSeek-V3-Base在大多数基准测试上实现了最佳性能，尤其是在数学和代码任务上](<截屏2025-02-17 18.25.49.png>)

在上表中，我们将DeepSeek-V3的基础模型与当前最先进的开源基础模型进行了对比，其中包括DeepSeek-V2-Base（DeepSeek - AI，2024c）（我们之前发布的模型）、Qwen2.5 72B Base（Qwen，2024b）以及LLaMA-3.1 405B Base（AI@Meta，2024b）。我们使用内部评估框架对所有这些模型进行评估，并确保它们采用相同的评估设置。需要注意的是，由于过去几个月我们评估框架有所变动，DeepSeek-V2-Base的性能与我们之前报告的结果略有差异。总体而言，DeepSeek-V3-Base全面超越了DeepSeek-V2-Base和Qwen2.5 72B Base，并且在大多数基准测试中超过了LLaMA-3.1 405B Base，从本质上讲，它已成为最强的开源模型。 
从更详细的角度来看，我们将DeepSeek - V3 - Base与其他开源基础模型逐一进行比较。（1）与DeepSeek - V2 - Base相比，由于我们在模型架构上的改进、模型规模和训练数据量的扩大，以及数据质量的提升，DeepSeek - V3 - Base如预期般取得了显著更优的性能。（2）与当前最先进的中文开源模型Qwen2.5 72B Base相比，DeepSeek - V3 - Base**仅用一半的激活参数**，就展现出显著优势，尤其是在英文、多语言、代码和数学相关的基准测试中。至于中文基准测试，除了中文多科目选择题任务CMMLU外，DeepSeek - V3 - Base的表现也优于Qwen2.5 72B。（3）与**拥有11倍激活参数**的最大开源模型LLaMA - 3.1 405B Base相比，DeepSeek - V3 - Base在多语言、代码和数学基准测试中同样展现出更优的性能。在英文和中文语言基准测试方面，DeepSeek - V3 - Base表现出竞争力或更优的性能，在BBH、MMLU系列、DROP、C - Eval、CMMLU和CCPM等测试中表现尤为出色。 
由于我们采用了高效的架构以及全面的工程优化，DeepSeekV3实现了极高的训练效率。在我们的训练框架和基础设施条件下，使用每一万亿个标记训练DeepSeek - V3仅需18万个H800 GPU小时，这比训练720亿参数或4050亿参数的密集型模型成本要低得多。 

##### 3.5 讨论
###### 3.5.1 多词元预测模型的消融研究
下表展示了MTP策略的消融实验结果。具体而言，我们在**不同规模的两个**基线模型上验证了多标记预测（MTP）策略。在小规模方面，我们用1.33万亿个token训练了一个总参数为157亿的基线混合专家（MoE）模型。在大规模方面，我们用5400亿个词元训练了一个总参数为2287亿的基线MoE模型。在这两个基线模型基础上，保持训练数据和其他架构不变，我们在其上附加一个深度为1的MTP模块，并使用MTP策略训练两个模型以作比较。请注意，在推理过程中，我们直接舍弃MTP模块，所以用于比较的模型推理成本完全相同。从表格中我们可以观察到，MTP策略在大多数评估基准上持续提升了模型性能。 

![MTP策略的消融结果。MTP策略在大多数评估基准上始终如一地提高了模型性能。](<截屏2025-02-17 19.42.47.png>)

###### 3.5.2 无辅助损失负载平衡策略
在下表中，我们展示了无辅助损失平衡策略的消融实验结果。我们在不同规模的两个基线模型上验证了这一策略。在小规模层面，我们使用1.33万亿个标记训练一个总参数为157亿的基线混合专家（MoE）模型。在大规模层面，我们使用5780亿个标记训练一个总参数为2287亿的基线MoE模型。这两个基线模型都单纯依靠辅助损失来促进负载均衡，并使用带有Top - K亲和度归一化的Sigmoid门控函数。它们控制辅助损失强度的超参数分别与DeepSeek - V2 - Lite和DeepSeek - V2相同。在这两个基线模型的基础上，保持训练数据和其他架构不变，我们去除所有辅助损失，并引入无辅助损失平衡策略进行对比。从表中我们可以观察到，无辅助损失策略在大多数评估基准上始终能实现更好的模型性能。 

![无损辅助损失负载平衡策略的消融结果。与纯auxiliary-loss-based方法相比，辅助无损失策略在大多数评估基准上都取得了更好的模型性能。](<截屏2025-02-17 19.48.57.png>)

###### 3.5.3 按批次负载均衡与按序列负载均衡
无辅助损失平衡与按序列辅助损失之间的关键区别在于它们的平衡范围：按批次平衡与按序列平衡。与按序列辅助损失相比，**按批次平衡施加的约束更为灵活**，因为它并不强制每个序列在域内保持平衡。这种灵活性使专家们能够更好地在不同领域实现专业化。为了验证这一点，我们记录并分析了基于160亿参数且采用辅助损失的基线模型，以及160亿参数无辅助损失模型在“堆”（Pile）测试集中不同领域的专家负载情况。如下图所示，我们观察到，正如预期的那样，无辅助损失模型展现出更显著的专家专业化模式。 

![Pile测试集中三个域上辅助无损失和auxiliary-loss-based模型的专家负载。辅助无损失模型比auxiliary-loss-based模型显示出更大的专家专业化模式。相对专家负载表示实际专家负载和理论平衡专家负载之间的比率。由于空间限制，我们只以两层的结果为例，所有层的结果在附录C中提供。](<截屏2025-02-17 19.53.58.png>)

为了进一步探究这种灵活性与模型性能优势之间的相关性，我们额外设计并验证了一种**按批次辅助损失方法**，该方法鼓励在每个训练批次而非每个序列上实现负载均衡。实验结果表明，当达到相似的按批次负载均衡水平时，按批次辅助损失方法也能实现与无辅助损失方法相近的模型性能。具体而言，在我们针对10亿参数混合专家（MoE）模型的实验中，验证损失分别为：2.258（使用按序列辅助损失）、2.253（使用无辅助损失方法）以及2.253（使用按批次辅助损失） 。
此外，尽管按批次负载均衡方法展现出持续的性能优势，但它们在效率方面也面临两个潜在挑战：（1）某些序列或小批次内的**负载不均衡**；（2）推理过程中因领域转移导致的**负载不均衡**。第一个挑战可由我们的训练框架自然解决，该框架采用大规模的专家并行和数据并行，确保每个微批次的数据量足够大。对于第二个挑战，正如2.4节所述，我们还设计并实现了一个带有冗余专家部署的高效推理框架来加以克服。 

#### 4.后训练
[回顶部🔝](#deepseek-v3)

##### 4.1 有监督微调

V3精心整理了用于指令微调的数据集，其中包含150万个实例，涵盖多个领域。每个领域都根据其特定需求，采用了不同的数据生成方法。 

**推理数据**。对于与推理相关的数据集，包括那些专注于数学、代码竞赛问题和逻辑谜题的数据集，我们通过利用内部DeepSeek-R1模型来生成数据。具体来说，虽然R1生成的数据表现出很强的准确性，但它存在思考过度、格式不佳和长度过长等问题。我们的目标是平衡R1生成的推理数据的高准确性和规则格式的推理数据的清晰和简洁。
> 用大模型来驱动大模型，通过对R1生成的推理内容进行处理的，得到更优秀的推理样本。

为确立我们的方法，我们首先针对特定领域（如代码、数学或通用推理）开发一个**专家模型**，采用监督微调（Supervised Fine- Tuning，SFT）与强化学习（Reinforcement Learning，RL）相结合的训练流程。**这个专家模型充当最终模型的数据生成器**。训练过程涉及为每个实例生成两种不同类型的监督微调样本：第一种以<问题，原始回答>的格式，将问题与其原始回答配对；第二种则以<系统提示词，问题，R1回答>的格式，把系统提示词、问题及R1回答整合在一起。 
系统提示经过**精心设计**，包括指导模型产生富含**反射和验证机制**的响应的指令。在RL阶段，即使没有明确的系统提示，模型也利用高温采样生成集成R1生成和原始数据模式的响应。经过数百个RL步骤，中间RL模型学会了合并R1模式，从而从战略上提高了整体性能。
完成RL训练阶段后，我们实施**拒绝抽样**，为最终模型管理高质量的SFT数据，其中专家模型用作数据生成源。这种方法确保最终训练数据保留DeepSeek-R1的优势，同时产生简洁有效的响应。

**非推理数据**。对于非推理数据，例如创意写作、角色扮演和简单问答，我们使用DeepSeek-V2.5生成响应并招募人工注释器来验证数据的准确性和正确性。

**SFT设置**。我们使用SFT数据集微调DeepSeek-V3-Base两个时期，使用从5×10-6开始并逐渐降低到1×10-6的余弦衰减学习率调度。在训练期间，每个单个序列都来自多个样本。然而，我们采用样本掩蔽策略以确保这些示例保持孤立和相互不可见。

> 在SFT设置中，确保示例相互隔离、彼此不可见，原因如下：
> - **防信息泄露与过拟合**：样本不隔离，模型易记特定关联，而非普遍模式，致新数据下表现差、过拟合，无法正确处理新组合。
> - **保训练独立稳定**：隔离样本让模型基于自身能力学习推理，不受干扰，助其稳定收敛，避免因样本相互影响致训练波动、参数更新偏差。
> - **模拟真实场景**：实际中模型常逐个处理数据，训练样本隔离可模拟此场景，让模型适应独立处理方式，提升泛化能力。
> - **增强泛化能力**：样本不可见时，模型需挖掘本质特征规律，学习通用知识，以更好应对不同数据集和任务及复杂现实情况。 

##### 4.2 强化学习

###### 4.2.1 奖励模型
我们在RL流程中使用基于规则的奖励模型（Reward Model，RM）和基于模型的RM。

**基于规则的奖励模型**。基于规则的RM。对于可以使用特定规则验证的问题，我们采用基于规则的奖励系统来确定反馈。例如，某些数学问题具有确定性结果，我们要求模型在指定格式（例如，在框中）内提供最终答案，允许我们应用规则来验证正确性。类似地，对于**LeetCode问题**，我们可以利用编译器根据测试用例生成反馈。通过尽可能利用基于规则的验证，我们确保了更高水平的可靠性，因为这种方法可以抵抗操纵或利用。
> DeepSeek对leetcode问题的解答估计要吊打其他模型了！他们赶上好时候了。

**基于模型的奖励模型**。Model-based RM。对于具有自由形式的基本事实答案的问题，我们依靠奖励模型来确定回答是否与预期的基本事实相匹配。相反，对于没有明确的基本事实的问题，例如那些涉及创造性写作的问题，奖励模型的任务是根据问题和相应的答案提供反馈作为输入。奖励模型从DeepSeek-V3 SFT检查点训练。为了提高其可靠性，我们构建了偏好数据，不仅提供最终奖励，还包括导致奖励的**思维链**。这种方法有助于降低特定任务中奖励黑客的风险。

###### 4.2.2 分组相对策略优化
与DeepSeek - V2（DeepSeek - AI，2024c）类似，我们采用分组相对策略优化（GRPO）（Shao等人，2024），该方法舍弃了通常与策略模型规模相同的评论家模型 ，而是从分组得分中估计基线。具体来说，对于每个问题 $q$，GRPO 从旧策略模型 $\pi_{\theta_{old }}$ 中采样一组输出 ${o_{1}, o_{2}, \cdots, o_{G}}$，然后通过最大化以下目标来优化策略模型 $\pi_{\theta}$：

$$\begin{aligned} \mathcal{J}_{GRPO }(\theta) & =\mathbb{E}\left[q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{old }}(O | q)\right] \\ & \frac{1}{G} \sum_{i=1}^{G}\left(min \left(\frac{\pi_{\theta}\left(o_{i} | q\right)}{\pi_{\theta_{old }}\left(o_{i} | q\right)} A_{i}, clip\left(\frac{\pi_{\theta}\left(o_{i} | q\right)}{\pi_{\theta_{old }}\left(o_{i} | q\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{i}\right)-\beta \mathbb{D}_{K L}\left(\pi_{\theta} \| \pi_{r e f}\right)\right), \end{aligned}$$ 

$$\mathbb{D}_{K L}\left(\pi_{\theta} \| \pi_{r e f}\right)=\frac{\pi_{r e f}\left(o_{i} | q\right)}{\pi_{\theta}\left(o_{i} | q\right)}-log \frac{\pi_{r e f}\left(o_{i} | q\right)}{\pi_{\theta}\left(o_{i} | q\right)}-1$$

其中$\varepsilon,\beta$是超参数，$\pi_{ref}$是参考模型；$A_{i}$ 是优势值，由每组输出对应的奖励 ${r_{1}, r_{2},..., r_{G}}$ 得出：

$$A_{i}=\frac{r_{i}-mean\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)}{std\left(\left\{r_{1}, r_{2}, \cdots, r_{G}\right\}\right)} $$

在RL过程中，我们整合了来自不同领域的提示，例如编码、数学、写作、角色扮演和问答。这种方法不仅使模型更**接近人类偏好**，还提高了基准测试的性能，尤其是在可用SFT数据有限的情况下。

##### 4.3 评估
###### 4.3.1 评估设置
**评估基准**。除了我们用于基础模型测试的基准之外，我们还在 IFEval（周等人，2023 年）、FRAMES（克里希纳等人，2024 年）、LongBench v2（白等人，2024 年）、GPQA（赖因等人，2023 年）、SimpleQA（OpenAI，2024c）、C-SimpleQA（何等人，2024 年）、SWE-Bench 验证（OpenAI，2024d）、Aider 1、LiveCodeBench（Jain 等人，2024 年）（2024 年 8 月至 2024 年 11 月的问题）、Codeforce 2、中国全国高中数学奥林匹克（CNMO 2024 年）3 和 2024 年美国数学邀请赛（AIME 2024 年）（MAA，2024 年）上进一步评估指令模型。

**比较基线**。我们针对七个强基线对我们的聊天模型进行综合评估，包括DeepSeek-V2-0506、DeepSeek-V2.5-0905、Qwen2.572BInstruct、LLaMA-3.1405BInstruct、Claude-Sonnet-3.5-1022和GPT-4-0513。对于DeepSeek-V2模型系列，我们选择最具代表性的变体进行比较。对于闭源模型，评估通过各自的API执行。

**详细的评估配置**。对于包括MMLU、DROP、GPQA和SimpleQA在内的标准基准测试，我们采用[OpenAI的简单评估框架](https://github.com/openai/simple-evals)中的评估提示。我们在零触发设置中使用MMLU-Redux的零评估提示格式（Lin，2024）。对于其他数据集，我们按照数据集创建者提供的默认提示遵循其原始评估协议。对于代码和数学基准测试，HumEval-Mul数据集总共包括8种主流编程语言（Python、Java、Cpp、C#、JavaScript、TypeScript、PHP和Bash）。我们使用CoT和非CoT方法来评估LiveCodeBench上的模型性能，其中数据收集于2024年8月至2024年11月。Codeforce数据集使用竞争对手的百分比进行测量。使用无代理框架评估经过验证的SWE-Bench（夏和其他人，2024年）。我们使用“diff”格式来评估与Aider相关的基准测试。对于数学评估，AIME和CNMO 2024在0.7的温度下进行评估，结果在16次运行中平均，而MATH-500采用贪婪解码。我们允许所有模型为每个基准输出最多8192个标记。

###### 4.3.2 标准评估
下表显示了评估结果，展示了DeepSeek-V3是性能最好的开源模型。此外，它与GPT-4o和Claude-3.5-Sonnet等前沿闭源模型具有竞争力。
![DeepSeek-V3和其他代表性聊天模型之间的比较。所有模型都在将输出长度限制为8K的配置中进行评估。包含少于1000个样本的基准测试使用不同的温度设置进行多次测试，以获得稳健的最终结果。DeepSeek-V3是性能最好的开源模型，并且在与前沿闭源模型的竞争中也表现出竞争力。](<截屏2025-02-18 11.26.12.png>)

**英文基准**。**MMLU**是一个广受认可的基准，旨在评估大型语言模型在不同知识领域和任务中的性能。DeepSeek-V3展示了竞争性能，与LLaMA-3.1-405B、GPT-4o和Claude-Sonnet 3.5等顶层模型旗鼓相当，同时显著优于Qwen2.572B。此外，DeepSeek-V3在更具挑战性的教育知识基准MMLU-Pro中表现出色，紧随Claude-Sonnet 3.5之后。在MMLU-Redux上，DeepSeek-V3超越了同行。MMLU-Redux是MMLU的改进版本，带有更正的标签。此外，在博士级评估测试平台GPQA-Diamond上，DeepSeek-V3取得了显著的成绩，排名仅次于Claude 3.5 Sonnet，并大幅优于所有其他竞争对手。
在**DROP、LongBench v2和FRAMES等**长上下文理解基准测试中，DeepSeek-V3继续展示其作为顶层模型的地位。它在DROP上的3镜头设置中获得了令人印象深刻的91.6 F1分数，优于该类别中的所有其他模型。在FRAMES（一个需要在100k令牌上下文中回答问题的基准测试）上，DeepSeek-V3紧随GPT-4o之后，同时以显着优势优于所有其他模型。这证明了DeepSeek-V3在处理极长上下文任务方面的强大能力。DeepSeek-V3的长上下文功能在LongBench v2上的同类最佳性能进一步得到了验证，LongBench v2是一个在DeepSeek V3发布前几周发布的数据集。在事实知识基准测试中，SimpleQA、DeepSeek-V3落后于GPT-4o和Claude-Sonnet，主要是由于其设计重点和资源分配。DeepSeek-V3分配了更多的训练令牌来学习中文知识，从而在C-SimpleQA上获得了卓越的性能。在instruction-following基准测试中，DeepSeek-V3明显优于其前身DeepSeek-V2系列，突出了其理解和遵守用户定义格式约束的能力。

**编程和数学基准**。对于LLM来说，编码是一项具有挑战性和实用性的任务，需要通过以工程为重点的任务，如**SWE-Bench-Verfied和Aider**，以及以算法为重点的任务，如**HumEval和LiveCodeBench**。在工程任务中，DeepSeek-V3落后于Claude-Sonnet-3.5-1022但明显优于开源模型。开源的DeepSeek-V3有望促进编码相关工程任务的进步。通过提供对其强大功能的访问，DeepSeek-V3可以推动软件工程和算法开发等领域的创新和改进，使开发人员和研究人员能够突破开源模型在编码任务中可以实现的界限。在算法任务中，DeepSeek-V3表现出卓越的性能，超过了像HumEval-Mul和LiveCodeBench等基准测试的所有基线。这一成功可以归因于其先进的知识提炼技术，该技术有效地增强了其在以算法为重点的任务中的代码生成和解决问题的能力。
在数学基准测试方面，DeepSeek-V3表现出卓越的性能，大大超过了基线，**为非o1类模型设定了新的最先进水平**。具体来说，在**AIME、MAH-500和CNMO 2024**上，DeepSeek-V3的绝对分数比第二好的模型Qwen2.572B高出约10%，这对于如此具有挑战性的基准测试来说是一个巨大的优势。这一非凡的能力凸显了DeepSeek-R1蒸馏技术的有效性，事实证明这对非o1类模型非常有益。


![英语开放式对话评估。对于AlpackaEval 2.0，我们使用长度控制的胜率作为指标。](<截屏2025-02-19 08.39.04.png>)

## DeepSeek R1 
<a href="#deekseek系列">🔝</a>



