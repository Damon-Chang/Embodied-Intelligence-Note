# 大模型的具身智能：发展与挑战
> 白辰甲, 许华哲, 李学龙. 大模型驱动的具身智能: 发展与挑战. 中国科学: 信息科学, 2024, 54: 2035–2082, doi: 10.1360/ SSI- 2024- 0076
Bai C J, Xu H Z, Li X L. Embodied-AI with large models: research and challenges (in Chinese). Sci Sin Inform, 2024, 54: 2035–2082, doi: 10.1360/SSI-2024-0076

## 摘要
> 大模型驱动的具身智能是涵盖**人工智能、机器人学和认知科学**的交叉领域, 重点研究如何将大模型的感知、推理和逻辑思维能力与具身智能相结合, 提升现有模仿学习、强化学习、模型预测控制等具身智能框架的数据效率和泛化能力. 近年来, 随着大模型能力的不断提升, 以及具身智能中示教数据、仿真平台、任务集合的不断完善, 大模型和具身智能的结合将成为人工智能的下一个浪潮, 有望成为人工智能迈向实体机器人的重要突破口. 本文围绕**大模型驱动的具身智能**这一研究领域, 从3个方面进行了系统的调研、分析和展望. 
> - 首先, 回顾了大模型和具身智能的相关技术背景, 以及具身智能现有的学习框架. 
> - 其次, 按照大模型赋能具身智能的方式, 将现有研究分为大模型驱动的环境感知、大模型驱动的任务规划、大模型驱动的基础策略、大模型驱动的奖励函数、大模型驱动的数据生 成等5类范式. 
> - 最后, 总结了大模型驱动的具身智能中存在的挑战, 对可行的技术路线进行展望, 为相 关研究人员提供参考, 进一步推动国家人工智能发展战略.  
> 
> **关键词**: 具身智能, 大模型, 环境感知, 任务规划, 基础策略

## 背景与定义
### 具身智能
- **具身智能**（Embodied AI）是人工智能、机器人学、认知科学的交叉领域，主要研究如何使机器人具备类似人类的感知、规划、决策和行为能力。具身智能可以追溯到20世纪50年代，图灵首次提出具身智能的概念，探索如何使用机器感知和理解世界，并作出相应的决策和行动。**基于高算力平台和大规模标注数据的深度学习方法形成的非具身智能体缺乏与环境交互学习的经验，无法直接驱动机器人实体完成特定任务**。具身智能很强调**感知-运动回路（perception-action loop），使用物理实体来感知和建模环境，根据任务目标和实体能力进行规划和决策，最后使用物理实体的运动能力来完成任务**。具身实体完成任务结果的反馈将进一步优化智能体的策略，从而使智能体的行为能够适应变化的环境。
> 具身智能在研究中更多体现智能的理念，在具身实体中融合了**视觉、语言、决策**等多方面的技术来提升智能体的通用性和泛化性。

### 大模型
- **大语言模型** (Large Language Model, LLM)以ChatGPT为代表的大语言模型技术取得了突破性进展。通过在大规模网络对话数据中进行学习，ChatGPT能够实现包括自动问答、文本分类、自动摘要、机器翻译、聊天对话等各种自然语言理解和自然语言生成任务，同时具备在**少样本**和**零样本**场景下达到了传统学习方法的性能，并具有较强的泛化能力。通过先进的**思维链（Chainof Thoughts，CoT）**等提示技术，大语言模型的逻辑推理能力获得了大幅提升，从而有望解决复杂具身智能场景中的任务分解和推理问题。
- **视觉基础模型**（visual foundation model，VFM）通过自监督学习目标可以获得强大的**视觉编码器**，能够解决如**图像分类、语义分割、场景理解等视觉感知任务**，在具身智能任务中，强大的视觉编码器能够对**视觉传感器**获得的周围环境信息进行分析和理解，从而帮助智能体进行决策；
- **视觉-语言模型**（visual-language model，VLM）通过引入**预训练视觉编码器和视觉-语言模态融合模块**，使得大语言模型能够获取视觉输入。同时根据语言提示进行**视觉问答**。在具身智能中，引入视觉-语言模型能够使智能体根据任务语言指令和环境的视觉观测进行推理和决策，从而提升智能体对环境的感知和理解能力；
- **多模态大模型**（large multimodal model，LMM）视频、音频、肢体语言、面部表情和**生理信号**等更多模态，可以分析更丰富的传感器输入并进行信息融合，同时结合具身智能体中特有的**机器人状态、关节动作**等模态信息，帮助解决更复杂的具身智能任务。

大模型通过充分利用大规模数据集中学习到的知识，结合特定的具身智能场景和任务描述，为智能体提供环境感知和任务规划能力。下图列举了近年来大模型驱动的具身智能领域的代表性成果。![alt text](<截屏2024-12-17 20.15.29.png>)

### 大模型驱动的具身智能
在赋能感知和规划之外，大模型能够和具身智能的经典框架结合，提升策略的泛化能力和对环境的适应能力。具身智能的传统框架主要包括**模仿学习（imitation learning，IL）、强化学习（reinforcement learning，RL）和模型预测控制（model predictive control，MPC）** 等。具体地：
- 模仿学习遵循监督学习的范式，通过直接从**专家轨迹数据**中学习策略，但往往受限于专家数据的规模和**协变量偏移（covariate shift）**问题而容易产生较高的泛化误差；
- 强化学习通过在环境交互中试错来获得样本，通过**最大化奖励**来获得策略和值函数，但在机器人任务中受限于**复杂的奖励设计**和**长时间的环境交互**；
- 模型预测控制通过使用**环境模型**产生对未来策略执行情况的预测，结合**策略搜索方法**获得当前最优的动作，但依赖于对环境的先验知识和环境模型的泛化能力。

大模型与上述框架结合：

- 在模仿学习中，大语言模型和视觉语言模型能够作为**基础策略**使智能体利用大模型对环境的理解和泛化能力，同时，大模型**对任务的分解**能够产生的任务的短期目标来降低模仿学习的难度；
- 在强化学习中，大模型能够根据对大模型能够根据对任务和场景的理解产生合适的**奖励函数**来引导强化学习中的**价值函数和策略函数**的学习，同时强化学习能够作为大模型的**基础策略**和**人类偏好**对齐的工具（引导大模型给出人类便好的答案）；
- 在模型预测控制的框架下，大模型能够利用从大量数据中获取的对物理世界的理解**构建环境模型**，进而使智能体能够使用环境模型进行交互和策略搜索。

此外，视觉生成模型和语言生成模型可以根据任务需求生成机器人交互环境供强化学习算法进行交互，或生成交互数据来扩充特定任务下的**专家样本**，用于缓解真实机器人任务重普遍存在的数据稀缺问题。
大模型驱动驱动的具身智能算法主要包含大模型驱动的环境感知、任务规划、基础策略、奖励函数和数据生成五个方面：
1. **大模型驱动的环境感知**从冗余的多传感器观测中进行特征抽取和信息融合，能够提取对策略学习有用的信息，从而使具身智能学习框架普遍受益；
2. **大模型对宏观任务的规划**使用大模型的逻辑推理能力对复杂任务进行分解，允许使用灵活的底层学习框架对分解后的任务进行策略学习；
3. **大模型驱动的基础策略**可以与模仿学习框架进行结合并作为模型学习的初始任务进行策略学习，在使用少量机器人的任务数据微调后，大模型能够将通用的环境理解能力和特定的具身应用场景结合，**减少测策略训练对机器人数据的需求量**并提升策略的泛化能力；
4. **大模型驱动的奖励函数**可与强化学习算法进行结合，减少机器人场景中认为进行奖励函数设计的难度，降低奖励函数设计对物理先验知识的依赖，客服强化学习算法在机器人任务中面临的稀疏奖励问题；
5. **大模型驱动的数据生成**根据学习框架的不同分为两类：
   1. 大模型可以作为环境模型生成智能体的**未来轨迹预测**，与模型预测算法和基于模型的强化学习算法相结合进行策略搜索；
   2. 大模型可以生成机器人数据用于具身策略训练，，作用于**无模型强化学习算法和模仿学习算法**，从而缓解机器人任务中的数据缺乏问题。

大模型驱动的具身智能研究中存在的5大挑战，主要包括：
1. 大模型在特定具体具身场景中的适应问题，将大模型的“通才”转化为机器人任务中的“专才”智能体；
2. 大模型策略和人类偏好对齐问题；
3. 具身策略的跨域泛化问题；
4. 大模型驱动多智能体协作的能力；
5. 大模型具身策略的决策实时性问题。

本文总结的内容具体框架如下图所示。
![alt text](<截屏2024-12-17 14.40.12.png>)

> 具身智能基本概念主要包括具身实体定义、传感器、任务定义等；具身智能学习框架主要包括模仿学习、强化学习、模型预测控制等；大模型技术主要包括大语言模型、视觉-语言大模型、多模态模型、扩散生成模型等。
### 具身系统基本概念
具身智能系统的基本结构如下图所示。![alt text](<截屏2024-12-17 20.28.56.png>)主要包括**实体、任务、环境**三个部分。
- **实体**是系统的核心，主要包括机器人、传感器、执行器等部分；
  - 在特定任务中，机器人通过传感器获取对环境的感知，随后由具身智能算法产生当前合适的动作，将动作传输给执行器，执行期产生底层机器人指令与环境进行交互，获得环境的反馈和更新后的场景信息斌循环进行上述过程。机器人类型往往决定了具体可以使用的传感器和执行器，常用的机器人包括机械臂、四足机器人、移动机器人、灵巧手、人形机器人等，如下图所示：![alt text](<截屏2024-12-17 20.36.46.png>)
    - **机械臂**是最为常见的具身实体类型。机械臂类型，仿真平台，机械臂策略数据集；
    - **四足机器人**可以在复杂的地形条件下稳定行走、奔跑、跳跃和避障等任务，不同地形采用不同策略，依赖仿真环境，模仿学习；
    - **移动机器人**扩展了固定机械臂的使用场景，具有移动底座。真实移动机器人的专家数据集，家务操作；
    - **灵巧手**是一种新兴的具身实体类型，用来执行复杂的灵巧操作任务，仿真，灵巧手示教数据集，模仿学习；
    - **人形机器人**，人形机器人的传感器，人形机器人的策略数据集，人形机器人的仿真环境，人形机器人的专家数据集，人形机器人的模仿学习。
  - 在**传感器**方面，具身机器人使用的传感器主要包括视觉传感器、机器人本体感知（proprioception）传感器、触觉/力学（haptic/tactile perception）等传感器、GPS和IMU传感器等。如下图（a）所示。。![机器人传感器](<截屏2024-12-17 21.18.03.png>)
    - **大模型驱动的具身智能中由于大模型主要用于处理视觉和语言，因此对视觉传感器较为关注**，如上图（b）所示，通过视觉传感器或仿真环境中的视觉渲染功能、可以获得**RGB、深度图、语义分割图、点云**等视觉输入信息;
    - **机器人的本体感知传感器**根据机器人类型而有所不同. 例如,
      - **机械臂的本体感知包括各个关节的位置、速度、扭矩和机械臂末端位置**等;
      - **四足机器人的本体感知信息包括自身线速度、角速度, 身体关节位置、力传感器等**各种传感器信息, 动作通常设置为身体关节的目标位置等;
      - **灵巧手的本体感知包括关节位置、速度、力矩, 指尖位置、力传感, 手腕的位置与角度等**.
    - **触觉/力学传感器**主要用于灵巧手和人形机器人手部关节中, 用于灵巧手感受抓握物体的力度, 估计物体的形状等. 
    - **GPS/IMU传感器**在机器人运动中进行定位, 多用于移动机器人、人形机器人等.
    - 此外, 由于仿真器中可以获取预设的环境参数, 仿真环境中机器人常拥有 “特权” 观测. 例如, 仿真机械臂抓取任务中可以获得准确的抓取点或物体位置, 仿真四足机器人运动中可以获取地形信息和环境摩擦参数等. 特权信息能够帮助策略在仿真环境中快速收敛, 但在真实环境中是缺失的, 需要通过额外传感器或特权信息蒸馏等架构来进行仿真-真实 (sim-to-real) 的策略迁移.
  - 在**执行器**方面，根据机器人类型的不同，机器人底层分别采用不同的控制器进行执行。
    - 在机械臂中, 常见的动作输出有关节位置、**$\delta$关节位置、末端执行器姿态和$\delta$末端执行器姿态**等. 通过控制器驱动相应电机来进行关节位置、速度、扭矩的调整, 从而执行相应的动作;
    - 四足机器人的动作一般为**足部关节的位置**, 通过PD控制器来产生相应的电机扭矩;
    - 灵巧手的动作一般为**手指关节的力矩、手腕的位置、方向**等, 随后转换为关节扭矩进行执行. 人形机器人拥有更多的关节和更高的自由度, 一般采 用多个控制器分别控制手部关节、双臂系统、腿部关节、躯干等. 一般而言, 大模型更关注于高层的任 务执行和动作选择, 底层由传统的执行器进行执行.
- 具身智能的**任务**由常见的抓取任务逐步演变为更加复杂的任务集合。
  - 在**机械臂和移动机器人**中，仿真平台ManniSkill2任务集合包括精细操作任务，如刚性物体操作、铰链物体操作（开门、开箱）等、柔性物体操作（倒水、挂毛巾）等。其中铰链物体和柔性物体都具有复杂的模型，在策略学习中需要隐式估计的位移和形变；
  - **四足机器人**学习的目标是适应不同的地形，同时有研究使四足机器人学习不同的步态；
  - **灵巧手**用于测试的任务包括模仿、旋转物体、开门、倒水等；
  - **人形机器人**由于包含了双足运动、双臂操作、双灵巧手等系统，可执行的任务更为多样，几乎可以涵盖现有具身实体的所有任务集。

> 为了和人类进行更好的交互, 许多任务包含了语言描述的目标, 如 VIMA, Calvin等, 常见的语言任务描述如 “将红色方块放进抽屉”, “按下蓝色按钮打开柜子” 等, 任务的执行情况使用执行成功率和时间效率进行衡量.
- 具身智能任务所使用的**仿真环境**已经在对不同具身实体的介绍中进行了详细介绍。目前仿真环境的发展仍然十分迅速。

### 具身智能学习框架
常用的具身智能学习框架主要包括模仿学习、强化学习、模型预测控制等。具身智能的策略学习问题可以描述为马尔科夫决策过程（Markov Decision Process，MDP），表示为$M=(\mathcal{S},\mathcal{O},\mathcal{A},\mathcal{P},r,\mathcal{L})$。其中
1. $\mathcal{O}$为**观测空间**，用于表示机器人传感器的输入。观测空间可能包含了视觉观测、本体感知、触/力传感、定位信息等，取决于具体的机器人硬件设置和任务设置。一般而言，多种传感器之间会存在较多的信息冗余；
2. $\mathcal{S}$为状态空间，由历史的观测和当前观测进行**感知学习**的方法获取。视觉传感器可以捕捉周围环境的图像和视频信息，触觉传感器能够让机器人感知物体的形状、质地和温度，力学传感器则可以测量机器人在执行任务时施加的力和收到的反作用力。
   1. **感知学习**也包括对自身状态的感知，机器人需要了解自身的姿态、位置和运动状态，以便在执行任务时做出正确的决策；
   2. 一般人认为状态具有**马尔科夫性**（Markov property），即机器人后续的状态将完全由当前状态和动作决定，和历史状态无关。
3. $\mathcal{A}$为**动作空间**， 不同机器人的动作空间定义不同，一般为机械臂末端姿态或关节位置等；
4. $\mathcal{P}(s'|s,a)$为**状态转移函数**，表示在当前状态和动作下，智能体到达下一个状态$s'$的概率，是由**机器人动力学和环境**决定的。传统的方法一般假设已知系统的动力学方程或使用线性函数进行动力学建模，而在复杂的具身执行系统中动力学方程是较为复杂的，往往需要强大的**神经网络结构或大模型**来进行构建；
5. $r(s,a)$为**奖励函数**，用于衡量动作执行后得到的下一个状态$s'$的好坏，例如当前机械臂末端和目标位置的距离，四足机器人的行进速度和目标速度的差距等，一般用于**强化学习和模型预测控制**中。宏观上，奖励函数作为环境的反馈来帮助策略进行迭代，强化学习算法通过奖励反馈进行策略优化，以期望在未来获得更大的奖励；
6. **任务描述空间**$\mathcal{L}$一般使用**自然语言**的方式进行任务的描述，方便使用大模型进行理解和规划。MDP中的奖励函数和任务描述密切相关，例如“关闭抽屉”和“打开抽屉”的奖励函数在数值上具有较大差异。智能体策略表示为$\pi(a|s,l)$，**策略**根据任务描述和当前状态采取合适的动作。
   
#### 模仿学习
行为克隆（behavior cloning，BC）是模仿学习的基本框架，机器人的策略$\pi(a|s,l)$通过模仿专家数据得到。专家数据集表示为$\mathcal{D}=\{\tau_i,l_i\}_{i\in [N]}$，其中$\tau_i=[s_0,a_0,s_1,a_1,...,s_{t-1},a_{t-1},s_t]$表示一个专家轨迹，$l_i$表示该轨迹对应的任务描述。行为克隆的损失函数表示为：$$ \mathcal{L}(\theta):=-\mathbb{E}_{(\tau,l)\sim D}\left[\sum_{t=0}^{T-1}log \pi_{\theta}(a_t|s_t,l)\right] $$模仿学习的优势在于不需要建模环境的状态转移，今需要通过专家的状态-动作集合可以直接进行策略学习，如下图所示。![alt text](<截屏2024-12-18 09.26.13.png>)
模仿学习经过数十年的发展，其的细分领域主要有：
##### 分布偏移问题
模仿学习存在固有的**分布偏移问题**，在模仿策略的实际执行中会导致**泛化误差**，如上图(b)所示。具体来说，专家数据$\mathcal{D}$的状态是**有限**的，当智能体在执行任务时遇到新的状态$s_t^{new}$和数据集中的状态差距较大，则模仿学习策略$\pi(a|s,l)$会出现**不可预估的动作**，环境转移到下一个新状态$s_{t+1}^{new}$，交互轨迹和数据集轨迹产生分布偏移。随着分布偏移的积累，策略在真实环境中的轨迹会大大偏离数据集中的专家轨迹，从而由**泛化能力不足**导致任务失败。专家介入可能是解决分布偏移问题的有效手段。DAgger算法在策略执行过程中由专家进行介入，**对新采集的状态进行专家动作标记**，从而扩大数据对状态和动作空间的覆盖。LazyDAgger，SafeDAgger，ThriftyDAgger显著降低了专家介入的次数。
##### 动作分布建模
人类专家的策略往往较为复杂，产生的轨迹往往呈现出随机性和多模态。为建模复杂的人类专家策略，使用表达能力较强的**生成模型来进行策略建模**是一种选择——使用**生成对抗网络和扩散模型**来建模复杂的**策略分布**。GAIL提出使用GAN对动作分布进行建模，使用生成器建模策略分布，使用判别器来判断动作是否来源于专家动作。根据生成对抗网络的损失，生成器输出的动作将逐步接近于复杂的专家动作。扩散生成模型近期也被用于建模人类专家或离线数据集的动作分布，使用**多步的逆向去噪过程**，由随机噪声恢复出多模态的专家动作。
##### 无动作轨迹模仿
许多机器人模仿的数据没有记录动作，仅记录了机器人的**轨迹**。如互联网上的大量机器人操作物体的视频。如何**从无动作标记的轨迹中进行策略模仿学习**？VPT使用**逆环境模**为大量的无动作标记数据打上动作标签，随后使用**模仿学习**方法进行策略学习。其他方法通过学习整体状态和专家轨迹之间的**相似度**，将其作为奖励函数引导智能体产生和专家轨迹相似的轨迹。度量有：在**图像表征空间中的相似度**，**最优传输机制导出的相似度矩阵**等，从专家轨迹中挖掘奖励函数后使用**强化学习**算法进行优化。APV和GR-1设计了一种视频预测的结构，从大量无动作标签中构建视频预测模型。
##### 逆强化学习
**逆强化学习**旨在从观察到的智能体的行为中推断出其背后的**潜在目标或奖励函数**。早起的逆强化学习算法为边际类算法，旨在从**专家演示中学习奖励函数**，获得与专家策略相近的策略，随后**基于熵的逆强化学习**产生了最大熵逆强化学习、相对熵逆强化学习、贝叶斯逆强化学习等。在**深度学习的背景**下，结合**神经网络的感知力**和**强化学习的决策力**提出基于神经网络的逆强化学习，包含学徒学习逆强化学习、最大边际规划逆强化学习、最大熵深度逆强化学习、生成对抗逆强化学习等。通过逆强化学习，智能体可以从专家的总为中学习，并在没有明确奖励信号的情况下做出智能决策，使智能体更好的适应复杂和不确定的环境，提高其决策能力和智能水平。

#### 强化学习
**强化学习**的目标是通过智能体与环境的交互来最大化奖励。在时间步t，智能体根据当前的环境状态$s_t$来选择动作$a_t$，环境通过执行该动作得到下一个状态$s_(t+1)$，并反馈给智能体一个奖励信号$r_(t+1)$。定义t时刻的“回报”为从该时间步开始到周期结束的”折扣“奖励之和，形式化为：$$ R_t = r_t+\gamma r_(t+1) + \gamma^2 r_(t+2) + ... + \gamma ^(T-t-1)r_(T-1). $$其中，奖励权重随着时间步以$\gamma$的速度衰减。由于策略和环境存在随机性，从$(s_t,a_t)$出发按照同一策略进行交互，每次得到的$R_t$往往是不同。强化学习使用值函数（value function）表示多次交互中的回报水平。$(s,a)$在策略$\pi$下的值函数$Q^\pi (s,a)$衡量了从该状态动作出发按照策略$\pi$选择动作，在整个周期结束时获得的回报的期望。$$ Q^\pi (s,a) = \mathbb{E}_{\pi}[R_t|s_t=s,a_t=a] = \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k r_(r+k+1)|s_t=s,a_t=a \right]. $$。$Q^\pi (s,a)$又被称为动作值函数。对$Q^\pi (s,a)$在动作上取期望，可获得状态值函数$V^\pi (s)=\mathbb{E}\left[ Q^\pi (s,a) \right]$。在**有限动作空间**下，可以根据值函数的估计选择最优动作$a_t=argmax_a Q(s_t,a)$。在估计值函数中，值函数可以根据定义写为$Q(s,a)=\mathbb{E}[r+\gamma\mathbb{E}_{s'\sim p(s'|s,a),a'\sim \pi(a'|s')}Q(s',a')]$，使用立即奖励r和下一个时间步的Q值来更新当前时间步的Q值估计，称为**时序差分（temporal-difference，TD）估计法**。强化学习结构和基本算法分类如下图所示。![alt text](<截屏2024-12-21 08.43.00.png>)强化学习研究的算法分支有如下几种，这些分支均可以和发模型结合并在具身智能中使用。

##### 值函数学习
Q-学习（Q-learning）是一种**时序差分**为基础的**最优值函数求解方法**，在计算$Q_{target}$时选择使$Q(s',a')$最大的动作，即$Q_{target}=r_{t+1}+\gamma max_{a'} Q(s_{t+1},a')$，随后使用神经网络表示，网络权重记为$\theta$。在Q学习中，根据时序差分方法的思路，当前时刻的Q值可以根据立即奖励和下一时刻的Q值来得到。深度Q学习在基本原理的基础上，使用包括**深度Q网络、经验池和目标网络**来提升优化的稳定性。在此基础上，许多方法分别从解决值函数的高估问题，提升值函数学习样本的效率，提升策略的探索能力等方面进行了探索。
##### 策略学习
**策略梯度法**是强化学习进行策略求解的另一类重要方法。策略梯度法中对累积奖励的梯度进行求解，表示为$\nabla_\theta J(\pi_\theta)$，形式化为$$\nabla_\theta J(\pi_\theta)=\nabla_\theta \mathbb{E}_{\tau\sim P(\tau|\theta)}[R(\tau)]=\nabla_\theta \int_\tau\nabla_\theta P(\tau|\theta)R(\tau) $$其中$\nabla_\theta P(\tau|\theta)=P(\tau|\theta)\nabla_\theta \log P(\tau|\theta)$，且$$log P(\tau|\theta)=\log p(s_0)+\sum \log P(s+t|s_t,a_t)+\log \pi_\theta (a_t|s_t)$$带入后得到策略梯度的形式为$\nabla_\theta J(\pi_\theta)=\mathbb{E}_{\tau\sim P(\tau|\theta)}\left[\sum_{t=0}^{T} \nabla_\theta \log\pi_\theta (a_t|s_t)\right].$策略梯度法引入基线来降估计的方差。当使用$V^\pi (s_t)$作为基线时，梯度中$Q^\pi (s_t,a_t)$的项转化为$Q^\pi (s_t,a_t)=Q^\pi (s_t,a_t)-V^\pi (s_t)$，其中，$A^\pi(s_t,a_t)=Q^\pi (s_t,a_t)-V^\pi (s_t)=r+\gamma V^\pi (s_{t+1})-V^\pi(s_t)$定义为优势函数（advantage）因此策略梯度可以写成$\nabla_\theta J(\pi_\theta)=\mathbb{E}_{\tau\sim \pi_\theta}\left[\sum_{t=0}^{T} A_\phi(s_t,a_t)\nabla_\theta \log\pi_\theta(a_t,s_t)\right]$
**近端策略优化**（proximal policy optimization，PPO）从基本的策略梯度法中发展而来，具体有较好的稳定性和广泛的应用。PPO引入需要描述的两个策略$\pi_{\theta_{old}}$和$\pi_\theta$，前者是上一个时间步的策略，后者时当前策略。PPO一方面要求$\pi_\theta$相对于策略$\pi_{\theta_{old}}$能有稳定的性能提升，另一方面要求二者之间的距离不能太远。PPO算法在具身智能任务和大模型中有着广泛的应用。
##### 基于模型的方法
基于模型的方法希望智能体具有规划的能力，在做出行为之前能够**提前预测**执行该行为会带来的后果，从而快速找到最优动作。基于模型的方法需要建立环境模型，该模型是利用真实交互数据拟合而来的，模型输入状动作对$s,a$，输出$s'$的预测。Dyna-Q是一种典型的基于模型的强化学习的方法，假设换机型是确定的并且状态动作空间是离散可数的，这样可以吧环境模型想象成一个类似Q函数的表格，利用环境模型产生了虚拟样本进行额外的Q函数损失，加速了Q函数的收敛。基于模型的方法的主要问题是，模型预测误差会在模型向后推演中积累，从而对值函数学习产生了负面影响。MBPO等方法分析了误差累积导致的性能差距，IVE算法分析了模型预测过程的不确定性，Dreamer算法通过隐空间的环境建模进行多步预测和推理。
##### 离线强化学习
强化学习中智能体与环境的交互代价较高，同时具身智能任务重许多场景难以进行在线的样本收集。解决思路：**利用已有数据进行学习**，称为“离线”强化学习。数据集可以来自其他智能体采集的数据或由人类采集的数据。数据集数量和质量有限，无法覆盖现实世界的状态空间和动作空间。离线强化学习致力于**解决使用固定数据集来进行策略学习中出现的问题**。现有方法通过策略约束、不确定性估计、悲观值函数等思路，限制策略对离线数据集所覆盖以外的动作的访问来解决该问题。

#### 模型预测控制
在**模型预测控制**（MPC）中，环境模型可以使智能体无需与环境交互而得到下一步状态和奖励。利用环境模型进行推演，从而选择最优的动作，即找到如下序列：$$(a_t,a_{t+1},...,a_{t+H-1})=\argmax_{a_t,a_{t+1},...,a_{t+H-1}}\sum_{t'=1}^{t+H-1} \gamma^{t'-t}r(s_{t'},a_{t'}). $$随后，智能体将执行第一步的动作$a_t$。执行后再次使用规划方法得到下一步的最优规划动作。
##### 随机打靶法
**随机打靶法**（random shooting）随机采样帮助智能体决策。给出当前状态$s_0$和长度为T+1的随机动作序列$[a_0,a_1,...,a_{T-1}]$，利用训练的环境模型得到仿真轨迹$[s_0,a_0,\hat{r}_0,\hat{s_1},a_1,\hat{r}_1,...,\hat{s}_{T-1},a_{T-1},\hat{r}_{T-1},\hat{s}_T,a_T,\hat{r}_T]$，通过多次采样随机动作序列$[a_0,a_1,...,a_{T-1}]$，可以得到不同的状态动作对的值函数估计，记为$\hat{Q}(s,a)=\sum_{t=0}^{T}\gamma^t \hat{r}_t$，然后根据$\pi(s)=argmax_a \hat{Q}(s,a)$选择下一步的动作。智能体执行动作$\pi(s)$之后，再次重复采样过程进行规划。随机打靶法的缺点是不同轨迹方差较大，可能无法采样到高回报的动作。**交叉熵方法**（cross-entropy method，CEM）进行采样。CEM不使用随机的动作序列，而是**从某个动作分布中采样动作序列，根据动作取得的累积回报调整动作采样的分布**，这样有更大概率采样到高回报的动作序。
##### 集成概率轨迹采样法
**集成概率轨迹采样法**（probabilistic ensembles with trajectory sampling, PETS）是对随机打靶法的改进。将**不确定性感知的概率环境模型**和**轨迹采样**相结合，实现了和无模型强化学习方法接近的效果。PETS可以衡量任意不确定性（aleatic uncertainity）和认知不确定性（epistemic uncertainty），其中任意不确定性是由随机系统本身带来的不确定性, 比如观测噪声或 状态转移噪声; 认知不确定性是指由于数据缺失带来的不确定性, 可以随着训练数据量的增加而减少. PETS使用了**集成概率环境模型方法**，建立了若干个概率环境模型，每个环境模型使用参数化高斯分布来拟合。PETS使用基于 CEM的随机打靶法的方式进行规划, 可以区分两种不确定性, 任意不确定性使用**一条轨迹的预测方差**来衡量, 而认知不确定性通过**多条轨迹之间的预测方差**得到, 从而对环境未来的预测更加全面.

#### 具身框架和认知智能的联系
- 认知智能是指人类在感知、理解、记忆、推理、判断和解决问题等方面的能力, 涉及从外部环境中获取信息, 通过**大脑**处理这些信息并形成对世界的认识. 
- 具身智能理论强调, 认知过程不仅仅依赖于大脑, 而是与**整个身体及其与环境的互动**密切相关. 
- 身体的动作、感知和环境的交互对认知能力的发展和实现具有重要的影响和作用. 在具身智能的框架下, 认知智能通过身体与环境的动态交互形成. 例如, 触觉和运动感知帮助人们理解空间关系和物体属性, 这些直接的感知体验进一步影响复杂的认知过程, 如推理和决策. 在具身智能的框架下, 认知智能不仅仅是大脑的产物, 而是通过身体与环境的动态交互形成的. 人类通过身体的感知与运动获取信息, 这些信息不仅帮助形成对周围世界的理 解, 还影响思维和判断过程.

### 大模型技术
#### 大语言模型
语言模型从统计学角度计算词序列的联合概率分布。
- 2018 **BERT语言模型**使用双向Transformer结构，设计掩码预测和未来句子预测等自监督与训练任务进行语言模型的预训练；
- 2020 OpenAI提出GPT-3模型，使用更多餐数量进行模型的预训练，上下文长度单元扩展到2048token，每个注意力包含了96个注意力头；
- 近年来，OpenAI提出ChatGPT：
  - 训练过程分为三个阶段：
    - **基础语言模型的预训练**，大量语料库构建无监督预测任务，长序列模型预测训练；
    - **人工标注数据和开元数据进行指令微调**，使模型输出符合问答习惯；
    - **使用人类偏好数据训练奖励模型**，使用强化学习中的PPO策略梯度算法对语言模型进行优化，使模型的输出和人类偏好对齐。
- ChatGPT-4.

#### 视觉基础模型
视觉基础模型一般以卷积神经网络（ResNet等）或视觉Transformer（ViT）等为基础模型，通过自监督学习的方式提取图像的特征表示，随后将特征提取器用于下游任务中。
- MAE，自监督学习，使用掩码后的视觉输入还原原是图像，可以从大量无标价的图像中进行预训练；
- CLIP，使用大量配对的图像和文本描述来学习图像的语义特征，获得和**文本理解匹配的视觉特征**；
- Segment Anything（SAM），视觉分割大模型。从给定语言或视觉提示从图像中分割出对应的区域：
  - SAM使用1100万张图像的大规模数据进行预训练，具有很强的语义理解和泛化能力；
  - 少量数据微调解决医学图像分割问题等。

#### 视觉-语言模型
视觉-语言模型 (VLM) 同时融合了大语言模型和视觉基础模型, 使模型能够同时接收图像和语言作为输入, 并根据语言指令和图像信息产生输出, 进行图像问答任务。
- **SimVLM**，提出一种编码-解码结构，将图像和文本输入编码器，从解码器中输出后续文本或回复。即text+image 2 text；
- **BLIP2**，预训练的图像编码器提取特征，再经过**QFormer结构**从冻结的编码器中提取视觉特征。*QFormer使用多模态信息匹配的方式进行训练，从而对齐视觉和文本表征。提取的特征经过线性投影后和语言指令作为大语言模型的输入；
- **Flamingo**，使用类似结构，使用烧炼例子作为输入使模型具备少样本泛化能力。

下图对视觉模型和VLM结构进行了对比。![alt text](<截屏2024-12-23 09.42.03.png>)
此外，LLaVa，Mini-GPT4，instructBLIP等模型都是视觉-语言模型。Video-Chat，VideoLLaMa等将图片输入扩展为视频，使大模型能够根据语言指令和视频输入进行问答。训练中使用**类似QFormer的结构**进行模态转换。

### 生成式大模型
**扩散模型**在生成高质量图像方面已经超出了传统的VAE，显式概率模型网络，GAN。真实图像记为$x^0$，扩散模型参数化的扩散过程函数为$p_\theta(x^0)=\int p(x^T)\prod_{t=1}^T p_\theta(x^{t-1}|x^{t})|x^{1:T}$，使用马尔可夫过程将高斯噪声$x^T=\mathcal{N}(0,\mathbf{I})$通过T个时间步转换为原始图像$x^0$。**去噪序列**$x^{0:T}$是一个人马尔可夫链，每一个时间步产生的中间图像用参数话的高斯分布表示：$p_\theta(x^{t-1}|x^{t})=\mathcal{N}(\mu_\theta(x^{t},t),\Sigma_\theta(x^{t},t))$，其中$\mu_\theta(x^{t},t)$和$\sigma_\theta(x^{t},t)$是参数化的函数，$x^{t-1}$是前一个时间步的图像，$x^{t}$是当前时间步的图像。**加噪过程**使用固定参数的分布逐步添加高斯噪声：$x^{t}=\sqrt{\alpha^t}x^{t-1}+\sqrt{1-\alpha^t}\epsilon_t$，其中$\alpha^t=1-\beta^t,\epsilon_t\sim\mathcal{N}(0,\mathbf{I})$。根据贝叶斯定理，可以推导逆向过程的解为$$q(x^{t-1}|x^{t})=\mathcal{N}(\frac{1}{\sqrt{\alpha^t}}(x^{t}-\frac{\beta^t}{\sqrt{1-\bar{\alpha}^t}}\epsilon(x^t,t)),\beta^t\frac{1-\bar{\alpha}^{t-1}}{1-\bar{\alpha}^t})$$
其中，\(\bar{\alpha}^t = \prod_{i = 1}^{t} \alpha^i\)。扩散模型通过最大化概率的变分下界（ELBO）训练，损失函数可以简化为$$\mathcal{L}(\theta) = \mathbb{E}_{x_0, \epsilon, t} \left[ \left\| \epsilon - \epsilon_{\theta} \left( \sqrt{\bar{\alpha}^t} x^0 + \left( \sqrt{1 - \bar{\alpha}^t} \right) \epsilon, t \right) \right\|^2 \right].$$其中$\epsilon_\theta$为参数化的网络模型。扩散模型已经在可控图像生成，文生图等方面有着广泛的应用。DALL-E，Sora等在图像和视频生成方面应用成功，在具身智能领域，扩散模型已经被应用于强化学习策略、动作规划、图像目标生成、轨迹生成等方面。

**多模态生成大模型**（multimodal generative models）能够处理和生成多宗类型的数据（txt，image，video，audio等），从而实现更加复杂的内容创作。
- DALL_E：大规模训练，文本和图像之间的关系，从文本生成高质量图像。
- Video-LaVIT：通过**自回归方式**预测下一个图像或文本词元, 在统一的生成目标下同时处理图像和文本. 同时, 该模型提出了动态长度的离散视觉词元, 减少了图像块之间的相互依赖性, 增强了在大型语言模型中图像和文本表示的兼容性. 

多模态生成大模型可以用于模拟复杂的环境和任务场景, 为具身智能体提供丰富的训练数据, 提高其在真实环境中的适应性和鲁棒性.
## 大模型驱动的具身环境感知
### 图像观测特征学习
传统视觉观测特征提取方法主要包含四个方面：
1. **数据增广**。在强化学习的值函数或策略学习中，使用图像剪裁、移动、对抗增强等方式进行数据增广，对原有图像状态进行扩充$s\rightarrow \{s_{aug}\}$ . 增强值函数或策略函数在状态邻域内的平滑性；
2. **对比学习**。自监督视觉特征提取方法，在具身智能中常使用智能轨迹构建正负样本；
3. **环境模型**。通过当前状态表征和动作来重建下一个状态表征，能够提取与环境转移相关的表征，而忽略背景等无关的要素。**互模拟（bisimulation）算法**；
4. **值函数学习**。相比于单步环境模型的预测，**值函数**能获取较长时中未来的奖励的环境转移信息，通过从离线数据集中学习值函数的预测能得到有意义的视觉表征。

传统方法的不足之处：
1. 新环境和任务中，需要使用这些表征学习的目标进行重新训练，具有**较高的计算代价**；
2. 表征主要针对特征任务和环境，在多任务，多样化环境中的**泛化能力差**；
3. 由于学习表征所用的训练数据有限，表征的鲁棒性较差，容易受**环境扰动**的影响。

解决以上问题，有方法引入**视觉预训练模型**来提升表征的泛化性和鲁棒性，基本结构如下图所示：
![alt text](<截屏2024-12-25 22.33.20.png>)
- **PIE-G算法**。将预训练的ResNet编码器**对视觉观测进行编码, 使用编码后的向量学习值函数和策略**, 大大提升了策略的视觉泛化能力. 
- **PVR**对自然图像中学习得到的监督和自监督表征在多种强化学习框架下进行了对比, 结果表明使用预训练表征能够在大部分场景中提升具身策略的效果. 
- **ViGen**对比了**传统的表征学习方法和PIE-G**在机械臂操作、导航、灵巧手操作中的性能, 并使用背景、光照、相机视角、场景结构变换的方式的测试视觉编码器的鲁棒性, 结果表明预训练表征在能够在平均水平上提升策略的**鲁棒性**. 
  - 在常规的图像预训练数据集 (如 ImageNet等) 之外, 具身智能的表征学习中常用**第一人称视角的人类物体操作**数据集. 主要原因是人类操作的视频数据量大, 且涵盖了人类日常生活中场景的物体类型和交互方式, 同时具有语言标注. 人类第一人称示教视频和机械臂操作视频有很大的相似性, 容易学习到可迁移的表征, 常用的人类操作数据集有 Ego4D, Epic-Kitchen, Something-something等. 
- **R3M**。是一个专为机械臂操作任务设计的**预训练视觉提取器**, 使用ResNet作为基本结构, 使用视频序列的**时序关系和语言-视频的相关关系**构造对比学习损失. R3M在大规模Ego4d 数据集中进行训练, 得到的预训练模型能够和模仿学习或强化学习算法集合, 直接用于机械臂场景决策. 在常用的机械臂操作任务集合中, R3M 能够获得超越 CLIP, MVP等预训练模型的效果, 表明 R3M 能够从人类操作视频中能够提取机械臂操作相关的可迁移知识. 
- **VIP**在人类数据集上学习表征, 将表征学习问题转换为目标导向的强化学习问题, 通过对比学习实现了隐式的值函数学习, 在下游任务中能同时产生状态表征和值函数估计. 
- **Voltron模型**使用**语言描述和相应的人类操作视频**作为输入, 使用 MAE 学习目标从掩码图像中恢复原始图像, 从中学习**低层次的语义特征**. 同时, Voltron **使用掩码上下文图像信息预测视频对应的文本描述**, 从而获取**高层的人类意图的相关特征**. 
- **Huo等提出了人类导向**的表征学习方法, 从人类交互数据中进行状态变化分类、时序定位、目标检测、手部检测等任务的构造, 通过学习这些辅助任务来获得有意义的表征. 
- **RoboMP2算法**通过**微调多模态大模型**实现基于物体属性、空间关系和知识推理的物体定位, 从而根据定位产生规划.

对现有基准方法进行了对比和分析. ExploreVisual在不同的训练数据、模型架构、预训练目标上得到的表征对下游任务在策略学习中的影响, 结果表明: 
1. **人类交互数据** (如 Ego4D) 相比于自然图像能够更好的挖掘和具身决策相关的特征;
2. **ResNet** 相比于 ViT 结构能够更好的保留能够在下游任务泛化的特征; 
3. **图像增强和对比学习**相比于其他自监督任务能够更好的挖掘具身场景特征. 

在此基础上, 提出了 **Vi-PRoM 算法**, 结合对比学习、视觉语义预测和力学预测获得可泛化的具身特征提取器. Hansen 等对比了现有大模型驱动的视觉特征预训练方法和直接策略学习方法在不同背景、光照、颜色变化下的视觉泛化能力, 结果表明**使用视觉预训练方法虽然能够提升学习效率, 但相比于直接策略学习方法在最终性能上并没有明显的优势**, 表明挖掘对具身智能体具有更强泛化能力的表征仍然具有很大的挑战.

### Afforddance提取
预训练的视觉表征虽然能够在一定程度上提取上提升策略的泛化能力, 但紧凑的表征往往**缺乏可解释性**, 很难理解表征中编码了哪些信息. 
> 机器人的 **Affordance** 是一种对操作任务更具有解释性的通用特性. 具体地, Affordance 一般指机器人和物体交互时, 物体表现出的“**怎样使用**”的性质, 即 “被交互的物体应该怎么使用”, 代表对于其功能的视觉提示, 比如 “茶壶手柄是被握着的” “门是从外向里推开的” 等等. 在机器人场景中, 获取物体的交互方式能够给机器人更加直观的提示, **使机器人在交互中遵循 Affordance 的提示, 避免了完全通过尝试解决任务**.

在Affordance之前，有更粗粒度角度的方法获取场景的相关知识。
- **VIMA**使用预训练的目标检测器对场景中的物体进行检测，随后将分割出的物体区域进行编码作为策略的输入。目标检测能使策略直接关注到所抓取物体的信息，不需要通过模仿学习损失进行目标相关的特征提取；；
- **Instruct2Act**是使用了最新开放物体检测模型（包括SAM，CLIP物体检索等），根据任务指令从相对应的区域进行检索，随后通过坐标关系获取三维物体空间位置，使用机械臂直接进行抓取。

然而这些方法只提供了粗粒度的物体位置信息，缺乏“怎样使用”物体这些细粒度的信息，在复杂功能物体和铰链物体操作中存在不足。现有研究将Afordance作为一种细粒度的物体功能描述，一般包含两个方面：
1. 物体的**交互位置**（如门把手，茶壶手柄等）；
2. 物体的**交互轨迹**（如向内推，向上提等）。

Afordance描述对于复杂功能物体或铰链物体操作是非常重要的提示，可以直接指导机械臂的行为。
> 由于人类操作视频中广泛存在人类和物体的交互模式, 可以考虑**从人类操作视频中挖掘物体的 Affordance**. 具体地, 该方法根据人类操作视频构建预测模型来预测一系列动作 (如可推拉, 可拿起, 可倾倒等) , 随后将预测模型用于估计非活动对象是如何进行交互的.

- **TSC算法**在人类操作视频中检测人手交互对象和接触状态来提供监督信号, 随后**屏蔽手部位置**构建预测模型对监督信号进行建模, 从而对静止操作物体产生接触点和作用方式的预测.
- **OCT算法**从人类交互视频中同时学习**人手轨迹**和**人手与物体未来的接触点**, 并提出了一种自动化方法来对大量视频数据进行标记, 构建了以目标为中心的Transformer结构来对这些信息进行建模.
- **VRB算法**。从人类第一视角数据中预测忍受和物体的接触点，以及在接触后对物体的操作方向；
- **Mimicplay算法**通过人类示教数据来检测忍受操作物体的轨迹，并将该轨迹作为隐示的规划结果迁移到机器人策略中，从而引导机器人解决长周期的复杂决策任务。
- **ATM算法**构建了更加精细的轨迹预测模型, 该方法不仅检测视频中机械臂/人手和物体接触点的运动轨迹, 而可以直接预测视频帧内任意点的未来轨迹, 从而为机械臂操作提供更详细的控制指导, 用最少的动作标记数据学习稳健的视觉运动策略. 

下图可视化了代表性的 Affordance信息.![alt text](<截屏2025-01-14 10.34.47.png>)
