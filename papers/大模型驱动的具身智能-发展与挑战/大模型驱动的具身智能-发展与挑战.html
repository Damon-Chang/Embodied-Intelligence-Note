<!DOCTYPE html><html><head>
      <title>大模型驱动的具身智能-发展与挑战</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/damonchang/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.8.15/crossnote/dependencies/katex/katex.min.css">
      
      
      
      
      
      <style>
      code[class*=language-],pre[class*=language-]{color:#333;background:0 0;font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.4;-moz-tab-size:8;-o-tab-size:8;tab-size:8;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none}pre[class*=language-]{padding:.8em;overflow:auto;border-radius:3px;background:#f5f5f5}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal;background:#f5f5f5}.token.blockquote,.token.comment{color:#969896}.token.cdata{color:#183691}.token.doctype,.token.macro.property,.token.punctuation,.token.variable{color:#333}.token.builtin,.token.important,.token.keyword,.token.operator,.token.rule{color:#a71d5d}.token.attr-value,.token.regex,.token.string,.token.url{color:#183691}.token.atrule,.token.boolean,.token.code,.token.command,.token.constant,.token.entity,.token.number,.token.property,.token.symbol{color:#0086b3}.token.prolog,.token.selector,.token.tag{color:#63a35c}.token.attr-name,.token.class,.token.class-name,.token.function,.token.id,.token.namespace,.token.pseudo-class,.token.pseudo-element,.token.url-reference .token.variable{color:#795da3}.token.entity{cursor:help}.token.title,.token.title .token.punctuation{font-weight:700;color:#1d3e81}.token.list{color:#ed6a43}.token.inserted{background-color:#eaffea;color:#55a532}.token.deleted{background-color:#ffecec;color:#bd2c00}.token.bold{font-weight:700}.token.italic{font-style:italic}.language-json .token.property{color:#183691}.language-markup .token.tag .token.punctuation{color:#333}.language-css .token.function,code.language-css{color:#0086b3}.language-yaml .token.atrule{color:#63a35c}code.language-yaml{color:#183691}.language-ruby .token.function{color:#333}.language-markdown .token.url{color:#795da3}.language-makefile .token.symbol{color:#795da3}.language-makefile .token.variable{color:#183691}.language-makefile .token.builtin{color:#0086b3}.language-bash .token.keyword{color:#0086b3}pre[data-line]{position:relative;padding:1em 0 1em 3em}pre[data-line] .line-highlight-wrapper{position:absolute;top:0;left:0;background-color:transparent;display:block;width:100%}pre[data-line] .line-highlight{position:absolute;left:0;right:0;padding:inherit 0;margin-top:1em;background:hsla(24,20%,50%,.08);background:linear-gradient(to right,hsla(24,20%,50%,.1) 70%,hsla(24,20%,50%,0));pointer-events:none;line-height:inherit;white-space:pre}pre[data-line] .line-highlight:before,pre[data-line] .line-highlight[data-end]:after{content:attr(data-start);position:absolute;top:.4em;left:.6em;min-width:1em;padding:0 .5em;background-color:hsla(24,20%,50%,.4);color:#f4f1ef;font:bold 65%/1.5 sans-serif;text-align:center;vertical-align:.3em;border-radius:999px;text-shadow:none;box-shadow:0 1px #fff}pre[data-line] .line-highlight[data-end]:after{content:attr(data-end);top:auto;bottom:.4em}html body{font-family:'Helvetica Neue',Helvetica,'Segoe UI',Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ol,html body>ul{margin-bottom:16px}html body ol,html body ul{padding-left:2em}html body ol.no-list,html body ul.no-list{padding:0;list-style-type:none}html body ol ol,html body ol ul,html body ul ol,html body ul ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:700;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::after,html body code::before{letter-spacing:-.2em;content:'\00a0'}html body pre>code{padding:0;margin:0;word-break:normal;white-space:pre;background:0 0;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:after,html body pre code:before,html body pre tt:after,html body pre tt:before{content:normal}html body blockquote,html body dl,html body ol,html body p,html body pre,html body ul{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body code,html body pre{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview ul{list-style:disc}.markdown-preview ul ul{list-style:circle}.markdown-preview ul ul ul{list-style:square}.markdown-preview ol{list-style:decimal}.markdown-preview ol ol,.markdown-preview ul ol{list-style-type:lower-roman}.markdown-preview ol ol ol,.markdown-preview ol ul ol,.markdown-preview ul ol ol,.markdown-preview ul ul ol{list-style-type:lower-alpha}.markdown-preview .newpage,.markdown-preview .pagebreak{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center!important}.markdown-preview:not([data-for=preview]) .code-chunk .code-chunk-btn-group{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .status{display:none}.markdown-preview:not([data-for=preview]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0;min-height:100vh}@media screen and (min-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode]) .markdown-preview{font-size:14px!important;padding:1em}}@media print{html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for=html-export]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,.66);border:4px solid rgba(150,150,150,.66);background-clip:content-box}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div,html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p{display:inline}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% - 300px);padding:2em calc(50% - 457px - 300px / 2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for=html-export]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for=html-export]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
      <!-- The content below will be included at the end of the <head> element. --><script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function () {
    // your code here
  });
</script></head><body for="html-export">
    
    
      <div class="crossnote markdown-preview  ">
      
<h1 id="大模型的具身智能发展与挑战">大模型的具身智能：发展与挑战 </h1>
<blockquote>
<p>白辰甲, 许华哲, 李学龙. 大模型驱动的具身智能: 发展与挑战. 中国科学: 信息科学, 2024, 54: 2035–2082, doi: 10.1360/ SSI- 2024- 0076<br>
Bai C J, Xu H Z, Li X L. Embodied-AI with large models: research and challenges (in Chinese). Sci Sin Inform, 2024, 54: 2035–2082, doi: 10.1360/SSI-2024-0076</p>
</blockquote>
<h2 id="摘要">摘要 </h2>
<blockquote>
<p>大模型驱动的具身智能是涵盖<strong>人工智能、机器人学和认知科学</strong>的交叉领域, 重点研究如何将大模型的感知、推理和逻辑思维能力与具身智能相结合, 提升现有模仿学习、强化学习、模型预测控制等具身智能框架的数据效率和泛化能力. 近年来, 随着大模型能力的不断提升, 以及具身智能中示教数据、仿真平台、任务集合的不断完善, 大模型和具身智能的结合将成为人工智能的下一个浪潮, 有望成为人工智能迈向实体机器人的重要突破口. 本文围绕<strong>大模型驱动的具身智能</strong>这一研究领域, 从3个方面进行了系统的调研、分析和展望.</p>
<ul>
<li>首先, 回顾了大模型和具身智能的相关技术背景, 以及具身智能现有的学习框架.</li>
<li>其次, 按照大模型赋能具身智能的方式, 将现有研究分为大模型驱动的环境感知、大模型驱动的任务规划、大模型驱动的基础策略、大模型驱动的奖励函数、大模型驱动的数据生 成等5类范式.</li>
<li>最后, 总结了大模型驱动的具身智能中存在的挑战, 对可行的技术路线进行展望, 为相 关研究人员提供参考, 进一步推动国家人工智能发展战略.</li>
</ul>
<p><strong>关键词</strong>: 具身智能, 大模型, 环境感知, 任务规划, 基础策略</p>
</blockquote>
<h2 id="背景与定义">背景与定义 </h2>
<h3 id="具身智能">具身智能 </h3>
<ul>
<li><strong>具身智能</strong>（Embodied AI）是人工智能、机器人学、认知科学的交叉领域，主要研究如何使机器人具备类似人类的感知、规划、决策和行为能力。具身智能可以追溯到20世纪50年代，图灵首次提出具身智能的概念，探索如何使用机器感知和理解世界，并作出相应的决策和行动。<strong>基于高算力平台和大规模标注数据的深度学习方法形成的非具身智能体缺乏与环境交互学习的经验，无法直接驱动机器人实体完成特定任务</strong>。具身智能很强调<strong>感知-运动回路（perception-action loop），使用物理实体来感知和建模环境，根据任务目标和实体能力进行规划和决策，最后使用物理实体的运动能力来完成任务</strong>。具身实体完成任务结果的反馈将进一步优化智能体的策略，从而使智能体的行为能够适应变化的环境。</li>
</ul>
<blockquote>
<p>具身智能在研究中更多体现智能的理念，在具身实体中融合了<strong>视觉、语言、决策</strong>等多方面的技术来提升智能体的通用性和泛化性。</p>
</blockquote>
<h3 id="大模型">大模型 </h3>
<ul>
<li><strong>大语言模型</strong> (Large Language Model, LLM)以ChatGPT为代表的大语言模型技术取得了突破性进展。通过在大规模网络对话数据中进行学习，ChatGPT能够实现包括自动问答、文本分类、自动摘要、机器翻译、聊天对话等各种自然语言理解和自然语言生成任务，同时具备在<strong>少样本</strong>和<strong>零样本</strong>场景下达到了传统学习方法的性能，并具有较强的泛化能力。通过先进的**思维链（Chainof Thoughts，CoT）**等提示技术，大语言模型的逻辑推理能力获得了大幅提升，从而有望解决复杂具身智能场景中的任务分解和推理问题。</li>
<li><strong>视觉基础模型</strong>（visual foundation model，VFM）通过自监督学习目标可以获得强大的<strong>视觉编码器</strong>，能够解决如<strong>图像分类、语义分割、场景理解等视觉感知任务</strong>，在具身智能任务中，强大的视觉编码器能够对<strong>视觉传感器</strong>获得的周围环境信息进行分析和理解，从而帮助智能体进行决策；</li>
<li><strong>视觉-语言模型</strong>（visual-language model，VLM）通过引入<strong>预训练视觉编码器和视觉-语言模态融合模块</strong>，使得大语言模型能够获取视觉输入。同时根据语言提示进行<strong>视觉问答</strong>。在具身智能中，引入视觉-语言模型能够使智能体根据任务语言指令和环境的视觉观测进行推理和决策，从而提升智能体对环境的感知和理解能力；</li>
<li><strong>多模态大模型</strong>（large multimodal model，LMM）视频、音频、肢体语言、面部表情和<strong>生理信号</strong>等更多模态，可以分析更丰富的传感器输入并进行信息融合，同时结合具身智能体中特有的<strong>机器人状态、关节动作</strong>等模态信息，帮助解决更复杂的具身智能任务。</li>
</ul>
<p>大模型通过充分利用大规模数据集中学习到的知识，结合特定的具身智能场景和任务描述，为智能体提供环境感知和任务规划能力。下图列举了近年来大模型驱动的具身智能领域的代表性成果。<img src="%E6%88%AA%E5%B1%8F2024-12-17%2020.15.29.png" alt="alt text"></p>
<h3 id="大模型驱动的具身智能">大模型驱动的具身智能 </h3>
<p>在赋能感知和规划之外，大模型能够和具身智能的经典框架结合，提升策略的泛化能力和对环境的适应能力。具身智能的传统框架主要包括<strong>模仿学习（imitation learning，IL）、强化学习（reinforcement learning，RL）和模型预测控制（model predictive control，MPC）</strong> 等。具体地：</p>
<ul>
<li>模仿学习遵循监督学习的范式，通过直接从<strong>专家轨迹数据</strong>中学习策略，但往往受限于专家数据的规模和**协变量偏移（covariate shift）**问题而容易产生较高的泛化误差；</li>
<li>强化学习通过在环境交互中试错来获得样本，通过<strong>最大化奖励</strong>来获得策略和值函数，但在机器人任务中受限于<strong>复杂的奖励设计</strong>和<strong>长时间的环境交互</strong>；</li>
<li>模型预测控制通过使用<strong>环境模型</strong>产生对未来策略执行情况的预测，结合<strong>策略搜索方法</strong>获得当前最优的动作，但依赖于对环境的先验知识和环境模型的泛化能力。</li>
</ul>
<p>大模型与上述框架结合：</p>
<ul>
<li>在模仿学习中，大语言模型和视觉语言模型能够作为<strong>基础策略</strong>使智能体利用大模型对环境的理解和泛化能力，同时，大模型<strong>对任务的分解</strong>能够产生的任务的短期目标来降低模仿学习的难度；</li>
<li>在强化学习中，大模型能够根据对大模型能够根据对任务和场景的理解产生合适的<strong>奖励函数</strong>来引导强化学习中的<strong>价值函数和策略函数</strong>的学习，同时强化学习能够作为大模型的<strong>基础策略</strong>和<strong>人类偏好</strong>对齐的工具（引导大模型给出人类便好的答案）；</li>
<li>在模型预测控制的框架下，大模型能够利用从大量数据中获取的对物理世界的理解<strong>构建环境模型</strong>，进而使智能体能够使用环境模型进行交互和策略搜索。</li>
</ul>
<p>此外，视觉生成模型和语言生成模型可以根据任务需求生成机器人交互环境供强化学习算法进行交互，或生成交互数据来扩充特定任务下的<strong>专家样本</strong>，用于缓解真实机器人任务重普遍存在的数据稀缺问题。<br>
大模型驱动驱动的具身智能算法主要包含大模型驱动的环境感知、任务规划、基础策略、奖励函数和数据生成五个方面：</p>
<ol>
<li><strong>大模型驱动的环境感知</strong>从冗余的多传感器观测中进行特征抽取和信息融合，能够提取对策略学习有用的信息，从而使具身智能学习框架普遍受益；</li>
<li><strong>大模型对宏观任务的规划</strong>使用大模型的逻辑推理能力对复杂任务进行分解，允许使用灵活的底层学习框架对分解后的任务进行策略学习；</li>
<li><strong>大模型驱动的基础策略</strong>可以与模仿学习框架进行结合并作为模型学习的初始任务进行策略学习，在使用少量机器人的任务数据微调后，大模型能够将通用的环境理解能力和特定的具身应用场景结合，<strong>减少测策略训练对机器人数据的需求量</strong>并提升策略的泛化能力；</li>
<li><strong>大模型驱动的奖励函数</strong>可与强化学习算法进行结合，减少机器人场景中认为进行奖励函数设计的难度，降低奖励函数设计对物理先验知识的依赖，客服强化学习算法在机器人任务中面临的稀疏奖励问题；</li>
<li><strong>大模型驱动的数据生成</strong>根据学习框架的不同分为两类：
<ol>
<li>大模型可以作为环境模型生成智能体的<strong>未来轨迹预测</strong>，与模型预测算法和基于模型的强化学习算法相结合进行策略搜索；</li>
<li>大模型可以生成机器人数据用于具身策略训练，，作用于<strong>无模型强化学习算法和模仿学习算法</strong>，从而缓解机器人任务中的数据缺乏问题。</li>
</ol>
</li>
</ol>
<p>大模型驱动的具身智能研究中存在的5大挑战，主要包括：</p>
<ol>
<li>大模型在特定具体具身场景中的适应问题，将大模型的“通才”转化为机器人任务中的“专才”智能体；</li>
<li>大模型策略和人类偏好对齐问题；</li>
<li>具身策略的跨域泛化问题；</li>
<li>大模型驱动多智能体协作的能力；</li>
<li>大模型具身策略的决策实时性问题。</li>
</ol>
<p>本文总结的内容具体框架如下图所示。<br>
<img src="%E6%88%AA%E5%B1%8F2024-12-17%2014.40.12.png" alt="alt text"></p>
<blockquote>
<p>具身智能基本概念主要包括具身实体定义、传感器、任务定义等；具身智能学习框架主要包括模仿学习、强化学习、模型预测控制等；大模型技术主要包括大语言模型、视觉-语言大模型、多模态模型、扩散生成模型等。</p>
</blockquote>
<h3 id="具身系统基本概念">具身系统基本概念 </h3>
<p>具身智能系统的基本结构如下图所示。<img src="%E6%88%AA%E5%B1%8F2024-12-17%2020.28.56.png" alt="alt text">主要包括<strong>实体、任务、环境</strong>三个部分。</p>
<ul>
<li><strong>实体</strong>是系统的核心，主要包括机器人、传感器、执行器等部分；
<ul>
<li>在特定任务中，机器人通过传感器获取对环境的感知，随后由具身智能算法产生当前合适的动作，将动作传输给执行器，执行期产生底层机器人指令与环境进行交互，获得环境的反馈和更新后的场景信息斌循环进行上述过程。机器人类型往往决定了具体可以使用的传感器和执行器，常用的机器人包括机械臂、四足机器人、移动机器人、灵巧手、人形机器人等，如下图所示：<img src="%E6%88%AA%E5%B1%8F2024-12-17%2020.36.46.png" alt="alt text">
<ul>
<li><strong>机械臂</strong>是最为常见的具身实体类型。机械臂类型，仿真平台，机械臂策略数据集；</li>
<li><strong>四足机器人</strong>可以在复杂的地形条件下稳定行走、奔跑、跳跃和避障等任务，不同地形采用不同策略，依赖仿真环境，模仿学习；</li>
<li><strong>移动机器人</strong>扩展了固定机械臂的使用场景，具有移动底座。真实移动机器人的专家数据集，家务操作；</li>
<li><strong>灵巧手</strong>是一种新兴的具身实体类型，用来执行复杂的灵巧操作任务，仿真，灵巧手示教数据集，模仿学习；</li>
<li><strong>人形机器人</strong>，人形机器人的传感器，人形机器人的策略数据集，人形机器人的仿真环境，人形机器人的专家数据集，人形机器人的模仿学习。</li>
</ul>
</li>
<li>在<strong>传感器</strong>方面，具身机器人使用的传感器主要包括视觉传感器、机器人本体感知（proprioception）传感器、触觉/力学（haptic/tactile perception）等传感器、GPS和IMU传感器等。如下图（a）所示。。<img src="%E6%88%AA%E5%B1%8F2024-12-17%2021.18.03.png" alt="机器人传感器">
<ul>
<li><strong>大模型驱动的具身智能中由于大模型主要用于处理视觉和语言，因此对视觉传感器较为关注</strong>，如上图（b）所示，通过视觉传感器或仿真环境中的视觉渲染功能、可以获得<strong>RGB、深度图、语义分割图、点云</strong>等视觉输入信息;</li>
<li><strong>机器人的本体感知传感器</strong>根据机器人类型而有所不同. 例如,
<ul>
<li><strong>机械臂的本体感知包括各个关节的位置、速度、扭矩和机械臂末端位置</strong>等;</li>
<li><strong>四足机器人的本体感知信息包括自身线速度、角速度, 身体关节位置、力传感器等</strong>各种传感器信息, 动作通常设置为身体关节的目标位置等;</li>
<li><strong>灵巧手的本体感知包括关节位置、速度、力矩, 指尖位置、力传感, 手腕的位置与角度等</strong>.</li>
</ul>
</li>
<li><strong>触觉/力学传感器</strong>主要用于灵巧手和人形机器人手部关节中, 用于灵巧手感受抓握物体的力度, 估计物体的形状等.</li>
<li><strong>GPS/IMU传感器</strong>在机器人运动中进行定位, 多用于移动机器人、人形机器人等.</li>
<li>此外, 由于仿真器中可以获取预设的环境参数, 仿真环境中机器人常拥有 “特权” 观测. 例如, 仿真机械臂抓取任务中可以获得准确的抓取点或物体位置, 仿真四足机器人运动中可以获取地形信息和环境摩擦参数等. 特权信息能够帮助策略在仿真环境中快速收敛, 但在真实环境中是缺失的, 需要通过额外传感器或特权信息蒸馏等架构来进行仿真-真实 (sim-to-real) 的策略迁移.</li>
</ul>
</li>
<li>在<strong>执行器</strong>方面，根据机器人类型的不同，机器人底层分别采用不同的控制器进行执行。
<ul>
<li>在机械臂中, 常见的动作输出有关节位置、<strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span></span></span>关节位置、末端执行器姿态和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span></span></span>末端执行器姿态</strong>等. 通过控制器驱动相应电机来进行关节位置、速度、扭矩的调整, 从而执行相应的动作;</li>
<li>四足机器人的动作一般为<strong>足部关节的位置</strong>, 通过PD控制器来产生相应的电机扭矩;</li>
<li>灵巧手的动作一般为<strong>手指关节的力矩、手腕的位置、方向</strong>等, 随后转换为关节扭矩进行执行. 人形机器人拥有更多的关节和更高的自由度, 一般采 用多个控制器分别控制手部关节、双臂系统、腿部关节、躯干等. 一般而言, 大模型更关注于高层的任 务执行和动作选择, 底层由传统的执行器进行执行.</li>
</ul>
</li>
</ul>
</li>
<li>具身智能的<strong>任务</strong>由常见的抓取任务逐步演变为更加复杂的任务集合。
<ul>
<li>在<strong>机械臂和移动机器人</strong>中，仿真平台ManniSkill2任务集合包括精细操作任务，如刚性物体操作、铰链物体操作（开门、开箱）等、柔性物体操作（倒水、挂毛巾）等。其中铰链物体和柔性物体都具有复杂的模型，在策略学习中需要隐式估计的位移和形变；</li>
<li><strong>四足机器人</strong>学习的目标是适应不同的地形，同时有研究使四足机器人学习不同的步态；</li>
<li><strong>灵巧手</strong>用于测试的任务包括模仿、旋转物体、开门、倒水等；</li>
<li><strong>人形机器人</strong>由于包含了双足运动、双臂操作、双灵巧手等系统，可执行的任务更为多样，几乎可以涵盖现有具身实体的所有任务集。</li>
</ul>
</li>
</ul>
<blockquote>
<p>为了和人类进行更好的交互, 许多任务包含了语言描述的目标, 如 VIMA, Calvin等, 常见的语言任务描述如 “将红色方块放进抽屉”, “按下蓝色按钮打开柜子” 等, 任务的执行情况使用执行成功率和时间效率进行衡量.</p>
</blockquote>

      </div>
      
      
    
    
    
    
    
    
  
    </body></html>