# Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI
> Yang Liu, Weixing Chen, Yongjie Bai, Xiaodan Liang, Guanbin Li, Wen Gao, Liang Lin

[![arXiv](https://img.shields.io/badge/arXiv-2407.06886-orange)](https://arxiv.org/abs/2407.06886)
[![](https://img.shields.io/badge/Paper-%F0%9F%93%96-yellow)](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List/blob/main/EmbodiedAI_Review.pdf)
[![](https://img.shields.io/badge/Project-%F0%9F%9A%80-pink)](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List)


- [摘要](#摘要)
- [介绍](#介绍)
- [具身机器人](#具身机器人)
  - [固定基座机器人](#固定基座机器人)
  - [轮式机器人和履带式机器人](#轮式机器人和履带式机器人)
  - [四足机器人](#四足机器人)
  - [人形机器人](#人形机器人)
  - [仿生机器人](#仿生机器人)
- [具身模拟器](#具身模拟器)
  - [通用模拟器](#通用模拟器)
  - [基于真实场景的模拟器](#基于真实场景的模拟器)
- [具身感知](#具身感知)
  - [主动视觉感知](#主动视觉感知)
  - [三维视觉定位](#三维视觉定位)
  - [视觉语言导航](#视觉语言导航)
  - [非视觉感知：触觉](#非视觉感知：触觉)
- [具身交互](#具身交互)
  - [具身问答](#具身问答)
  - [具身抓取](#具身抓取)
- [具身智能体](#具身智能体)
  - [具身多模态基础模型](#具身多模态基础模型)
  - [具身任务规划](#具身任务规划)
  - [具身动作规划](#具身动作规划)
- [仿真到现实的适配](#仿真到现实的适配)
  - [具身世界模型](#具身世界模型)
  - [数据收集和训练](#数据收集和训练)
  - [具身控制](#具身控制)
  - [集所有功能于一身的机器人](#集所有功能于一身的机器人)
- [挑战和未来方向](#挑战和未来方向)
- [总结](#总结) 



## 摘要
---
**具身人工智能 Embodied AI**对于实现**通用人工智能AGI**至关重要，并且是连接网络空间与物理世界的各类应用的基础。最近，多模态大模型（Multi-modal Large Models,MLMs）和世界模型（World Models,WMs）的出现因其卓越的感知、交互和推理能力而备受关注，使其成为具身智能体 “大脑” 颇具潜力的架构。然而，目前尚无针对多模态大模型时代具身人工智能的全面综述。在本综述中，我们对具身人工智能的最新进展进行了全面探索。我们的分析首先梳理具身机器人和模拟器的前沿代表性成果，以充分了解研究重点及其局限性。然后，我们剖析四个主要研究目标：

1) **具身感知**，
2) **具身交互**，
3) **具身智能体**，
4) **从模拟到现实的迁移**.

涵盖最先进的方法、关键范式和综合性数据集。此外，我们探究**多模态大模型**在虚拟和真实具身智能体中的复杂性，强调其在促进动态数字和物理环境中交互的重要性。最后，我们总结具身人工智能面临的挑战与局限，并探讨其未来可能的发展方向。我们希望本综述能为研究界提供基础参考，并激发持续创新。相关项目可在 https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List 获取。 

## 介绍
---

下表展示了具身人工智能和非具身人工智能的区别。
![具身人工智能和非具身人工智能的区别](<截屏2025-02-20 09.27.56.png>)

多模态大型模型（Multi-modal Large，MLM）的最新进展为具身模型注入了强大的感知、交互和规划能力，以开发与虚拟和物理环境积极交互的通用具身代理和机器人。然而，**长期记忆、理解复杂意图和分解复杂任务的能力**对于当前的MLM来说是有限的。
> 长期记忆这一点确实很明显，豆包AI中的一个聊天机器人会很容易忘记当前的场景和过去的事情。

发展具身人工智能是实现通用人工智能的重要途径。不同于ChatGPT这样的聊天机器人，具身人工智能认为真正的AGI能够通过控制物理实体实施并与**虚拟和物理环境交互**来实现。具身人工智能包含计算机视觉（Computer Vision，CV）、自然语言处理（Natural Language Processing，NLP）和机器人技术（Robotics）等各种关键技术，最具代表性的有：具身感知、具身交互、具身代理记忆虚拟-现实机器人控制。

**具身代理**是具身人工智能最突出的基础。对于具身任务，具身主体必须充分理解语言指令中的人类意图，主动探索周围环境，从虚拟和物理环境中全面感知多模态元素，并针对复杂任务执行适当的动作，如下图所示。

![基于MLMs和WMs的具身智能体的整体框架。具身智能体有一个具身世界模型作为它的“大脑”。它具有理解虚拟物理环境和主动感知多模态元素的能力。它可以充分理解人类意图，与人类价值和事件因果关系保持一致，分解复杂任务，执行可靠的行动，以及与人类互动和利用知识和工具。](<截屏2025-02-20 09.29.42.png>)

与传统的**深度强化学习**相比，多模态模型的快速进步展示了在复杂环境中的多功能性、灵巧型和通用性。
- 来自最先进的视觉编码器的**预训练视觉表示**提供对象**类别、姿势和几何形状**的精确估计，这使得具身模型能够彻底感知复杂和动态的环境。
- 强大的**大型语言模型**（Large Language Models，LLM）使机器人更好地理解人类的语言指令。
- 有前途的**多模态大模型**（MLMs）为从具身机器人中对齐视觉和语言表示提供了可行的方法。
- **世界模型**表现出卓越的模拟能力和对物理定律的有希望的理解，这使得具身模型能够全面理解物理和真实环境。

这些创新使具身代理能够全面感知复杂环境，与人类自然交互，并可靠地执行任务。
> 具身人工智能表现出快速的进步，引起了研究界的极大关注，被认为是实现AGI最快的途径。


![该调查侧重于全面分析具身人工智能的最新进展。](<截屏2025-02-20 16.13.27.png>)

## 具身机器人
[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

实施剂积极地与物理环境相互作用，并涵盖广泛的实施例，包括机器人、智能电器、智能眼镜、自动驾驶汽车等。其中，**机器人**作为最突出的体现之一脱颖而出。根据应用，机器人被设计成各种形式，以利用其硬件特性来完成特定任务，如下图所示。

![化身机器人包括固定基地机器人、四足机器人、人形机器人、轮式机器人、履帶机器人和仿生机器人。](<截屏2025-02-20 16.13.39.png>)

### 固定基座机器人 
如上图（a）所示，固定基座机器人因其紧凑和高精度操作而被广泛用于实验室自动化、教育培训和工业制造。配备高精度**传感器**（sensors）和**执行器**（actuators），它们实现了微米级精度，使其适用于需要**高精度和可重复性**的任务。此外，固定基地机器人具有**高度可编程性**，允许用户根据各种任务场景对其进行调整，例如Franka（Franka Emika熊猫）、Kuka iiwa（KUKA）[23]和Sawyer（重新思考机器人）。

> [!NOTE]
> 然而，固定底座机器人也有一定的缺点。它们的固定底座设计限制了它们的操作**范围和灵活性**，使它们无法在大范围内移动或调整位置，并导致它们与人类和其他机器人协作。

### 轮式机器人和履带式机器人
如上图中（b）所示的**轮式机器人**（wheeled robots）拥有更复杂和多样的应用场景。以其高效的**移动性**而闻名，广泛应用于**物流、仓储和安全检查**。轮式机器人的优点包括结构简单、成本相对低、高能效和在平面上快速移动的能力。这些机器人通常配备高精度传感器，如**激光雷达和摄像头**，实现**自主导航**和**环境感知**，使它们在自动化仓库管理和检查任务中非常有效，例如Kiva机器人（Kiva Systems）和Jackal机器人（Clearpath Robotics）。

> [!NOTE]
> 然而，轮式机器人在复杂地形和恶劣环境中的**机动性**有限，尤其是在不平坦的地面上。此外，它们的**承载能力**和**机动性**受到一定限制。

如上图（c）所示，**履带式机器人**（tracked robots）具有强大的**越野能力**和**机动性**，在**农业、建筑和灾后恢复**方面显示出潜力。履带系统提供了更大的地面接触面积，分散了机器人的重量，降低了在泥沙等松软地形下沉的风险。此外，履带式机器人配备了强大的**动力和悬挂系统**，以在复杂的地形上保持稳定性和牵引力。因此，履带式机器人也用于**军事**等敏感领域。

> [!NOTE]
> - iRobot的PackBot是一款多功能军用履带机器人，能够执行侦察、爆炸物处理和救援任务。
> - 然而，由于履带系统的**高摩擦力**，履带式机器人经常受到能源效率低的困扰。此外，它们在平坦表面上的运动速度比轮式机器人慢，以及它们的灵活性和机动性。

### 四足机器人
如上图（d）所示。**四足机器人**（quadruped robots）以其**稳定性和适应性**而闻名，非常适合**复杂的地形探索、救援任务和军事应用**。受四足动物的启发，这些机器人可以在不平坦的表面上保持平衡和移动性。**多关节设计**允许他们模仿生物运动，实现复杂的步态和姿势调整。**高度可调性**使机器人能够自动调整其姿态以适应不断变化的地形，从而提高**机动性和稳定性**。激光雷达和摄像头等传感系统提供环境感知，使机器人能够**自主导航**并避开障碍物。几种类型的四足机器人被广泛使用：**Unitree Robotics、波士顿动力点和ANYmal C**。
> - Unitree Robotics的Unitree A1和Go1以其成本有效性和灵活性而闻名。A1和Go1具有强大的**机动性和智能避障能力**，适用于各种应用。
> - 波士顿动力点Boston Dynamics以其卓越的稳定性和操作灵活性而闻名，通常用于**工业检查和救援任务**。它具有强大的承载能力和适应性，能够在恶劣的环境中执行复杂的任务。
> - ANYbotics的ANYmal C以其模块化设计和高耐用性，广泛用于工业检测和维护。ANYmal C配备了自主导航和远程操作能力，适用于长时间的户外任务，甚至极端的月球任务。

> [!NOTE]
> 四足机器人的**复杂设计和高制造成本**导致大量初始投资，限制了它们在成本敏感领域的使用。此外，它们在复杂环境中的**电池寿命**有限，需要频繁充电或更换电池以延长运行时间。

### 人形机器人
**人形机器人**（Humanoid robots）如上图（e）所示。外形酷似人类，并且在**服务业、医疗保健和协作环境等领域**越来越普遍。这些机器人可以模仿人类的动作和行为模式，提供个性化的服务和支持。他们的灵巧的手设计使它们能够执行错综复杂的任务，将它们与其他类型的机器人区分开来。这些手通常具有**多个自由度和高精度传感器**，使它们能够模拟人手的抓握和操作能力，这在医疗手术和精密制造等领域尤为重要。在目前的人形机器人中，**Atlas（波士顿动力）**以其卓越的机动性和稳定性而闻名。
- **Atlas**可以执行复杂的动态动作，如跑步、跳跃和滚动，展示了人形机器人在高度动态环境中的潜力。
- **HRP系列**（AIST）用于各种研究和工业应用，设计重点是高稳定性和灵活性，使其在复杂环境中有效，特别是在与人类的协作任务中。
- **ASIMO**（本田，Honda）是最著名的人形机器人之一，可以走路、跑步、爬楼梯，并识别人脸和手势，使其适合接待和引导服务。
- 此外，小型社交机器人**Pepper**（软银机器人，Softbank Robotics）可以识别情绪并从事自然语言交流，并广泛用于客户服务和教育环境中

> 然而，由于其复杂的控制系统，人形机器人在复杂环境中面临着主要**操作稳定性和可靠性**的挑战。这些挑战包括**鲁棒的双足行走控制**和**灵巧的手握**。此外，基于液压系统的传统人形机器人以其笨重的结构和高昂的维护成本为特征，正越来越多地被电机驱动系统所取代。最近，特斯拉和Unitree Robotics推出了**基于电机系统的人形机器人**。随着LLM的集成，人形机器人有望智能地处理各种复杂任务，填补**制造业、医疗保健和服务业**的劳动力空白，从而提高效率和安全性。

### 仿生机器人

仿生机器人（biomimetic robots），如上图（f）所示。不同的是，仿生机器人通过**模拟自然生物体的有效运动和功能**在复杂和动态的环境中执行任务。通过模拟生物实体的形式和运动机制，这些机器人在**医疗保健、环境监测和生物研究等领域**表现出巨大的潜力。典型地，它们利用柔性材料和结构来实现逼真、敏捷的运动，并最大限度地减少对环境的影响。重要的是，仿生设计可以通过模仿生物有机体的有效运动机制**显著提高机器人的能源效率**，使它们在能源消耗方面更加经济。
这些仿生机器人包括类鱼机器人、类昆虫机器人和软体机器人，如上图（f）所示。
> [!NOTE]
> 然而，仿生机器人面临几个挑战。首先，它们的**设计和制造过程复杂且成本高昂**，限制了大规模生产和广泛应用。其次，由于它们使用柔性材料和复杂的运动机制，仿生机器人在极端环境下的**耐用性和可靠性**受到限制。

## 具身模拟器

[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

**具身模拟器**
**具身模拟器**（Embodied simulators）对于具身人工智能至关重要，因为它们提供具有成本效益的实验，通过模拟潜在危险场景确保安全性，在不同环境中进行测试的可扩展性，快速原型制作能力，可访问更广泛的研究社区，用于精确研究的受控环境，用于训**练和评估的数据生成**，以及用于比较算法的标准化基准。为了使代理与环境互动，有必要构建一个逼真的模拟环境。这需要考虑环境的物理特征、物体的特性以及它们的相互作用。
本节将分两部分介绍常用的仿真平台：基于底层仿真的**通用模拟器**和**基于真实场景的模拟器**。

### 通用模拟器
真实环境中存在的物理相互作用和动态变化是不可替代的。然而，在物理世界中部署体现模型通常会产生高成本，并面临许多挑战。通用模拟器提供了一个密切模仿物理世界的虚拟环境，允许算法开发和模型培训，这提供了显著的成本、时间和安全优势。
- **Isaac Sim**是机器人和人工智能研究的高级仿真平台。它具有高保真物理模拟、实时光线追踪、广泛的机器人模型库和深度学习支持。其应用场景包括**自动驾驶、工业自动化和人机交互**；
- **Gazebo**是机器人研究的开源模拟器。它具有广泛的机器人库，并与ROS紧密集成。它支持各种传感器的仿真，并提供大量预构建的机器人模型和环境。它主要用于**机器人导航和控制以及多机器人系统**；
- **PyBullet**是python接口用于子弹物理引擎。它易于使用，并具有不同的传感器模拟和深度学习集成。PyBullet支持实时物理模拟，包括刚体动力学、碰撞检测和约束解决；

下表展示了10种通用模拟器的关键特性和主要应用场景，它们各自在具身人工智能领域提供了独特的优势，研究人员可以根据自己的具体研究需求选择最合适的模拟器，从而加速具身人工智能技术的开发和应用。

![通用模拟器。HFPS：高保真物理模拟；HQGR：高质量图形渲染；RRL：丰富的机器人库；DLS：深度学习支持；LSPC：大规模并行计算；ROS：与ROS的紧密集成；MSS：多传感器模拟；CP：跨平台NAV：机器人导航AD：自动驾驶；RL：增强学习LSPS：大规模并行SIM MR：多机器人系统RS：机器人模拟。](<截屏2025-02-21 10.18.03.png>)

下图显示了通用模拟器的可视化效果。

![一般模拟器的例子。MuJoCo的形象来自C. Wang, Q. Zhang, Q. Tian, S. Li, X. Wang, D. Lane, Y. Petillot, and S. Wang, “Learning mobile manipulation through deep reinforcement learning,” Sensors, vol. 20, no. 3, p. 939, 2020.](<截屏2025-02-21 10.18.15.png>)

### 基于真实场景的模拟器
实现**能够执行家庭活动的通用具身智能体**一直是具身人工智能研究领域的主要关注点。这些具身智能体需要深入理解人类日常生活，并在室内环境中执行诸如导航和交互等复杂的具身任务。为了满足这些复杂任务的需求，模拟环境需要尽可能地贴近现实世界，这对模拟器的复杂性和真实性提出了很高的要求。这促使了基于现实世界环境的模拟器的诞生。这些模拟器大多从现实世界中收集数据，创建高度逼真的3D模型资源，并使用**虚幻引擎5（UE5）和Unity**等3D游戏引擎来构建场景。丰富且逼真的场景使得基于现实世界环境的模拟器成为家庭活动中具身人工智能研究的首选。 
- **AI2-THOR**是由艾伦人工智能研究所主导开发的一款基于**Unity3D**的室内具身场景模拟器。作为一款在现实世界基础上构建的高保真模拟器，AI2-THOR拥有丰富的可交互场景物体，并且为这些物体赋予了物理属性（例如可开启/关闭，甚至是冷/热属性）。AI2-THOR由两部分组成：**iTHOR和RoboTHOR**。
  - **iTHOR**包含120个房间，分类为厨房、卧室、浴室和客厅，拥有超过2000个独特的交互对象，并支持多智能体模拟；
  - **RoboTHOR**包含89个模块化公寓，拥有600+个对象，其独特之处在于这些公寓对应于现实世界中的真实场景。迄今为止，已经发表了一百多部基于AI2-THOR的作品。
- **Matterport3D**是在**R2R**中提出的，更常用于作为大规模2D-3D视觉数据集。Matterport3D数据集包括90个建筑室内场景，包括10,800张全景图和194,400张RGB-D图像，并提供表面重建、相机姿势以及2D和3D语义分割注释。Matterport3D将3D场景转换为离散的“视点”，在Matterport3D场景中，具身智能体在相邻的“视点”之间移动。在每个“视点”处，具身智能体可以获得以“视点”为中心的1280x1024全景图（18×RGB-D）。Matterport3D是最重要的具身导航基准之一。
- **Virtualhome**是由普伊格（Puig）等人推出的一款**家庭活动具身人工智能模拟器**。Virtualhome最特别之处在于其由环境图表示的环境。这种环境图描绘了场景中的物体以及它们之间的相互关系。用户还可以自定义和修改环境图，以实现对场景物体的定制化配置。这种环境图为具身智能体理解环境提供了一种新途径。与AI2-THOR类似，Virtualhome也提供了大量可交互的物体，具身智能体能够与这些物体进行交互并改变它们的状态。Virtualhome的另一个特点是其简单易用的应用程序编程接口（API）。具身智能体的动作被简化为“**操作+物体**”的格式。这一特点使得Virtualhome在具身规划、指令分解等研究领域中得到了广泛应用。 
- **Habitat**是Meta公司推出的一款用于**大规模人机交互**的开源模拟器。基于Bullet物理引擎，Habitat实现了高性能、高速的并行三维模拟，并为具身智能体的强化学习提供了丰富的接口。Habitat具有极高的开放程度。研究人员可以在Habitat中导入和创建三维场景，或者利用Habitat平台上丰富的开放资源进行扩展。Habitat拥有许多可定制的传感器，并且支持多智能体模拟。来自开放资源或定制的多个具身智能体（例如人类和机器狗）可以在模拟器中进行协作，自由移动，并与场景进行简单的交互。因此，Habitat正受到越来越多的关注。
- **SAPIEN**与其他更注重场景的模拟器不同，更注重模拟对象的动作。基于PhysX物理引擎，SAPIEN提供了细粒度的具身控制，可以通过ROS接口实现基于力和扭矩的联合控制。SAPIEN基于partNet-Mobability Dataset，提供了包含丰富交互对象的室内仿真场景，并支持自定义资源的导入。与AI2-THOR等直接改变对象状态的模拟器不同，SAPIEN支持模拟物理交互，具身代理可以通过物理动作控制对象的铰接部分，从而改变对象的状态。这些特性使得SAPIEN非常适合训练具身AI的细粒度对象操作。
- **iGibson**是斯坦福大学推出的一款**开源**模拟器。它基于Bullet物理引擎构建，提供了15个高质量的室内场景，并且支持从其他数据集（如Gibson和Matterport3D）导入模型资源。作为一款面向对象的模拟器，iGibson为物体赋予了丰富且可变的属性，不仅局限于物体的运动学属性（姿态、速度、加速度等），还包括温度、湿度、清洁度、开关状态等。此外，除了其他模拟器中标准配置的深度传感器和语义传感器外，iGibson还为具身智能体提供了**激光雷达**，使智能体能够轻松获取场景中的三维点云数据。在具身智能体配置方面，iGibson支持连续动作控制和细粒度的关节控制。这使得iGibson中的具身智能体在自由移动的同时，能够与物体进行精细的交互。
- **TDW**由麻省理工学院推出。作为**最新的具身模拟器之一**，TDW融合了高保真的视频和音频渲染、逼真的物理效果以及单一灵活的控制器，在模拟环境的感知和交互方面取得了一定进展。TDW将多个物理引擎集成到一个框架中，能够实现对刚体、软体、织物和流体等各种材料的物理交互模拟，并在与物体交互时提供情境音效。因此，与其他模拟器相比，TDW迈出了重要的一步。TDW支持部署多个智能体，并为用户提供了丰富的应用程序编程接口（API）库和资源库，使用户能够根据自己的需求自由定制场景和任务，甚至包括户外场景及相关任务。 

![基于真实场景的模拟器示例。](<截屏2025-02-21 10.18.30.png>)

下表总结了上述所有基于真实场景的模拟器。**Sapien 在其设计方面表现突出**，它专门为模拟与门、橱柜和抽屉等带合页物体的交互而定制。**VirtualHome 因其独特的环境图而备受关注**，该环境图有助于根据对环境的自然语言描述进行高层次的具身规划。虽然 AI2Thor 提供了丰富的交互场景，但这些交互与VirtualHome 中的交互类似，都是基于脚本的，**缺乏真实的物理交互**。对于那些不需要细粒度交互的具身任务来说，这种设计是足够的。**iGibson 和 TDW 都提供了细粒度的具身控制**以及高度模拟的物理交互。**iGibson 的优势在于提供了丰富且逼真的大规模场景**，使其适用于复杂且长期的移动操作，而** TDW 在场景扩展方面给予用户更大的自由度**，并且具有独特的音频效果和灵活的流体模拟功能，这使得它在相关模拟场景中不可或缺。**Matterport3D 作为一个基础的 2D-3D 视觉数据集**，在具身人工智能基准测试中被广泛使用和扩展。尽管 Habitat 中的具身智能体缺乏交互能力，但其**广泛的室内场景、用户友好的界面以及开放的框架**，使其在具身导航领域备受推崇。

![基于真实屏幕的模拟器的比较。对于传感器，S表示语义，L表示激光雷达，A表示音频](<截屏2025-02-22 22.17.10.png>)

此外，**自动化模拟场景构建**对于获取高质量的具身数据非常有益。**RoboGen**通过大语言模型（LLMs）从随机采样的三维模型资源中定制任务，从而创建场景并自动训练智能体；**HOLODECK**可以根据人类指令在AI2-THOR中自动定制相应的高质量模拟场景；**PhyScene**基于条件扩散生成具有交互性且在物理上一致的高质量三维场景。艾伦人工智能研究所对AI2-THOR进行了扩展，并提出了**ProcTHOR**，它可以自动生成具有足够交互性、多样性和合理性的模拟场景。这些方法为具身人工智能提供了思路。 

## 具身感知

[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

未来视觉感知的 “北极星” 是**具身中心的视觉推理和社交智能**。与仅仅识别图像中的物体不同，具备具身感知能力的智能体必须在物理世界中移动并与环境进行交互。这需要对三维空间和动态环境有更深入的理解。**具身感知**（embodied perception）需要**视觉感知**（visual perception）和**推理能力**（reasoning），要理解场景内的三维关系，并基于视觉信息预测和执行复杂任务。 

### 主动视觉感知
主动视觉感知系统需要具备状态估计、场景感知和环境探索等基本能力。如下图所示，这些能力已在**视觉同步定位与建图**（Visual Si- multaneous Localization and Mapping，vSLAM）、**三维场景理解**以及**主动探索**等领域中得到了广泛研究。 

这些研究领域有助于开发强大的主动视觉感知系统，促进在复杂、动态环境中更好地与环境进行交互和导航。我们简要介绍这三个组成部分，并在下表中总结了每个部分所提到的方法。

![表四 主动视觉感知方法的比较](<截屏2025-02-23 10.21.49.png>)

> 如此看来具身智能还需要我了解一下SLAM这种视觉感知的算法咯。

1) **视觉同步定位与建图**（Visual Simultaneous Localisation and Mapping）：同步定位与建图（SLAM）是一种在未知环境中确定移动机器人位置的同时构建该环境地图的技术。基于测距的同步定位与建图（SLAM）使用**测距仪**（如激光扫描仪、雷达和/或声纳）创建**点云表示**，但成本较高，且提供的环境信息有限。视觉同步定位与建图（vSLAM）使用**车载摄像头**捕获图像帧并构建环境的表示。由于其硬件成本低、在小规模场景中精度高，以及能够捕获丰富的环境信息，它已受到广泛欢迎。经典的视觉同步定位与建图（vSLAM）技术可分为传统**视觉同步定位与建图**（vSLAM）和**语义视觉同步定位与建图**（Semantic vSLAM）。 
   1) **传统的视觉同步定位与建图**（vSLAM）系统利用图像信息和多视图几何原理，在未知环境中估计机器人的位姿，以构建由点云组成的低级地图（如稀疏地图、半稠密地图和稠密地图），例如**基于滤波器的方法**（如单目视觉同步定位与建图（MonoSLAM）、**多传感器扩展卡尔曼滤波**（MSCKF））、**基于关键帧的方法**（如并行跟踪与建图（PTAM）、ORB特征点法视觉同步定位与建图（ORB-SLAM））以及**直接跟踪方法**（如密集跟踪与建图（DTAM）、大尺度直接单目视觉同步定位与建图（LSD-SLAM））。**由于低级地图中的点云并不直接对应环境中的物体，这使得具身机器人难以对其进行解释和利用**。
   2) 然而，语义概念的出现，尤其是**结合语义信息解决方案的语义视觉同步定位与建图**（Semantic vSLAM）系统，显著提升了机器人在未探索环境中的感知和导航能力。 
   
   早期的研究成果，例如**SLAM++**，利用实时三维物体识别与跟踪技术来创建高效的物体图，从而在杂乱的环境中实现可靠的回环检测、重定位以及物体检测。**立方体同步定位与建图**（CubeSLAM）和**分层狄利克雷过程同步定位与建图**（HDP-SLAM）将三维矩形引入地图中，以构建轻量级的语义地图。**二次曲面同步定位与建图**（QuadricSLAM）采用语义三维椭球体，在复杂几何环境中实现对物体形状和位姿的精确建模。**So-SLAM**在室内环境中纳入了完全耦合的空间结构约束（共面性、共线性和邻近性）。为了应对动态环境带来的挑战，**动态语义同步定位与建图**（DS-SLAM）、**动态同步定位与建图**（DynaSLAM）和**语义几何同步定位与建图**（SG-SLAM）运用语义分割进行运动一致性检查，并使用多视图几何算法来识别和过滤动态物体，从而确保稳定的定位和建图。**基于光流、视觉和深度的同步定位与建图**（OVD-SLAM）利用语义、深度和光流信息来区分没有预定义标签的动态区域，实现更准确、更可靠的定位。**高斯球面同步定位与建图**（GSSLAM）利用三维高斯表示法，通过实时可微的溅射渲染管线和自适应扩展策略，在效率和准确性之间取得了平衡。

> [!NOTE]
> 这一部分主要是视觉感知中的地图建立，主要是确定智能体在环境中的位置，相对于环境的存在。

2) **3D场景理解**（3D Scene Understanding）：三维场景理解旨在从三维场景数据中区分物体的**语义**，确定物体的位置，并推断其几何属性，这在自动驾驶、机器人导航以及人机交互等领域中是至关重要的基础。一个场景可以使用诸如激光雷达或 RGB-D 传感器等三维扫描工具记录为三维点云数据。与图像不同的是，点云数据是稀疏的、无序的且不规则的，这使得场景解读极具挑战性。近年来，人们提出了许多用于三维场景理解的深度学习方法，这些方法可以分为**基于投影的方法**、**基于体素的方法**和**基于点的方法**。具体而言:
   1) **基于投影的方法**（例如，多视角三维目标检测网络（MV3D）、点柱网络（PointPillars）、多视角卷积神经网络（MVCNN））将三维点投影到各种图像平面上，并使用基于二维卷积神经网络（CNN）的主干网络进行特征提取。
   2) **基于体素的方法**将点云转换为规则的体素网格，以便于进行三维卷积操作（例如，体素网络（VoxNet）、分层卷积神经网络（SSCNet）），并且一些研究通过稀疏卷积提高了它们的效率（例如，闵可夫斯基网络（MinkowskiNet）、稀疏分层卷积网络（SSCNs）、具身扫描网络（Embodiedscan））。
   3) 相比之下，**基于点的方法**直接处理点云（例如，点云网络（PointNet）、点云网络++（PointNet++）、点多层感知机（PointMLP））。最近，为了实现模型的可扩展性，出现了基于Transformer的架构（例如，点Transformer（PointTransformer）、三维窗口多头自注意力网络（Swin3d）、点Transformer 2（PT2）、点Transformer 3（PT3）、三维视觉Transformer注意力网络（3D-VisTA）、局部增强目标感知网络（LEO）、点四叉树三维网络（PQ3D））和基于Mamba的架构（例如，点Mamba网络（PointMamba）、点云卷积Mamba网络（PCM）、三维Mamba网络（Mamba3D））。值得注意的是，除了直接使用点云的特征外，点四叉树三维网络（PQ3D）还无缝地融合了来自多视角图像和体素的特征，以增强场景理解能力。  

> [!NOTE]
> 理解三维场景，是智能体与环境交互的基础，感知环境之后要做的就是理解和识别。

3) **主动探索**（Active Exploration）：前面介绍的三维场景理解方法赋予了机器人以被动方式感知环境的能力。在这种情况下，感知系统的信息获取和决策过程并不会随着场景的变化而做出调整。然而，被动感知是主动探索的重要基础。鉴于机器人具备移动能力且能频繁与周围环境互动，它们也应当能够主动地探索和感知所处环境。二者之间的关系如下图所示。![图7. 主动视觉感知示意图。视觉同步定位与地图构建（Visual SLAM）和三维场景理解为被动视觉感知提供了基础，而主动探索能够为被动感知系统赋予主动性。这三个要素相互补充，对于主动视觉感知系统而言缺一不可。](<截屏2025-02-23 10.07.03.png>)当前解决主动感知问题的方法主要侧重于**与环境进行交互**，或者通过**改变观察方向**来获取更多的视觉信息。
   例如，平托等人提出了一种具有**好奇心**的机器人，它通过与环境的物理交互来学习视觉表征，而不是仅仅依赖数据集中的类别标签。为了解决不同形态的机器人在交互式物体感知方面的挑战，塔蒂亚等人提出了一个**多阶段投影框架**，该框架通过学习到的探索性交互来传递隐性知识，使机器人无需从头重新学习就能有效地识别物体属性。贾亚拉曼等人认识到自主获取有价值观测数据的挑战，提出了一种**强化学习方法**，在这种方法中，智能体通过减少对环境中未观测部分的不确定性来学习主动获取有价值的视觉观测数据，利用**循环神经网络**来主动完成全景场景和三维物体形状的构建。**NeU-NBV**引入了一种无地图规划框架，该框架通过迭代定位RGB相机来捕捉未知场景中最具信息价值的图像，利用基于图像的神经渲染中一种新颖的不确定性估计来引导数据收集朝着最不确定的视角进行。胡等人开发了一种**机器人探索算法**，该算法使用状态价值函数来预测未来状态的价值，结合了离线蒙特卡罗训练、在线时间差分自适应以及基于传感器信息覆盖范围的内在奖励函数。为了解决开放世界环境中的意外输入问题，范等人将主动识别视为一个顺序证据收集过程，在证据组合理论下提供逐步的不确定性量化和可靠的预测，同时通过专门设计的奖励函数有效地描述开放世界环境中行动的价值。 

> [!NOTE]
> 是主动交互的，不是被动的，来进行环境的观测和理解。

### 三维视觉定位
与在平面图像范围内进行操作的传统**二维视觉定位**（Visual Grounding，VG）不同，**三维视觉定位**（VG）融入了**深度、视角以及物体之间的空间关系**，为智能体与环境进行交互提供了一个更强大的框架。三维视觉定位的任务是使用自然语言描述，在三维环境中定位物体。如表五所总结的那样，近期三维视觉定位的方法大致可以分为两类：**两阶段方法**和**一阶段方法**。 
1) **2阶段3D视觉定位方法**：与相应的二维任务[146]类似，早期关于三维定位的研究主要采用两阶段的先检测后匹配流程。它们首先使用预训练的检测器[147]或分割器[148]–[150]从三维场景中的众多物体提议中提取特征，然后将这些特征与语言查询特征相融合，以匹配目标物体。两阶段研究的重点主要在于第二阶段，例如探索**物体提议特征与语言查询特征之间的相关性**，从而选择匹配度最高的物体。**ReferIt3D**[130]和**TGNN**[131]不仅学习将提议特征与文本嵌入进行匹配，还通过图神经网络对物体之间的上下文关系进行编码。为了在自由形式的描述和不规则的点云中增强三维视觉定位效果，**FFL-3DOG**[133]利用语言场景图来处理短语之间的相关性，利用多层三维提议关系图来丰富视觉特征，并利用描述引导的三维视觉图来对全局上下文进行编码。 
最近，由于**Transformer架构**在自然语言处理[151]、[152]以及计算机视觉任务[14]、[153]中展现出了卓越的性能，相关研究越来越多地聚焦于在三维视觉定位任务中使用Transformer来提取和融合视觉语言特征。例如，**LanguageRefer**[135]采用了一种基于Transformer的架构，将三维空间嵌入、语言描述以及类别标签嵌入相结合，以实现稳健的三维视觉定位。**3DVG-Transformer**[134]是一种针对三维点云的关系感知型视觉定位方法，其特点是拥有一个坐标引导的上下文聚合模块，用于生成关系增强的提议，以及一个多路注意力模块，用于消除跨模态提议的歧义。 
为了能够对三维物体和指代表达进行更细粒度的推理，**TransRefer3D**[154]利用实体和关系感知注意力来增强跨模态特征表示，其中融合了自注意力、实体感知注意力和关系感知注意力。**GPS**[140]提出了一个统一的学习框架，通过利用三个层次的对比对齐学习和掩码语言建模目标学习，从百万量级的三维视觉语言数据集（即**SCENEVERSE**[140]）中提取知识。上述大多数用于三维视觉定位（VG）的方法都专注于特定的视角，但当视角发生变化时，所学习到的视觉语言对应关系可能会失效。为了学习到更具视角鲁棒性的视觉表示，**MVT**[137]提出了一种多视图Transformer，它能够学习与视角无关的多模态表示。为了减轻点云数据稀疏、有噪声且不完整的局限性，各种方法都在探索结合从捕获的（例如，**SAT**[132]）或合成的（例如，**LAR**[136]）图像中获取的详细二维视觉特征，以增强三维视觉定位任务的效果。 
**现有的三维视觉定位（VG）方法通常依赖大量的带标注数据进行训练，或者在处理复杂的语言查询时表现出局限性**。受**大语言模型**（LLMs）强大的语言理解能力的启发，**LLM-Grounder**[138]提出了一种无需带标注数据的开放词汇三维视觉定位流程。该流程利用大语言模型分解查询内容，并生成用于物体识别的方案，然后通过评估空间关系和常识关系来选择最匹配的物体。为了捕捉依赖视角的查询，并解读三维空间中的空间关系，**ZSVG3D**[139]设计了一种零样本开放词汇三维视觉定位方法。该方法使用大语言模型来识别相关物体并进行推理，将这一过程转化为脚本化的视觉程序，然后再转化为可执行的Python代码，以预测物体的位置。 
然而，如下图（b）所示，这些两阶段方法面临着**确定提议数量的两难境地**，因为第一阶段的三维检测器需要对关键点进行采样，以表示整个三维场景，并为每个关键点生成相应的提议。稀疏的提议可能会在第一阶段忽略目标，从而导致在第二阶段无法匹配。相反，密集的提议可能包含不可避免的冗余物体，由于提议之间的关系过于复杂，使得在第二阶段难以区分目标。此外，关键点采样策略与语言无关，这增加了检测器识别与语言相关提议的难度。 

![图8. 两阶段（上）和一阶段（下）三维视觉定位方法示意图[141]。（a）展示了三维视觉定位的示例。（b）两阶段方法包括在检测阶段可能会忽略目标的稀疏提议，以及在匹配阶段可能会造成混淆的密集提议。（c）一阶段方法可以在语言描述的引导下逐步选择关键点（蓝点→红点→绿点） 。 ](<截屏2025-02-23 17.50.39.png>)

2) **一阶段三维视觉定位方法**：如上图（c）所示，与两阶段三维视觉定位方法不同，一阶段三维视觉定位方法将物体检测和**在语言查询引导下的特征提取**相结合，使得定位物体更加容易。 
   **3D-SPS**[141]将三维视觉定位任务视为一个**关键点选择**问题，从而避免了检测和匹配的分离。具体而言，3D-SPS首先通过描述感知关键点采样模块粗略地对与语言相关的关键点进行采样。随后，它通过面向目标的渐进挖掘模块精细地选择目标关键点，并预测定位结果。受诸如**MDETR**[155]和**GLIP**[156]等二维图像语言预训练模型的启发，**BUTD-DETR**[142]提出了一种自下而上再自上而下的检测Transformer可用于二维和三维视觉定位。具体来说，BUTD-DETR利用带标注的自下而上的边界框提议以及自上而下的语言描述，通过**预测头**来引导对目标物体和相应语言跨度的解码。
   然而，这些方法要么提取将所有单词耦合在一起的句子级特征，要么更多地关注描述中的物体名称，这会丢失单词级别的信息或忽略其他属性。为了解决这些问题，**EDA**[143]明确地将句子中的文本属性解耦，并在细粒度的语言和点云物体之间进行了密集对齐。首先，长文本被解耦为五个语义成分，包括主要物体、辅助物体、属性、代词和关系。随后，设计的密集对齐方式用于将所有与物体相关的解耦后的文本语义成分与视觉特征进行对齐。为了从隐含指令中推断出人类意图，**ReGround3D**[144]设计了一个以视觉为中心的推理模块（由掩码语言模型驱动），以及一个三维定位模块，该模块通过重新审视三维场景中增强的几何信息和细粒度细节来准确获取物体的位置。此外，还采用了一种“定位链”机制，通过交错的推理和定位步骤来改进三维推理定位。 

> [!NOTE]
> 定位是交互的基础。只有通过定位方法找到实体，具身才可以进行相应的具体交互。

### 视觉语言导航
**视觉语言导航**（Visual Language Navigation，VLN）是计算机视觉和自然语言处理（NLP）技术在导航领域中的重要应用。它通过将视觉信息和自然语言描述结合在一起，实现智能导航和智能规划。**视觉语言导航**（VLN）是具身人工智能的一个关键研究问题，其目标是使智能体能够根据语言指令在未知环境中进行导航。视觉语言导航要求机器人理解复杂多样的视觉观测信息，同时以不同的粒度解读指令。视觉语言导航的输入通常由两部分组成：**视觉信息**和**自然语言指令**。视觉信息可以是过去轨迹的视频，也可以是一组涵盖历史和当前情况的观测图像。自然语言指令包括具身智能体需要到达的目标，或者是期望具身智能体完成的任务。具身智能体必须利用上述信息，从候选动作列表中选择一个或一系列动作，以满足自然语言指令的要求。这一过程可以表示为： $$Action=\mathcal{M}(O,H,I)$$ 其中，Action（动作）是选定的动作或候选动作列表，O是当前观测信息，H是历史信息，I是自然语言指令 。 
**成功率**（Success Rate, SR）、**轨迹长度**（Trajectory Length, TL）和路**径长度加权成功率**（Success Weighted by Path Lengt, SPL）是视觉语言导航（VLN）中最常用的评估指标。其中，成功率直接反映具身智能体的导航性能，轨迹长度反映导航效率，而路径长度加权成功率则综合了这两方面，用于表明具身智能体的整体性能。下面，我们将**从数据集和方法**两个部分来介绍视觉语言导航。 
1) **数据集**：在视觉语言导航（VLN）中，自然语言指令可以是一系列详细的动作描述、对目标的完整描述，也可能只是对任务的大致描述，甚至仅仅是人的需求。具身智能体需要完成的任务可能只是一次单一的导航，也可能是涉及交互的导航，或者是需要按顺序完成的多项导航任务。这些差异给视觉语言导航带来了不同的挑战，因此构建了许多不同的数据集。基于这些差异，我们来介绍一些重要的视觉语言导航数据集。 
   **房间到房间（R2R）**[63]是一个基于Matterport3D的视觉语言导航（VLN）数据集。在R2R数据集中，具身智能体根据分步指令进行导航，依据视觉观测选择下一个相邻的**导航图节点**来前进，直至到达目标位置。具身智能体需要动态跟踪进展情况，以使导航过程与细粒度的指令保持一致。“**房间间导航**（Room-for-Room）”[157]将R2R中的路径扩展为更长的轨迹，这要求具身智能体具备更强的长距离指令理解和历史信息对齐能力。视觉语言导航连续环境数据集（VLN-CE）[158]将R2R和R4R扩展到了连续环境中，具身智能体可以在场景中自由移动。这使得具身智能体的动作决策变得更加困难。与上述基于室内场景的数据集不同，“**触达**（TOUCHDOWN）”数据集[159]是基于谷歌街景创建的。在TOUCHDOWN数据集中，具身智能体按照指令在纽约市的街景渲染模拟环境中进行导航，以找到指定的物体。 
   与房间到房间（R2R）数据集类似，“**幻想**（REVERIE）”数据集[160]也是基于Matterport3D模拟器构建的。REVERIE数据集要求具身智能体根据由人工标注的简洁、高层次的自然语言指令，准确地定位远处不可见的目标物体。这意味着具身智能体需要在大量的物体中找到目标物体。在“**SOON**”[161]数据集中，具身智能体接收一条从粗到细的长而复杂的指令，以便在三维环境中找到目标物体。在导航过程中，具身智能体首先搜索较大的区域，然后根据视觉场景和指令逐渐缩小搜索范围。这使得“SOON”的导航以目标为导向，且与初始位置无关。“**DDN**”[162]数据集比上述这些数据集更进一步，它只提供人类需求，而不指定明确的物体。具身智能体需要在场景中进行导航，以找到满足人类需求的物体。
   **ALFRED数据集**[163]基于AI2-THOR模拟器。在ALFRED数据集中，具身智能体需要理解环境观测信息，并在交互式环境中根据粗粒度和细粒度的指令完成家务任务。**OVMM**[164]的任务是在任何陌生环境中挑选任意一个物体，并将其放置在指定位置。智能体需要在家庭环境中定位目标物体，导航至该物体处并抓取它，然后再导航至目标位置放下物体。OVMM提供了一个基于Habitat的模拟环境以及一个可在现实世界中实施的框架。行为1K（Behavior-1K）数据集[165]是基于人类需求构建的，包含1000个长序列、复杂且依赖技能的日常任务，这些任务是在OmniGibson（iGibson模拟环境的扩展）中设计的。智能体需要根据视觉信息和语言指令完成包含数千个低级动作步骤的长跨度导航交互任务。这些复杂任务需要强大的理解能力和记忆能力。 
   也有一些更为特殊的数据集。**CVDN**[166]要求具身智能体根据对话历史记录导航至目标位置，并且在不确定时通过提问求助来决定下一步行动。**DialFRED**[167]是ALFRED数据集的扩展版本，它允许智能体在导航和交互过程中提出问题以获取帮助。这些数据集都引入了额外的“神谕oracles”机制，具身智能体需要通过提问来获取更多有利于导航的信息。 
   ![表六 不同视觉语言导航（VLN）数据集的比较。](<截屏2025-02-23 22.59.36.png>)


2) **方法**：近年来，随着大语言模型（LLMs）展现出惊人的性能，视觉语言导航（VLN）取得了巨大进展，其发展方向和重点也受到了深远影响。尽管如此，视觉语言导航方法可以分为两个方向：**基于记忆理解的方法**和**基于未来预测的方法**。
   - **基于记忆理解的方法**侧重于对环境的感知和理解，以及基于历史观测或轨迹的模型设计，这是一种基于过往学习的方法。
   - **基于未来预测的方法**则更注重对未来状态进行建模、预测和理解，这是一种面向未来学习的方法。
  
   由于视觉语言导航（VLN）可以被看作是一个部分可观测的马尔可夫决策过程，在这个过程中，未来的观测结果取决于智能体当前所处的环境以及所采取的行动，因此历史信息对于导航决策，尤其是长跨度的导航决策具有重要意义，正因如此，**基于记忆理解的方法一直是视觉语言导航领域的主流方法**。然而，基于未来预测的方法同样具有重要意义。其对环境的本质性理解在连续环境的视觉语言导航中极具价值，特别是随着世界模型这一概念的兴起，基于未来预测的方法正受到研究人员越来越多的关注。 
   - **基于记忆理解的方法**。**基于图的学习**是基于记忆理解方法的重要组成部分。基于图的学习通常以图的形式来表示导航过程，在这个过程中，具身智能体在每个时间步获取的信息被编码为图的节点。具身智能体获取全局或部分导航图信息，以此作为历史轨迹的一种表示。**LVERG**[168]分别对每个节点的语言信息和视觉信息进行编码，设计了一种新的语言和视觉实体关系图，用于对文本和视觉之间的跨模态关系以及视觉实体之间的模态内关系进行建模。**LM-Nav**[172]使用了一种以目标为条件的距离函数来推断原始观测集合之间的连接，并构建一个导航图，并通过一个大语言模型（LLM）从指令中提取地标，使用一个视觉语言模型将这些地标与导航图的节点进行匹配。尽管HOP[173]并非基于图学习，但其方法与图类似，要求模型对不同粒度的按时间排序的信息进行建模，从而实现对历史轨迹和记忆的深度理解。
   导航图将环境进行了**离散化处理**，但同时对环境进行理解和编码也同样重要。**FILM**[171]在导航过程中利用RGB-D观测数据和语义分割技术，从三维体素逐步构建出语义地图。**VER**[178]通过二维到三维的采样方式，将物理世界量化为结构化的三维单元，提供了细粒度的几何细节和语义信息。
   不同的学习方案探索了如何更好地利用历史轨迹和记忆。通过**对抗学习**，**CMG**[169]在**模仿学习和探索激励方案**之间交替进行，有效地加强了对指令和历史轨迹的理解，缩短了训练和推理之间的差异。**GOAT**[177]通过后**门调整因果学习**（BACL）和**前门调整因果学习**（FACL）直接训练无偏差模型，将视觉信息、导航历史信息及其组合与指令进行对比学习，使智能体能够更充分地利用信息。**RCM**[170]提出的**增强型跨模态匹配方法**，利用面向目标的外部奖励和面向指令的内部奖励，在全局和局部进行跨模态基础构建，并通过自监督模仿学习从自身过去的良好决策中学习。**FSTT**[175]将测试时自适应（TTA）引入到视觉语言导航（VLN）中，并在时间步长和任务这两个尺度上从梯度和模型参数方面对模型进行优化，有效地提高了模型性能。
   **大模型在基于记忆理解的方法中的具体应用是理解历史记忆的表征，并基于其广泛的世界知识来理解环境和任务**。**NaviLLM**[174]通过视觉编码器将历史观测序列整合到嵌入空间中，将融合编码后的多模态信息输入到大语言模型（LLM）中并对其进行微调，在多个基准测试中达到了领先水平。**NaVid**[179]在历史信息的编码方面进行了改进，通过不同程度的池化操作，在历史观测和当前观测上实现了不同程度的信息保留。**DiscussNav**[176]为不同的角色分配了具有不同能力的大模型专家，促使大模型在导航行动前进行讨论以完成导航决策，并在零**样本视觉语言导航**（VLN）中取得了优异的性能。 
  
> 通过历史信息和环境条件进行“预测”导航。

   - **基于未来预测的方法**：基于图的学习在基于未来预测的方法中也有广泛应用。**BGBL**[182]和**ETPNav**[185]采用了类似的方法，设计了一种路标点预测器。这种预测器能够基于对当前导航图节点的观测，在连续环境中预测可移动的路径点。它们的目的是将连续环境中的复杂导航转化为离散环境中的节点到节点的导航，从而弥合从离散环境到连续环境的性能差距。 
   通过**环境编码**来提升对未来环境的理解和感知，也是预测和探索未来的研究方向之一。**NvEM**[181]使用了一个主题模块和一个参考模块，从全局和局部的视角对相邻视图进行融合编码。这实际上是对未来观测的一种理解和学习。**HNR**[184]使用了一个大规模预训练的分层神经辐射表示模型，通过三维特征空间编码直接预测未来环境的视觉表征，而非预测像素级别的图像，并基于对未来环境的表征构建了一棵**可导航的未来路径树**。他们从不同层面预测未来环境，为导航决策提供了有效的参考。 
   **一些强化学习方法也被应用于预测和探索未来状态**。**LookBY**[180]运用了强化预测技术，使预测模块能够模拟现实世界，并对未来状态和奖励进行预测。这使得智能体能够将“当前观测结果”以及“对未来观测结果的预测”直接映射为行动，在当时达到了最先进的性能水平。大型模型丰富的世界知识和零样本学习性能，为基于未来预测的方法提供了多种可能性。**MiC**[183]要求大语言模型（LLM）直接从指令中预测目标及其可能的位置，并通过对场景感知的描述来给出导航指令。这种方法要求大语言模型充分发挥其“想象力”，并通过提示构建出一个想象中的场景。 

> 通过可能的导航结果预测未来环境，来为当前的导航提供线索。

此外，还有一些方法既能从过去中学习，又能着眼于未来。**MCR-Agent**[186]设计了一种三层行动策略，该策略要求模型根据指令预测目标，预测待交互目标的像素级掩码，并从前有的导航决策中学习；**OVLM**[187]要求大语言模型（LLMs）根据指令预测相应的操作和地标序列。在导航过程中，视觉语言地图将持续更新和维护，并且操作会与地图上的路点关联起来。 

![表7 不同视觉语言导航（VLN）方法的比较。](<截屏2025-02-23 23.00.32.png>)

### 非视觉感知：触觉
**触觉传感器**（tactile sensor）为智能体提供诸如纹理、硬度和温度等详细信息。对于同一个动作，从视觉传感器和触觉传感器中所学到的知识可能是相互关联且互补的，这使得机器人能够充分掌握手头的**高精度任务**。因此，触觉感知对于现实物理世界中的智能体来说至关重要，并且无疑会增强人机交互[188]–[190]。 
对于触觉感知任务，智能体需要从物理世界中收集触觉信息，然后执行复杂的任务。在本节中，如下图所示，我们首先介绍现有的触觉传感器类型及其对应的数据集，然后讨论触觉感知中的三项主要任务：**估计、识别和操作**。 

![图10. 不同类型的触觉传感器。非视觉传感器（a）主要使用力、压力、振动和温度传感器来获取触觉信息。基于视觉的触觉传感器（（b）-（e））基于光学原理。将一个摄像头放置在凝胶后方，利用来自不同方向光源的照明来记录凝胶的变形图像。（a）-（e）分别是BioTac、Gelsight、DIGIT、9DTact和Gelsilm触觉传感器的细节展示。](<截屏2025-02-24 09.43.16.png>)

1) **传感器设计**：人类触觉的原理是皮肤在受到触摸时会发生形状变化，其丰富的神经细胞会发送电信号，这也成为了设计触觉传感器的基础。触觉传感器的设计方法可可分为三类：**基于非视觉的**、**基于视觉**的以及**多模态**的。基于非视觉的触觉传感器主要运用**电学和力学原理**，主要记录力、压力、振动和温度等基本的低维感官输出信息[191]–[196]。其中一个显著的代表是**BioTac**[197]及其模拟器[198]。基于视觉的触觉传感器则基于光学原理。像GelSight[199]、Gelslim[200]、DIGIT[201]、9DTact[202]、TacTip[203]、GelTip[204]和AllSight[205]等基于视觉的触觉传感器，将凝胶变形的图像用作触觉信息，已被应用于众多领域。像TACTO[206]和Taxim[207]这样的模拟器也颇受欢迎。近期的研究工作主要集中在降低成本[202]以及将其集成到机器人手上[201]、[208]、[209]。多模态触觉传感器受人类皮肤的启发，利用柔性材料和模块化设计，融合了压力、接近度、加速度和温度等多模态信息。
2) **数据集**：基于非视觉传感器的数据集主要由**BioTac系列**[197]收集，包含电极值、力向量以及接触位置信息。由于相关任务主要是力的估计以及抓取细节方面的，所以数据集中的对象通常是**力和抓取样本**。基于视觉的传感器能获取变形凝胶的高分辨率图像，其更侧重于更高级的估计、纹理识别以及操作任务。这些数据集由GelSight传感器、DIGIT传感器及其模拟器[199]、[201]、[202]、[206]收集，涵盖了家用物品、野外环境、不同材料以及抓取物品等方面的数据。由于图像信息能够很容易地与其他模态（图像、语言、音频等）进行对齐和关联[14]、[210]，具身智能体中的触觉感知主要围绕基于视觉的传感器展开。我们介绍十个主要的触觉数据集，相关内容总结在下表中。 
    
![不同的触感数据集对比](<截屏2025-02-24 09.48.17.png>)

3) **方法**：触觉感知有着众多的应用，并且可以被分为三类：**估计**、**精确机器人操作**，以及**多模态识别任务**。
   1) **估计**：早期关于估计的研究工作主要集中在用于**形状、力和滑动测量**的基本算法上[202]、[220]、[221]。研究人员只是基于触觉图像的颜色以及不同帧中标记分布的变化，使用一个阈值或者应用卷积神经网络（CNN）来完成这些任务。估计工作的重点主要在于第二个阶段，**即触觉图像的生成以及物体的重建**。触觉图像的生成[222]–[225]旨在从视觉数据中生成触觉图像。起初，人们应用深度学习模型，将RGB-D图像作为输入并输出触觉图像[222]、[223]。最近，随着图像生成技术的快速发展，伊格拉等人[224]以及杨等人[225]将扩散模型应用于触觉图像生成，且效果良好。物体的重建可以分为二维重建[226]、[227]和三维重建[202]、[219]、[228]–[241]。二维重建主要关注物体的形状和分割，而三维重建则侧重于物体的表面和姿态，甚至是全场景感知。这些任务最初采用数学方法、自动编码器方法以及神经网络方法，将视觉特征（有时是点云）和触觉特征融合在一起。最近，像科米等人[236]和窦等人[219]这样的研究人员，将基于**神经辐射场**（NeRF）和**三维高斯溅射**（3DGS）的新方法应用到了触觉重建工作中。

> 衡量触觉，重建触觉表面。

   2) **机器人操作**：在触觉任务中，弥合模拟与现实之间的差距极为重要。人们已经提出了**强化学习**和基于**生成对抗网络**（GAN）的方法，以应对精确、实时的机器人操作任务中的各种变化情况。 
      - **强化学习方法**。**视觉触觉强化学习**（Visuotactile-RL）[242]针对现有的强化学习方法提出了几种方案，包括触觉门控、触觉数据增强和视觉退化。“**Rotateit**”[243]是一个系统，它利用多模态感官输入，能够实现基于指尖的物体多轴旋转。**该系统通过带有特权信息的强化学习策略来训练网络，并支持在线推理**。[244]提出了一种仅使用触觉感知来进行物体推动的深度强化学习方法。它提出了一种**目标条件公式**，使得无模型和基于模型的强化学习都能获得将物体推到目标位置的精确策略。“**AnyRotate**”[245]专注于手部内操作。它是一个利用密集特征的模拟到现实的触觉来实现不受重力影响的手部内物体多轴旋转的系统，构建了连续的接触特征表示，以便为在模拟中训练策略提供触觉反馈，并引入了一种通过训练观察模型来弥合模拟与现实之间的差距，从而实现**零样本策略迁移**的方法。 
      - **基于生成对抗网络（GAN）的方法**。**ACTNet** [246] 提出了一种无监督的对抗域自适应方法，以缩小像素级触觉感知任务中的域差距。该方法引入了一种**自适应相关注意力机制**来改进生成器，这种机制能够利用全局信息并聚焦于显著区域。然而，像素级的域自适应会导致误差累积、性能下降，并且增加了结构复杂性和训练成本。相比之下，**STR-Net**[247] 提出了一种用于触觉图像的**特征级无监督框架**，缩小了在特征层面的触觉感知任务方面存在主要差距。此外，一些方法专注于从模拟到真实的转换。例如，**触觉健身房2.0**（Tactile Gym 2.0）[248]。然而，由于其复杂性和高成本，在实际应用中存在挑战。 

> 经过触觉生成机器人操作的方法，基于深度学习的方法是主流，尤其是利用强化学习和GAN。

   3) **识别**：触觉表征学习专注于材料分类和多模态理解，这可以分为两类：**传统方法**以及**大语言模型和视觉语言模型方法**。 
      - **传统方法**。人们采用了各种传统方法来加强**触觉表征学习**。**自动编码器框架**在开发紧凑的触觉数据表征方面发挥了重要作用。波利克等人[249]使用卷积神经网络自动编码器对基于光学的触觉传感器图像进行降维处理。高等人[250]创建了一个有监督的循环自动编码器来处理异构传感器数据集，而曹等人[251]创建了TacMAE，使用掩码自动编码器来处理不完整的触觉数据。张等人[252]引入了MAE4GM，这是一种集成了视觉-触觉数据的多模态自动编码器。由于触觉是对其他模态的一种补充，联合训练方法被用于融合多种模态。袁等人[253]使用包括深度、视觉和触觉数据等模态来训练卷积神经网络。同样，李等人[254]使用变分贝叶斯方法来处理力传感器序列和末端执行器指标等模态数据。为了实现更好的学习表征，诸如对比学习之类的自监督方法也是将不同模态结合起来的关键技术。在对比方法上，不同的研究存在差异。林等人[255]只是简单地将触觉输入与多种视觉输入进行配对，而杨等人[256]采用了视觉触觉对比多视图特征。克尔等人[215]使用了InfoNCE损失函数，古泽伊等人[257]使用了**自引导对比学习**（BYOL）方法。这些传统方法为触觉表征学习奠定了坚实的基础。 
      - **大语言模型和视觉语言模型方法**。最近，大语言模型（LLM）和视觉语言模型（VLM）在跨模态交互理解方面展现出了惊人的能力，并且具有强大的零样本学习性能。 杨等人[189]、傅等人[218]和于等人[258]的最新研究通过**对比预训练**方法，对触觉数据与视觉和语言模态数据进行了编码和对齐。然后，像大语言模型（LLaMA）这样的模型会被应用，通过微调方法来适配诸如触觉描述之类的任务。大语言模型和视觉语言模型技术的出现进一步推动了该领域的发展，使得能够生成更全面、更稳健的跨模态触觉表征。 

> 主要是通过触觉感知识别材料，主要适用VAE的方法实现端到端识别。也强调了一下大语言模型在跨模态信息上的巨大潜力。

   4) **难点**：
      1) **不同传感器类型的缺点**：传统传感器提供的是简单的低维数据，这对多模态学习来说颇具挑战。基于视觉的传感器和电子皮肤虽然精度很高，但成本昂贵。
      2) **数据采集方面的挑战**：尽管在开发简化的数据采集设备方面取得了一些进展，但收集数据，尤其是同时采集触觉和视觉数据仍然很困难.
      3) **标准不一致**：触觉传感器的运行标准和原理各不相同，这阻碍了大规模学习，并限制了公共数据集的实用性。目前需要标准化且广泛的数据集。 

## 具身交互

[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

具身交互任务是指智能体在真实物理空间或模拟空间中与人类及环境进行交互的场景。典型的具身交互任务包括**具身问答**（EQA）和**具身抓取**。

> 大概就是因为我的实习内容涉及了机器人工件抓取所以被分到了这个具身智能的部门了吧。

### 具身问答
对于**具身问答（EQA）任务**，智能体需要从第一人称视角探索环境，以收集回答给定问题所需的信息。具备**自主探索和决策能力的智能体**不仅要考虑采取哪些行动来探索环境，还必须确定何时停止探索以便回答问题。现有研究工作关注不同类型的问题，其中一些问题如下图所示。在本节中，我们将介绍现有的**数据集**，讨论相关的**方法**，描述用于评估模型性能的**指标**，并阐述这项任务仍然存在的**局限性**。 

![图11. 灰色方框展示了智能体在探索过程中所观察到的场景。其他方框展示了各类问答任务。除了基于情景记忆来回答问题的任务之外，一旦智能体收集到了足以回答问题的信息，它就会停止探索。](<截屏2025-02-24 10.26.03.png>)

1) **数据集**：在真实环境中进行机器人实验常常受到场景和机器人硬件的限制。作为**虚拟实验平台**，模拟器为构建具身问答数据集提供了合适的环境条件。在模拟器中创建的数据集上对模型进行训练和测试，能显著降低实验成本，并提高在真实机器上部署模型的成功率。我们简要介绍几个具身问答数据集，相关内容总结在下表中。 
![alt text](<截屏2025-02-24 22.46.23.png>)

- **EQA v1**版本[259]是首个为具身问答任务设计的数据集。它基于House3D[269]模拟器中来自SUNCG数据集[95]的合成三维室内场景构建而成。EQA v1版本包含四种类型的问题：**位置问题**、**颜色问题**、**带颜色的房间问题**以及**介词问题**。它的特点是有超过5000个问题，分布在750多个环境中。这些问题是通过函数程序执行来构建的，使用模板来选择和组合基本操作。 与具身问答（EQA）v1版本类似，
- **多任务具身问答**（MT-EQA）[260]也是在House3D模拟器中，利用SUNCG数据集，通过执行由一些基本操作组成的函数程序构建而成的。然而，它进一步将单物体问答任务扩展到了多物体的情境中。该数据集设计了**六种类型的问题**，涉及多个物体之间颜色、距离和大小的比较。该数据集在588个环境中包含了19287个问题。 
- **MP3D-EQA** [261] 是基于一个在MINOS [270] 基础上开发的模拟器构建的，使用了Matterport3D数据集 [271]，将问答任务扩展到了**逼真的三维环境中**。参照EQA v1，MP3D-EQA 使用了三种类型的模板**：位置、颜色和带颜色的房间**，在83个家庭环境中总共生成了1136个问题。
- **IQUAD V1** [262] 建立在AI2-THOR之上，由三种类型的问题组成：**存在性问题、计数问题和空间关系问题**。它使用一组预先编写好的模板来生成超过75000个选择题，每个问题都配有一个独特的场景配置。与其他数据集不同的是，回答IQUAD V1的问题需要智能体对可供性有很好的理解，并与动态环境进行交互。
- **VideoNavQA** [263] 将**视觉推理与EQA问题中的导航方面分离开来**。在这项任务中，智能体访问与探索轨迹相对应的视频，这些视频包含足够的信息来回答问题。同样参照EQA v1，VideoNavQA 根据功能性的、模板式的表达方式来生成问题。它还**渲染出最短轨迹**，以模拟接近最优的导航路径，创建出与智能体在探索环境时会看到的画面相对应的视频。VideoNavQA 在使用SUNCG的House3D环境中生成了大约101000组视频和问题，涵盖了属于存在性、计数和定位等8个类别的28种类型的问题。
- **SQA3D** [264] 简化了流程（仅问答），同时仍然保留了对具身场景理解进行基准测试的功能，能够处理更复杂、知识密集型的问题，并且可以进行更大规模的数据收集。具体来说，SQA3D 提供了一个数据集，该数据集基于ScanNet [272] 场景，包含约6800个独特的情境、20400条描述以及针对这些情境的33400个不同的推理问题。
- 与之前在问题中明确指定目标物体的数据集不同，**K-EQA** [265] 的特点是包含带有**逻辑子句和与知识相关短语的复杂问题**，需要先验知识才能回答。它是在AI2Thor中构建的，包括四种类型的问题：**存在性问题、计数问题、列举问题和比较问题**。每个实体都被映射到一个知识库中，并且进一步构建了一个知识图谱。在这项工作中，**IQA** [262] 和**MT-EQA** 中提供的模板扩展成了一组语法规则。在指定了物体和逻辑关系之后，引入了知识图谱、场景图谱等，以生成问题并计算出正确答案。最终得到的K-EQA数据集包含了在6000种不同环境设置下的60000个问题。  
- **OpenEQA** [266] 是首个用于具身问答（EQA）的开放词汇数据集，支持**情景记忆**和**主动探索**两种情况。**情景记忆具身问答**（EM-EQA）任务要求智能体从其情景记忆中形成对环境的理解，进而回答问题，这与VideoNavQA类似。在主动具身问答（A-EQA）任务中，智能体通过采取探索性行动来收集必要信息以回答问题。利用ScanNet和HM3D [273]，人工标注员从Habitat中180多个真实世界环境里构建了1600多个高质量问题。 
- **HM-EQA** [267] 利用**GPT4**-V技术，在Habitat模拟器中基于HM3D构建而成。它涵盖了267个不同场景下的500个问题，这些问题大致可分为**识别**、**计数**、**存在性**、**状态**以及**位置**这几类。为保持一致性，每个问题都设有四个选项的选择题。 
> 这些数据集还是包括了场景和问题以及答案，意思是能够让具身机器人了解场景。
- **S-EQA** [268] 在虚拟家园（VirtualHome）环境中利用GPT-4进行数据生成，并采用余弦相似度计算来决定是否保留所生成的数据，从而提高了数据集的多样性。在S-EQA中，回答问题需要对一系列达成共识的物体和状态进行评估，以得出 “**是/否**” 的存在性答案。 

2) **方法**：具身问答任务主要涉及**导航**和**问答子任务**，其实现方法大致可分为两类：**基于神经网络的方法**和**基于大语言模型 / 视觉语言模型的方法**。
   - **神经网络方法**。在早期的研究工作中，研究人员主要通过构建深度神经网络来处理具身问答任务。他们运用**模仿学习**和**强化学习**等技术对这些模型进行训练和微调，以提升其性能。 
    **具身问答**（EQA）任务最初是由达斯（Das）等人[259]提出的。在他们的研究中，智能体由四个主要模块组成：**视觉模块**、**语言模块**、**导航模块**和**问答模块**。这些模块主要是使用传统的神经构建模块——**卷积神经网络**（CNN）和**循环神经网络**（RNN）来构建的。它们分两个阶段进行训练。**首先**，利用模仿学习或监督学习，在自动生成的专家导航演示数据上对导航模块和回答模块分别进行训练。随后，**在第二阶段**，使用策略梯度对导航架构进行微调。后续的一些研究[274]、[275]保留了达斯等人[259]提出的诸如问答模块之类的组件，并对模型进行了改进。此外，吴（Wu）等人[275]提出将导航模块和问答模块整合到一个统一的随机梯度下降（SGD）训练流程中进行联合训练，从而避免了使用**深度强化学习**来同时训练那些分别训练过的导航模块和问答模块。 
    也有一些研究尝试增加问答任务的复杂性和完整性。从任务单一性的角度来看，几项研究[260]、[276]分别将任务扩展到涵盖**多个目标和多个智能体**的情况，这就要求模型必须通过**特征提取**和**场景重建**等方法，存储并整合智能体在探索过程中获取的信息。考虑到智能体与动态环境之间的交互，戈登（Gordon）等人[262]引入了**分层交互式记忆网络。**控制权在负责任务选择的规划器和执行任务的低层控制器之间交替。在此过程中，利用以自我为中心的空间门控循环单元（esGRU）来存储空间记忆，使智能体能够进行导航并给出答案。先前的研究还存在一个局限性，即智能体无法利用外部知识来回答复杂问题，并且对场景中已探索的部分缺乏了解。为了解决这个问题，谭（Tan）等人[265]提出了一个框架，该框架利用**神经程序合成方法**以及**从知识图谱和三维场景图转换而来的表格**，让动作规划器能够获取与物体相关的信息。此外，还采用了一种基于**蒙特卡洛树搜索**（MCTS）的方法来确定智能体下一步要移动到的位置。 
   - **大语言模型/视觉语言模型方法**。近年来，大语言模型（LLMs）和视觉语言模型（VLMs）取得了持续的进展，并在各个领域展现出了卓越的能力。因此，研究人员尝试应用这些模型来解决具身问答任务，且无需进行任何额外的微调。 
    马宗达尔（Majumdar）等人[266]探索了使用**大语言模型**（LLMs）和**视觉语言模型**（VLMs）来处理情景记忆具身问答（EM-EQA）任务和主动具身问答（A-EQA）任务。对于情景记忆具身问答任务，他们考虑了以下几种情况：**无视觉信息的大语言模型**（Blind LLMs）、**带有情景记忆语言描述的苏格拉底式大语言模型**（Socratic LLMs）、**带有已构建场景图描述的苏格拉底式大语言模型**，以及处理多个场景帧的视觉语言模型。主动具身问答任务在情景记忆具身问答方法的基础上，采用了基于前沿探索（FBE）[277]的方法，以实现与问题无关的环境探索。其他一些研究[267]、[278]也使用了基于前沿探索的方法来确定后续探索的区域，并构建语义地图。他们利用共形预测或图像-文本匹配的方法提前结束探索，以避免过度探索。帕特尔（Patel）等人[279]强调了该任务中问答的方面。他们利用多个基于大语言模型的智能体来探索环境，并使这些智能体能够独立地回答 “是” 或 “否” 的问题。这些单独的回答被用于训练一个中央答案模型，该模型负责汇总这些回答并生成可靠的答案。 

3) **评估指标**：通常从两个方面来评估性能：**导航**和**问答**。
   1) 在**导航**方面，许多研究沿用了达斯（Das）等人[259]提出的方法，并使用诸如导航完成时**到目标物体的距离**（\((d_{T})\)）、从起始位置**到最终位置与目标物体距离的变化量**（\((d_{\Delta})\)）以及在整个过程中**任意时刻到目标物体的最小距离**（\((d_{min })\)）等指标来评估模型的性能。会在距离目标 10 步、30 步或 50 步动作的情况下对其进行测试。也有一些研究是基于**轨迹长度**、**目标物体的交并比得分**（\((IoU)\)）等指标来进行衡量的。
   2) 对于**问答**部分，评估主要涉及**真实答案在答案列表中的平均排名**（\((MR)\)）以及**答案的准确率**。最近，马宗达尔（Majumdar）等人[266]引入了一个基于大语言模型的“聚合”正确性指标（LLM-Match）来评估开放词汇答案的准确性。此外，他们通过纳入智能体路径的**归一化长度**作为正确性指标的权重，来评估效率。 

4) **局限性**：
   1) **数据集**：构建数据集需要大量的人力和资源。此外，大规模的数据集仍然较少，而且不同数据集评估模型性能的指标各不相同，这使得性能测试和比较变得复杂。
   2) **模型**：尽管大语言模型带来了一些进展，但这些模型的性能仍远远落后于人类水平。未来的研究工作可能会更多地聚焦于**有效地存储智能体所探索到的环境信息**，并根据环境记忆和问题引导智能体规划行动，同时还会增强其行动的**可解释性**。 


### 具身抓取
具身交互，除了与人类进行问答交互之外，还包括依据人类指令执行操作，例如**抓取和放置物体**，从而实现机器人、人类与物体之间的交互。具身抓取需要**全面的语义理解、场景感知、决策能力以及可靠的控制规划**。具身抓取方法将传统的机器人运动学抓取与诸如大语言模型[280]和视觉语言基础模型[14]等大型模型相结合，这使得智能体能够在包括视觉主动感知、语言理解和推理在内的多感官感知条件下执行抓取任务。下图展示了人-智能体-物体交互的概况，其中智能体完成具身抓取任务。 

![图 12. 具身抓取任务概述。（a）展示了针对不同类型任务的语言引导抓取示例，（b）提供了人 - 智能体 - 物体交互的概述，（c）展示了关于 “语言引导抓取” 主题的谷歌学术搜索结果。](<截屏2025-02-25 21.20.21.png>)

1) **抓取器**：目前抓取技术的研究重点在于**两指平行抓取器**和**五指灵巧手**。对于两指平行抓取器而言，抓取姿势通常可分为两类：四自由度和六自由度[290]。**四自由度抓取合成方法**[281]、[282]、[286]使用三维位置和从上至下的手部朝向（偏航角）来定义抓取方式，通常被称为“从上至下抓取”。相比之下，**六自由度抓取合成方法**[284]、[291]、[292]则通过六维的位置和朝向来定义抓取姿势。对于五指灵巧手抓取器而言，**ShadowHand**（一种广泛使用的五指机器人灵巧手）具有26个自由度（DOF）。这种高自由度极大地增加了生成有效抓取姿势以及规划执行轨迹的复杂性。 
2) **数据集**：近年来，生成了大量的抓取数据集[281]–[285]。这些数据集通常包含**基于图像**（RGB、深度图像）、**点云**或**三维场景**的带注释的抓取数据。随着**掩码语言模型**（MLM）的出现以及**基础语言模型**在机器人抓取中的应用，迫切需要包含语言文本的数据集。因此，现有的数据集已被扩展或重新构建，以创建**语义抓取数据集**[287]–[289]、[293]。这些数据集对于研究基于语言的抓取模型很有帮助，能让智能体对语义形成广泛的理解。 
   传统的抓取数据集包含**单个物体**[281]和**杂乱场景**[286]的数据，为每个物体提供了符合运动学的**稳定抓取注释**（四自由度或六自由度）。这些数据可以从真实的桌面环境[281]中收集，通常包括**RGB图像数据**、**深度数据**和**点云数据**，也可以从虚拟环境[284]中收集，其中包含图像数据、点云或场景模型。尽管这些数据集对抓取模型很有用，但它们**缺乏语义信息**。为了弥补这一差距，这些数据集已通过语义表达式进行了扩充或扩展[287]、[294]，从而将语言、视觉和抓取联系起来。通过纳入语义信息，智能体能够更好地理解和执行抓取任务。这种改进有助于开发更复杂且具有语义性感知能力更强的抓取模型，从而促进与环境进行更直观、更有效的交互。下表展示了上述数据集，包括**传统的抓取数据集**和**基于语言的抓取数据集**。 

![表十 具身抓取数据集。 ](<截屏2025-02-25 21.20.41.png>)

3) **语言引导抓取**：从这种融合中发展而来的语言引导抓取概念[287]、[288]、[294]，结合了**掩码语言模型**（MLM），使智能体具备**语义场景推理的能力**。这使得智能体能够根据人类的隐性或显性指令执行抓取操作。上图（c）展示了近年来关于语言引导抓取这一主题的论文发表趋势。随着大语言模型（LLM）的发展，研究人员对这一主题的兴趣日益浓厚。目前，抓取研究越来越关注开放世界场景，强调开放集泛化[295]方法。通过利用掩码语言模型的泛化能力，机器人能够在开放世界环境中更智能、高效地执行抓取任务。 
   在语言引导抓取中，语义可以源自**显性指令**[295]、[296]以及**隐性指令**[288]、[289]。显性指令会明确指定要抓取的物体类别，例如一根香蕉或一个苹果。然而，隐性指令则需要通过推理来确定要抓取的物体或物体的某个部分，这涉及到**空间推理**和**逻辑推理**。 
   - **空间推理**[287]是指那些指令可能包含了待抓取物体或物体部分的空间关系，这就需要根据场景中物体间的空间关系来推断抓取姿势。例如，“抓取棕色纸巾盒右边的键盘” 这一指令就涉及对物体空间布局的理解与推断。
   - 另一方面，**逻辑推理**[288]所涉及的指令可能包含逻辑关系，需要通过推理来洞悉人类意图，进而抓取目标物体。比如，“我口渴了，你能给我点喝的吗？” 这样的指令会促使智能体有可能递上一杯水或一瓶饮料。智能体在递物过程中必须确保液体不会洒出，从而生成合理的抓取姿势。 
  
   在这两种情况下，**语义理解与空间和逻辑推理的结合**使智能体能够高效且准确地执行复杂的抓取任务。上图（a）展示了各种类型的语言引导式抓取任务。 
4) **端到端方法**：**CLIPORT** [294] 是一种基于语言条件的模仿学习智能体，它将视觉语言预训练模型CLIP与传输网络（Transporter Net）相结合，构建了一个端到端的双流架构，用于语义理解和抓取生成。它使用从虚拟环境中收集的大量专家演示数据进行训练，使智能体能够执行语义引导的抓取任务。基于OCID数据集，CROG [287] 提出了一个**视觉-语言-抓取数据集**，并引入了一个具有竞争力的端到端基线模型。它利用CLIP的视觉基础能力，直接从图像-文本对中学习抓取合成方法。**推理抓取**（Reasoning Grasping）[288] 基于GraspNet10亿数据集（GraspNet1 Billion dataset）引入了首个推理抓取基准数据集，并提出了一个端到端的推理抓取模型。该模型将**多模态大语言模型与基于视觉的机器人抓取框架相结合**，以便基于语义和视觉来生成抓取动作。**SemGrasp** [289] 是一种基于**语义**的抓取生成方法，它**将语义信息融入到抓取表征中**，从而生成灵巧手的抓取姿势。该方法引入了一种离散表征，使抓取空间与语义空间保持一致，能够根据语言指令生成抓取姿势。为了便于训练，还提出了一个大规模的抓取-文本对齐**数据集CapGrasp**。 

> 这部分有点实习的内容基础，还能勉强好懂一点。

5) **模块化方法**：**F3RM** [295] 试图将CLIP的文本-图像先验知识提升到三维空间中，利用提取的特征进行语言定位，然后生成抓取动作。**它将精确的三维几何形状与来自二维基础模型的丰富语义相结合**，利用从CLIP中提取的特征，通过**自由文本形式的自然语言**来指定要操作的物体。它展示了对未曾见过的表述和新物体类别的泛化能力。**高斯抓取器**（GaussianGrasper）[296] 利用三维高斯场来完成语言引导的抓取任务。所提出的方法首先构建一个**三维高斯场**，然后进行特征提取。接着，利用提取的特征执行基于语言的定位。最后，基于最先进的**预训练抓取网络**[297] 来生成抓取姿势。它将开放词汇语义与精确的几何形状相结合，能够根据语言指令进行抓取。 
   这些方法通过利用**端到端**和**模块化**框架，推动了语言引导抓取领域的发展，从而增强了机器人智能体通过自然语言指令理解和执行复杂抓取任务的能力。具身抓取使机器人能够与物体进行交互，进而提高了它们在**家庭服务**和**工业制造中**的智能水平和实用性。然而，现有的具身抓取方法存在一些局限性，比如依赖大量数据，以及对未见过的数据泛化能力较差。未来的研究将聚焦于提高智能体的通用性，使机器人能够理解更复杂的语义，抓取更多种类的未见过的物体，并完成复杂的抓取任务。 

## 具身智能体

[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

**智能体被定义为一个能够感知其所处环境并采取行动以实现特定目标的自主实体**。**掩码语言模型**（MLM）的最新进展进一步将智能体的应用扩展到实际场景中。当这些基于掩码语言模型的智能体被实体化到物理实体中时能够有效地将它们的能力从虚拟空间转移到物理世界中，从而成为具身智能体[298]。下图展示了具身智能体的发展历程概述。 
![图13. 具身智能体的发展历程概述。不同颜色代表不同的范式。MLM指的是能够直接感知世界并控制具身的多模态语言模型，VLM指的是搭配外部策略模型的视觉语言模型，LLM + VLM指的是基于大语言模型、借助视觉语言模型来感知世界的智能体，而LLM指的是带有视觉情境和外部策略模型的大语言模型。 ](<截屏2025-02-26 09.03.42.png>)
为了使具身智能体能够在信息丰富且复杂的现实世界中运行，具身智能体已发展出强大的**多模态感知**、**交互**和**规划能力**，如下图所示。

为了完成一项任务，具身智能体通常涉及以下过程：
 1. **将抽象而复杂的任务分解为具体的子任务**，这被称为**高级具身任务规划**。
 2. 通过有效利用具身感知和具身交互模型，或借助基础模型的策略功能，逐步执行这些子任务，这被称为**低级具身动作规划**。
![图14. 基于具身多模态基础模型的具身智能体架构图，该架构由视觉感知模块、高级任务规划模块和低级动作规划模块组成。 ](<截屏2025-02-26 09.10.18.png>)
值得注意的是，任务规划涉及在行动前进行**思考**，因此通常是在虚拟空间中进行考量的。相比之下，动作规划必须考虑与环境的有效交互，并将这些信息反馈给任务规划器以调整任务规划。因此，对于具身智能体而言，将其能力**从虚拟空间校准并泛化到物理世界**中是至关重要的。 

### 具身多模态基础模型
具身智能体需要通过视觉识别其所处环境，通过听觉理解指令，并了解自身状态，以便能够进行复杂的交互和操作。这就需要一个整合多种**感官模态**和**自然语言处理能力**的模型，通过综合处理不同类型的数据来增强智能体的理解能力和决策能力。因此，具身多模态基础模型应运而生。最近，谷歌DeepMind发现，利用基础模型和庞大多样的数据集是最优策略。他们开发了一系列基于**机器人Transformer**（RT）[11] 的研究成果，为未来关于具身智能体的研究提供了重要的见解。 
在基础机器人模型领域已经取得了重大进展，这一发展从**SayCan*》[299] 中最初的方法起步，该方法使用三个独立的模型分别进行**规划**、**可供性分析**以及**底层策略制定**。后来，**Q-Transformer** [300] 将可供性分析和底层策略进行了统一，而**PaLM-E** [301] 则整合了规划和可供性分析功能。接着，**RT-2**[302] 取得了一项突破，它将这三项功能整合到了一个单一模型中，实现了联合扩展和正向迁移。这标志着机器人基础模型领域的重大进步。RT-2 引入了**视觉-语言-动作**（VLA）模型，该模型具有 “**思维链**” 推理能力，能够进行多步骤语义推理，比如在不同情境下选择替代工具或饮品。最终，**RT-H** [4] 实现了一个**带有动作层级结构的端到端机器人Transformer**模型，能够在细粒度层面上对任务规划进行推理。 
为了解决具身模型在**通用性**方面的局限，谷歌与33所顶尖学术机构合作，创建了全面的开放**X-具身**（Open X-Embodiment）数据集[303]，该数据集整合了22种不同类型的数据。利用这个数据集，他们训练了通用大模型RT-X。这也促进了更多用于机器人领域的开源视觉语言模型（VLM）的参与，比如**基于LLaVA的具身GPT**（EmbodiedGPT）[304] 以及**基于Flamingo的机器人火烈鸟**（RoboFlamingo）[305]。尽管开放X-具身数据集提供了大量的数据集合，但鉴于具身机器人平台的快速发展，构建数据集仍然是一项挑战。为了解决这个问题，**自动机器人Transformer**（AutoRT）[306] 创建了一个系统，用于在新环境中部署机器人以收集训练数据，并利用大语言模型（LLM），通过更全面和多样的数据来增强学习能力。 
此外，**基于Transformer架构的模型**面临着效率低下的问题，因为具身模型需要包含来自**视觉**、**语言**以及**具身状态**以及与当前执行任务相关的记忆等信息的**长上下文**，这使得基于Transformer架构的模型面临效率低下的问题。例如，尽管RT-2性能强大，但其推理频率仅为1-3赫兹。人们已经做出了一些努力，比如通过**量化和蒸馏**技术来部署模型。此外，改进模型框架是另一种可行的方法。**SARA-RT** [307] 采用了更高效的**线性注意力机制**，而**RoboMamba** [308] 则利用了**mamba架构**，**该架构在处理长序列任务时效率更高**。这使得它的推理速度比现有的机器人多模态语言模型**快七倍**。 

> 这一部分在讲多模态的具身智能体的模型。

基于**生成模型**的RT在**高级任务理解**和**规划**方面表现出色，但在低级动作规划方面存在局限，这是因为生成模型无法精确生成动作参数，且高级任务规划与低级动作执行之间存在差距。为了解决这一问题，谷歌推出了**RT-轨迹**（RT-Trajectory）[309]，它通过自动添加机器人轨迹，为学习机器人控制策略提供了低级视觉线索。同样，在RT-2框架的基础上，带有动作层级结构的机器人Transformer（RT-H）融入了层级动作框架，**通过中间语言动作将高级任务描述与低级机器人动作联系起来**[4]。此外，视觉-语言-动作（VLA）模型仅在与视觉语言模型（VLM）相关的高级规划和可供性任务中展现出涌现能力。它们在低级物理交互中无法展现出新技能，并且受到其数据集中技能类别的限制，导致动作笨拙。未来的研究应该**将强化学习整合到大型模型的训练框架中**，以提高通用性，使VLA模型能够在现实环境中自主学习和优化低级物理交互策略，从而更加灵巧、准确地执行各种物理动作。 

### 具身任务规划
如前所述，对于“把一个苹果放到盘子上”这样的任务，任务规划器会将其分解为子任务，即“找到苹果，拿起苹果”、“找到盘子”以及“放下苹果”。由于如何寻找（导航任务）或者拿起/放下的动作（抓取任务）并不在任务规划的范畴内。这些动作通常是在模拟器中预先设定好的，或者是在现实世界场景中使用预训练的策略模型来执行的，例如使用**CLIPort** [294] 来执行抓取任务。 
传统的具身任务规划方法通常基于**明确的规则和逻辑推理**。例如，像**STRIPS** [310]和**规划域定义语言**（PDDL）[311]这样的符号规划算法，以及蒙特卡洛树搜索（MCTS）[312]和A* [313]等搜索算法，都被用于生成规划方案。然而，这些方法往往依赖于预先定义的规则、约束条件和启发式方法，这些都是固定不变的，可能无法很好地适应环境中的动态变化或意外情况。随着大语言模型（LLM）的普及，**许多研究工作都尝试使用大语言模型进行规划**，或者将传统方法与大语言模型相结合，利用大语言模型中蕴含的丰富世界知识来进行推理和规划，而无需手工定义，这极大地增强了模型的泛化能力。 
1) **利用大语言模型（LLM）涌现能力进行规划**：在自然语言模型扩大规模之前，就像**FILM** [316]所展示的那样，任务规划器也类似地通过在诸如阿尔弗雷德（Alfred）[314]和阿尔夫世界（Alfworld）[315]这样的具身指令数据集上训练像BERT这样的模型来实现。然而，这种方法受到训练集中示例的限制，并且无法有效地与物理世界相契合。如今，多亏了大语言模型的涌现能力，它们能够**利用其内部的世界知识和思维链推理来分解抽象任务**，这类似于人类在行动前思考完成任务的步骤的方式。例如，**翻译语言模型**（Translated LM）[317]和内心独白（Inner Monologue）[318]能够将复杂任务分解为可处理的步骤，并利用它们的内部逻辑和知识系统制定解决方案，而无需额外的训练，就像基于**推理和行动**（ReAct）[319]那样。同样，多智能体协作框架基于阅读的规划（ReAd）[320]被提出，用于通过不同的提示来对规划进行高效的自我完善。此外，一些方法将过去成功的示例抽象为一系列存储在记忆库中的技能，以便在推理过程中加以考虑，从而提高规划的成功率[321]–[323]。还有一些研究工作使用代码而非自然语言作为推理媒介。在这种情况下，任务规划是基于可用的**应用程序编程接口**（API）库以代码的形式生成的[324]–[326]。此外，**多轮推理**能够有效地纠正任务规划中潜在的幻觉，这也是许多基于大语言模型的智能体研究的重点。例如，苏格拉底模型（Socratic Models）[327]和苏格拉底规划器（Socratic Planner）[328]使用苏格拉底式提问来得出可靠的规划方案。 
然而，在任务规划过程中，执行阶段可能会出现潜在的失败情况，这通常是由于规划器没有充分考虑到真实环境的复杂性以及任务执行的难度[318]，[329]。由于缺乏视觉信息，规划好的子任务可能会偏离实际场景，从而导致任务失败。因此，有必要在规划过程中融入**视觉信息**，或者在执行过程中**重新进行规划**。这种方法能够显著提高任务规划的**准确性和可行性**，更好地应对现实世界环境带来的挑战。 
2) **利用来自具身感知模型的视觉信息进行规划**：基于上述讨论，进一步将**视觉信息**整合到任务规划（或重新规划）中尤为重要。在这个过程中，视觉输入所提供的**物体标签**、**位置**或**描述**，能够为大语言模型（LLM）进行**任务分解**和**执行**提供关键参考。通过视觉信息，大语言模型能够更准确地识别当前环境中的目标物体和障碍物，从而优化任务步骤或修改子任务目标。一些研究工作在任务执行过程中使用**物体检测器查询环境中存在的物体**，并将这些信息反馈给大语言模型，使其能够修改当前规划中不合理的步骤[327]，[329]，[330]。**机器人GPT**（RoboGPT）考虑了同一任务中相似物体的不同名称，进一步提高了重新规划的可行性[10]。然而，标签所提供的信息仍然十分有限。能否提供更多的场景信息呢？“**说计划**”（SayPlan）[331] 提出使用**分层三维场景图来表示环境**，有效地缓解了在大型、多层和多房间环境中进行任务规划所面临的挑战。同样地，“**概念图**”（ConceptGraphs）[332] 也采用了三维场景图来提供给大语言模型（LLMs）。与“SayPlan”相比，它提供了更详细的开放世界物体检测功能，并且以基于代码的格式呈现任务规划，这种方式效率更高，也更能满足复杂任务的需求。 
   然而，**有限的视觉信息**可能会导致智能体对其所处环境的理解不足。虽然大语言模型（LLMs）能够获取视觉线索，但它们往往无法捕捉到环境的复杂性和动态变化，从而导致误解和任务失败。例如，如果一条毛巾被锁在浴室的柜子里，智能体可能会反复在浴室中搜索，而没有考虑到这种可能性[10]。为了解决这个问题，必须开发更强大的算法来整合多种感官数据，以增强智能体对环境的理解。此外，即使在视觉信息有限的情况下，利用历史数据和情境推理也可以帮助智能体做出合理的判断和决策。这种多模态融合和基于情境的推理方法，不仅提高了任务执行的成功率，也为具身人工智能的发展提供了新的视角。 

3) **利用视觉语言模型（VLM）进行规划**：与使用外部视觉模型将环境信息转换为文本的方式相比，**视觉语言模型能够在潜在空间中捕捉视觉细节**，尤其是那些难以用物体标签来表示的情境信息。视觉语言模型可以识别视觉现象背后的规律；例如，即使在环境中看不到毛巾，也可以推断出毛巾可能存放在柜子里。这一过程从本质上展示了**抽象的视觉特征和结构化的文本特征如何在潜在空间中更有效地对齐**。在“**具身生成式预训练变换器**（EmbodiedGPT）” [304] 中，**具身变换器**（Embodied-Former）模块对齐**具身信息**、**视觉信息**和**文本信息**，在任务规划过程中有效地考虑了智能体的**状态和环境信息**。与直接使用第三人称视角图像的具身生成式预训练变换器不同，“**低嵌入目标**（LEO）” [333] 将二维以自我为中心的图像和三维场景编码为视觉标记。这种方法能够有效地感知三维世界的信息，并相应地执行任务。同样，“**具身智能基础未知**（EIF-Unknow）”模型利用从体素特征中提取的语义特征图作为视觉标记，这些视觉标记与文本标记一起输入到经过训练的大语言视觉模型（LLaVA）中，用于**任务规划** [334]。此外，在诸如**实时**（RT）系列 [11]、[302]、**基于路径语言模型的具身智能**（PaLM-E）[301] 和**抹茶**（Matcha）[335] 等研究中，具身多模态基础模型，即**视觉语言行为**（VLA）模型，已经在大型数据集上进行了广泛训练，以实现具身场景中视觉特征和文本特征的对齐。 

然而，对于智能体来说**，任务规划只是完成指令任务的第一步**；后续的行动规划决定了任务是否能够完成。在“机器人生成式预训练变换器（RoboGPT）” [10] 的实验中，任务规划的准确率达到了96%，但由于受底层规划器性能的限制，整体任务完成率仅为60%。因此，一个具身智能体能否从“想象任务如何完成”的虚拟空间过渡到“与环境交互并完成任务”的物理世界，关键在于**有效的行动规划**。 

### 具身动作规划
第六部分B讨论了任务规划和行动规划之间的定义及区别。显然，行动规划必须应对现实世界中的不确定性，因为任务规划所提供的子任务的精细程度不足以指导智能体与环境进行交互。一般来说，智能体可以通过两种方式来实现行动规划：1）使用预训练的具身感知和具身干预模型作为工具，通过**应用程序接口**（API）逐步完成任务规划所指定的子任务；2）利用**视觉语言行为**（VLA）模型的固有能力来推导出行动规划。此外，行动规划器的执行结果会反馈给任务规划器，以便对任务规划进行调整和改进。  
1) **利用应用程序编程接口（API）采取行动**：一种典型的方法是向大语言模型（LLMs）提供各种经过良好训练的策略模型的定义和描述作为**背景信息**，使它们能够理解这些工具，并确定如何以及何时为特定任务调用这些工具[299]、[329]。此外，通过生成代码，**可以将一系列更精细的工具抽象成一个函数库以供调用**，而不是将子任务所需的参数直接传递给导航和抓取模型[326]。考虑到环境的不确定性，“**反思**（Reflexion）”方法可以在执行过程中进一步调整这些工具，以实现更好的泛化能力[336]。优化这些工具可以增强智能体的稳健性，并且可能需要新的工具来完成未知任务。“**动态环境中的策略选择**（DEPS）”在**零样本学习**的前提下，赋予大语言模型各种角色设定，以便在与环境交互时学习多样化的技能。在后续的交互过程中，大语言模型可以学会选择和组合这些技能，进而开发出新的技能[337]。 
   这种**分层规划范式**使智能体能够专注于**高层次的任务规划和决策**，同时将具体的**行动执行委托给策略模型**，从而简化了开发过程。任务规划器和行动规划器的**模块化设计**使其能够独立进行开发、测试和优化，增强了系统的灵活性和可维护性。通过调用不同的行动规划器，这种方法使智能体能够适应各种任务和环境，并且便于进行修改，而无需对智能体的结构进行重大改动。然而，调用外部策略模型可能会引入延迟，这有可能影响**响应时间和效率**，尤其是在实时任务中。智能体的性能在很大程度上取决于策略模型的质量。如果策略模型效果不佳，智能体的整体性能将会受到影响。 
2) **利用视觉语言行为（VLA）模型采取行动**：与之前任务规划和行动执行在同一系统内完成的方法不同，这种范式利用具身**多模态基础模型**的能力来进行行动的规划和执行，**减少了通信延迟**，提高了系统的响应速度和效率。在视觉语言行为模型中，感知、决策和执行模块的紧密集成使得系统能够更高效地处理复杂任务，并适应动态环境中的变化。这种集成还便于实现实时反馈，使智能体能够自我调整策略，从而增强了任务执行的**稳健性和适应性**。[3]、[303]、[304]。然而，这种范式无疑**更加复杂且成本更高**，尤其是在处理复杂或长期任务时。此外，一个关键问题是，没有具身世界模型的行动规划器仅依靠大语言模型（LLM）的内部知识，**无法模拟物理规律**。这一限制阻碍了智能体在物理世界中准确有效地完成各种任务，使其无法实现从虚拟空间到物理世界的无缝转换。 

> 从环境和状态到采取环境的过程，需要很多努力。大模型的努力。

## 仿真到现实的适配

[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

具身人工智能中的**从仿真到现实的适配**（Sim-to-Real adaptation）是指将在**模拟环境**（虚拟空间）中学习到的能力或行为迁移到**现实世界**场景（物理世界）中的过程。这包括**验证和提升在模拟环境中开发的算法**、模型以及控制策略的有效性，以确保它们在物理环境中能够稳健且可靠地运行。为了实现从仿真到现实的适配，**具身世界模型**、**数据收集与训练方法**以及**具身控制算法**是三个关键要素。

### 具身世界模型
从仿真到现实的转换涉及在**模拟环境**中创建与现实世界环境极为**相似**的**世界模型**，这有助于算法在迁移到现实环境时能更好地进行**泛化**。世界模型方法旨在构建一个**端到端**的模型，通过以**生成**或**预测**的方式预测**下一个状态来做出决策**，从而将视觉信息映射到行动上，甚至可以实现任意输入到任意输出的映射。这类世界模型与视觉语言行为（VLA）模型之间最大的区别在于，VLA模型首先在大规模的互联网数据集上进行训练，以获得高级的涌现能力，然后再结合现实世界中的机器人数据进行**联合微调**。相比之下，世界模型是在现实世界的数据基础上从零开始训练的，随着数据量的增加逐渐发展出高级能力。然而，它们仍然是低层次的现实世界模型，在某种程度上类似于人类神经反射系统的机制。这使得它们更适合于输入和输出都**相对结构化的场景**，例如**自动驾驶**（输入：视觉信息，输出：油门、刹车、方向盘操作）或**物体分拣**（输入：视觉信息、指令、数字传感器数据，输出：抓取目标物体并放置到目标位置）。它们不太适合泛化应用于非结构化、复杂的具身任务。 

> 所以简单的任务例如工件抓取就可以使用世界模型来构建智能体。

![图15. 具身世界模型大致可分为三种类型。（a）基于生成的方法使用自动编码器框架进行训练，以学习输入空间和输出空间之间的转换关系。（b）基于预测的方法可被视为一种更通用的框架，在这种框架中，世界模型是在潜在空间中进行训练的。（c）知识驱动的方法将人工构建的知识注入到模型中，赋予模型世界知识，从而获得符合给定知识约束的输出。请注意，虚线内的组件是可选的。](<截屏2025-02-28 09.40.39.png>)

**学习世界模型在物理模拟领域颇具前景**。与传统的模拟方法相比，它具有显著的优势，比如能够在信息不完整的情况下对交互作用进行推理，满足**实时计算**的需求，并且随着时间的推移**提高预测的准确性**。这类世界模型的预测能力至关重要，它使机器人能够培养出在人类世界中运行所必需的**物理直觉**。如上图所示，根据对世界环境的学习流程，它们可以分为**基于生成的方法**、**基于预测的方法**以及**知识驱动的方法**。我们在下表中简要总结了上述这些方法。 

![表十一  第七部分A中所讨论的具身世界模型方法概述。](<截屏2025-02-28 10.00.10.png>)

1) **基于生成的方法**：随着模型规模和数据量的逐步增加，生成模型已展现出**理解**和生成符合**物理规律**的图像（例如，“世界模型” [338]）、视频（例如，“空拉” [17]、“潘多拉” [339]）、点云（例如，“三维视觉语言行为模型” [340]）或其他格式数据（例如，“深度世界模型” [341]）的能力。这种能力表明生成模型能够**学习并内化**世界知识。具体而言，在接触大量数据后，生成模型不仅能够捕捉数据的统计特性，还能通过其内在**结构和机制**模拟现实世界的**物理关系和因果关系**。因此，这些生成模型不应仅仅被视为简单的模式识别工具：它们展现出了世界模型的特征。因此，嵌入在生成模型中的世界知识可被用来提升其他模型的性能。通过挖掘和利用生成模型中所表征的世界知识，我们可以提高模型的**泛化能力和稳健性**。这种方法不仅增强了模型对新环境的适应性，还提高了其对未知数据的预测准确性 [339]、[340]。然而，生成模型也存在一定的局限性和缺点。例如，当**数据分布存在明显偏差**或**训练数据不足时**，生成模型可能会产生不准确或失真的输出。此外，这些模型的训练过程通常需要大量的计算资源和时间，并且模型往往**缺乏可解释性**，这使得它们的实际应用变得复杂。总体而言，尽管生成模型在理解和生成符合物理规律的内容方面已显示出巨大潜力，但要实现其有效应用，仍需解决一些技术和实际挑战。这些挑战包括提高模型效率、增强可解释性以及解决与数据偏差相关的问题。随着研究的不断推进和发展，预计生成模型在未来的应用中会展现出更大的价值和潜力。 

2) **基于预测的方法**：基于预测的世界模型通过构建和利用**内部表征**来预测和理解环境。它根据所提供的条件在**潜在空间**中**重构**相应的特征，从而捕捉到更深入的**语义信息**以及相关的世界知识。该模型将输入信息映射到一个**潜在空间**，并在这个空间内进行运算，以提取和利用高级语义信息，进而使机器人能够感知世界环境的本质表征（例如，**图像联合嵌入预测架构**（I-JEPA）[16]、**多模态对比联合嵌入预测架构**（MC-JEPA）[342]、**音频联合嵌入预测架构**（A-JEPA）[343]、**点云联合嵌入预测架构**（Point-JEPA）[354]、**图像世界模型**（IWM）[344]），并更准确地执行具身的下游任务（例如，**图像视频生成式预训练变换器**（iVideoGPT）[345]、**图像机器人模拟**（IRASim）[346]）、**时空规划**（STP）[347]、**多模态梦行者**（MuDreamer）[348]）。与**像素级**信息相比，潜在特征能够对各种形式的知识进行**抽象和解耦**，使模型能够更有效地处理复杂任务和场景，并提高其**泛化能力**[355]。例如，在时空建模中，世界模型需要根据物体的当前状态和交互的性质来预测物体在**交互后的状态**，并将这些信息与它的内部知识相结合。具体来说，具身世界模型通过整合感知信息和先验知识来生成对环境的**动态预测**。这种方法不仅依赖于感官数据，还依赖于内在的世界知识来推断和预测环境变化，从而得出更准确的时空预测[345]、[347]、[348]。这个过程既考虑了物体的当前状态，也考虑了它们的**历史数据**和**情境信息**。 
   同样，利用嵌入在其表征中的世界知识，能够进一步提升模型的**感知能力**和**稳健性**[16]、[342]、[344]、[356]。通过在潜在空间中运行，有望使机器人能够以较低的成本在不同环境中保持高性能[348]。这种方法的关键在于进行**抽象处理和知识解耦**，从而能够高效地适应复杂的情况。然而，当处理以前从**未遇到过**的环境和条件时，这类模型可能会表现出局限性和不稳定性。此外，在潜在空间中解耦出来的世界知识可能存在**可解释性方面**的问题。 
3) **知识驱动的方法**：**知识驱动**的世界模型将人工构建的知识注入到模型中，赋予它们世界知识。这种方法在具身人工智能领域已展现出广泛的应用潜力。例如，在“**真实-模拟-真实**”（real2sim2real）方法[357]中，利用现实世界的知识来构建**符合物理规律的模拟器**，然后用这些模拟器来训练机器人，从而增强了模型的**稳健性和泛化能力**。此外，人工构建常识或符合物理规律的知识，并将其应用于生成模型或模拟器，这是一种常见的策略（例如，“**弹性生成**”（ElastoGen）[350]、“**一到三四五**”（One-2-3-45）[351]、“**物理语言优化与训练**”（PLoT）[349]）。这种方法对模型施加了更符合物理准确性的约束，提高了其在生成任务中的**可靠性和可解释性**。这些约束确保了模型的知识既准确又一致，减少了训练和应用过程中的不确定性。一些方法将人工创建的物理规则与**大语言模型**（LLMs）或**掩码语言模型**（MLMs）相结合。通过利用大语言模型和掩码语言模型的常识能力，这些方法（例如，“**全息甲板**”（Holodeck）[71]、“**联想知识嵌入与导航**”（LEGENT）[352]、“**基于图的具身乌托邦**”（GRUtopia）[353]）通过自动空间布局优化生成多样化且语义丰富的场景。通过在新颖多样的环境中对通用具身智能体进行训练，这极大地推动了通用具身智能体的发展。 

### 数据收集和训练
对于**从仿真到现实的适配**而言，高质量的数据至关重要。传统的数据收集方法需要使用昂贵的设备，操作要求精准，并且既耗时又费力，还常常缺乏灵活性。最近，人们提出了一些高效且经济实惠的方法，用于高质量演示数据的收集和训练。本节将讨论在现实世界和模拟环境中进行数据收集的各种方法。下图展示了来自现实世界和模拟环境的演示数据。 

![图16. 演示数据收集示意图。左侧的黄色方框展示了弗兰克（Franka）机械臂和威多X（WidowX）机械臂的操作演示，而蓝色方框中呈现的是人类操作演示。中间部分，黄色方框展示了优傲UR5e机械臂和弗兰克机械臂在模拟环境中的操作场景，蓝色方框则显示了已标注的模拟数据。右侧的紫色方框展示了这些数据集的数据格式。](<截屏2025-02-28 10.30.26.png>)

1) **现实世界数据**：在大量丰富的数据集上训练大型、高容量的模型，已展现出卓越的能力，并在有效处理下游应用方面取得了显著成功。例如，像ChatGPT、GPT-4和大语言模型Meta AI（LLaMA）等大语言模型，不仅在自然语言处理领域表现出色，还为**下游任务提供了出色的问题解决能力**。因此，有没有可能在机器人领域训练一个具身大模型，使其通过训练具备强大的泛化能力，并能适应新的场景和机器人任务呢？这就需要大量的具身数据集来为模型训练提供数据支持。**Open X-Embodiment** [303] 是一个来自22种不同机器人的具身数据集，涵盖527种技能和160,266项任务。收集到的数据由机器人的真实演示数据组成，是通过记录机器人执行操作的过程获得的。这个数据集主要聚焦于家庭和厨房场景，涉及家具、食品和餐具等物品。操作主要围绕**抓取和放置**任务，其中一小部分涉及更复杂的操作动作。在这个数据集上训练的高容量模型**RT-X**展现出了出色的迁移能力。**UMI** [358] 提出了一种数据收集和策略学习框架。他们设计了一款手持夹爪和一个精美的数据收集界面，能够以便携、低成本且信息丰富的方式收集具有挑战性的双手协作和动态演示数据。通过简单修改训练数据，机器人就能实现零样本可泛化的**双手协作**精确任务。**Mobile ALOHA** [359] 是一种低成本的全身移动操作系统。它可用于收集在全身可移动情况下的双手协作操作任务数据，比如炒虾仁和上菜等。使用该系统以及静态ALOHA所收集的数据来训练智能体，可以提高移动操作任务的性能。这样的智能体可充当家庭助手或工作助手。在**人机协作** [360] 中，人类和智能体在数据收集过程中共同学习，减轻了人类的工作量，加快了数据获取速度，并提高了数据质量。具体而言，在具身场景中，在数据收集期间，人类提供初始动作输入。随后，智能体通过迭代扰动和去噪过程对这些动作进行优化，逐步完善它们，从而生成精确、高质量的操作演示。整个过程可以总结如下：**人类在操作中贡献直觉和多样性，而智能体负责处理优化和稳定性方面的问题**，减少了对操作人员的依赖，使得能够执行**更复杂**的任务，并收集到**更高质量**的数据。 
2) **模拟数据**：上述的数据收集方法涉及在现实世界中直接收集演示数据以用于智能体的训练。这类收集方法通常需要耗费大量的人力、物力和时间，导致效率低下。因此，在大多数情况下，研究人员可以选择在模拟环境中收集数据集用于模型训练。在模拟环境中收集数据不需要大量的资源，并且通常可以通过程序实现自动化，从而节省大量时间。**CLIPORT** [294] 和**传送器网络**（Transporter Networks）[361] 从**Pybullet模拟器**中收集演示数据，用于端到端的网络模型训练，并且成功地将模型从模拟环境迁移到了现实世界。**GAPartNet** [362] 构建了一个大规模的以部件为中心的交互式数据集GAPartNet，为感知和交互任务提供了丰富的部件级注释。他们提出了一种用于领域通用化的三维部件分割和姿态估计的流程，该流程在模拟器和现实世界中对于从未见过的物体类别都能有很好的泛化能力。**语义抓取**（SemGrasp）[289] 构建了一个大规模的抓取与文本对齐的数据集CapGrasp，这是一个来自虚拟环境的语义丰富的灵巧手抓取数据集。
3) **从仿真到现实的范式**：最近，人们引入了几种从仿真到现实的范式，旨在通过在仿真环境中进行大量学习，然后迁移到现实世界场景中，以此来减少对大量且昂贵的现实世界演示数据的需求。本节概述了五种从仿真到现实的迁移范式，如下图所示。 

![图17. 实现从仿真到现实差距弥合的五种流程。“真实-仿真-真实（Real2Sim2Real）”方法通过重建真实场景来缩小差距。“基于转移的交互校正（TRANSIC）”通过人为校正的干预措施来弥补从仿真到现实的迁移差距。“域随机化（Domain Randomization）”通过模拟环境的多样性来增强模型的迁移适应性。“系统辨识（System Identification）”提高了仿真环境与现实环境的相似度，从而减轻了仿真与现实之间的差距。“用于从仿真到现实的语言方法（Lang4Sim2Real）”使用自然语言来连接两个领域，学习不变的图像表征，并减少视觉方面的差距。](<截屏2025-02-28 10.30.41.png>)

- “**真实-仿真-真实**”（Real2Sim2real）[363] 通过利用在 “**数字孪生**” 仿真环境中训练的**强化学习**（RL），增强了在现实世界场景中的模仿学习能力。该方法包括在仿真环境中通过大量的强化学习来强化策略，然后将这些策略迁移到现实世界中，以解决数据稀缺问题，并实现有效的机器人操作模仿学习。首先，使用**神经辐射场**（NeRF）**和虚拟现实**（VR）技术进行场景扫描和重建，将构建好的场景资源导入到模拟器中，以实现从真实到仿真的高保真度。随后，在仿真环境中的强化学习对从现实世界中收集的**稀疏专家演示**中得出的初始策略进行**微调**。最后，将经过优化的策略迁移到现实世界场景中。 
- “**基于转移的交互校正**”（TRANSIC）[364] 通过实现实时的人为干预来纠正现实世界场景中机器人的行为，从而缩小了从仿真到现实之间的差距。它通过以下几个步骤来提升从仿真到现实的**迁移性能**：首先，使用强化学习对机器人进行训练，以便在仿真环境中建立基础策略。然后，将这些策略应用于真实的机器人上，**当出现错误时，人类会通过远程控制进行实时干预并纠正机器人的行为**。从这些干预措施中收集到的数据将用于训练一个**残差策略**。将基础策略和残差策略相结合，确保了在从仿真到现实的迁移之后，机器人在现实世界应用中的运行轨迹更加平稳。这种方法显著减少了对现实世界数据收集的需求，从而减轻了负担，同时实现了有效的从仿真到现实的迁移。 
- **域随机化** [365]–[367] 通过在仿真过程中引入**参数随机化**，增强了在仿真环境中训练的模型对现实世界场景的**泛化能力**。虽然仿真环境和现实环境都涉及通过相机获取的视觉图像进行感知，但诸如物体摩擦力和光泽度等方面的差异使得精确仿真具有挑战性。在仿真训练期间对参数进行随机化处理涵盖了广泛的条件，有可能包含在现实世界场景中可能出现的**各种变化**情况。这种方法提高了训练模型的**稳健性**，使其能够从仿真环境部署到现实环境中。 
- **系统辨识** [368], [369] 构建了一个关于现实世界环境中物理场景的精确**数学模型**，其中涵盖了诸如动力学和视觉渲染等参数。其目的是使仿真环境尽可能贴近现实世界场景，从而有助于将在仿真环境中训练的模型顺利迁移到现实环境中。 
- “**用于从仿真到现实的语言方法**”（Lang4sim2real）[370] 使用**自然语言作为桥梁**，通过将图像的文本描述作为**跨域统一信号**来解决从仿真到现实的差距问题。这种方法有助于学习域不变的图像表征，从而提高在仿真环境和现实环境之间的**泛化性能**。首先，在标注有跨域语言描述的图像数据上对**编码器**进行预训练。随后，利用域不变的表征，训练一种多领域、多任务且以语言为条件的**行为克隆策略**。该方法通过利用丰富的仿真数据中的额外信息来弥补现实世界数据的不足，从而增强了从仿真到现实的迁移能力。 

> 用于从虚拟到现实训练智能体的数据。


### 具身控制
具身控制通过与环境的交互来学习，并利用**奖励机制**来优化行为以获得**最优策略**，从而避免传统物理建模方法的弊端。具身控制方法可以分为两类： 
1) **深度强化学习**（DRL）。深度强化学习能够处理**高维**数据，并学习复杂的行为模式，使其适用于决策制定和控制。针对双足行走，人们提出了**混合动态策略梯度算法**（HDPG）[371]，它能让控制策略通过多种标准同时进行动态优化。**深度步态算法**（DeepGait）[372] 是一种用于地形感知行走的神经网络策略，它结合了基于模型的**运动规划方法**和**强化学习方**法。它包含一个地形感知规划器，用于生成**步态序**列和引导机器人朝着目标方向移动的基座运动，同时还有一个步态和基座运动控制器，用于在保持平衡的同时执行这些序列。**规划器和控制器都使用神经网络函数近似器进行参数化**，并使用**深度强化学习算法进行优化**。 
2) **模仿学习**。深度强化学习存在着需要从大量试验中获取大量数据的缺点。为了解决这一问题，模仿学习应运而生，其目的是通过收集**高质量的演示数据**来尽量**减少数据的使用量**。为了提高数据效率，有人提出了 “**离线强化学习+在线强化学习**” 的方法，以降低交互成本并确保安全性。这种方法首先利用离线强化学习，从**预先收集的静态大型数据集**中学习策略。然后，将这些策略部署到现实环境中进行实时交互和探索，并根据反馈进行调整。从人类演示中衍生出的具有代表性的模仿学习方法有**ALOHA** [373] 和**Mobile ALOHA** [359] 。 

尽管具身人工智能涵盖了**高级算法、模型和规划模块**，但其最基础且至关重要的组成部分是**具身控制**。因此，考虑**如何控制物理实体并赋予它们物理智能**是势在必行的。具身控制与硬件密切相关，比如控制**关节运动**、**末端执行器的位置**以及**行走速度**等。
- 对于**机械臂**而言，在知道末端执行器位置的情况下，要如何规划关节轨迹，从而将机械臂移动到目标位置呢？
- 对于**人形机器人**来说，在知晓其运动模式的情况下，又要如何实现目标姿态呢？

这些都是在控制过程中需要解决的关键问题。有几项研究专注于机器人控制，以提高机器人动作的灵活性。[374] 提出了一种**基于视觉的全身控制框架**。通过将**机械臂和机器狗**连接起来，利用所有的自由度（腿部12个关节、机械臂6个关节以及夹爪1个关节），它能够跟踪机器狗的速度和机械臂末端执行器的位置，从而实现更灵活的控制。一些研究 [375], [376] 采用传统方法来控制双足机器人的行走。麻省理工学院的**猎豹3型机器人** [377]、**ANYmal机器人** [378] 以及**阿特拉斯**（Atlas）机器人 [379] 都使用了强大的行走控制器来操控机器人。这些机器人可用于执行更敏捷的运动任务，比如跳跃或者跨越各种障碍物 [380]–[384]。其他一些研究 [385], [386] 则专注于人形机器人的控制，使其能够像人类一样执行各种动作并**模仿**人类行为。下图展示了一些示例。 

![图18. 各种移动模式的具身控制示例，展示了机器人的敏捷移动和交互能力。左侧的橙色方框展示了四足机器人（宇树B1、ANYmal、Spot、幽灵机器人Vision-60、麻省理工学院猎豹机器人）以及一台双足机器人（ATRIAS）的移动情况。中间的红色方框展示了阿特拉斯机器人爬楼梯、远程操作机器人TELESAR V写字、宇树H1机器人击掌以及波士顿机器人跳跃的动作场景。绿色方框展示了多机器人协作控制，包括寻找路径和规划路径。 ](<截屏2025-03-01 20.09.10.png>)

具身控制整合了**强化学习**（RL）和从**仿真到现实**的技术，通过与环境的交互来优化策略，从而能够探索未知领域，有可能**超越人类**的能力，并适应非结构化的环境。虽然机器人可以模仿许多人类行为，但要有效地完成任务，往往需要基于环境反馈进行**强化学习训练**。最具挑战性的场景包括需要大量接触的任务，在这类任务中，操作需要根据被操作**物体的状态**、**变形情况**、**材料特性**以及**受力情况**等反馈进行实时调整。在这种情况下，强化学习是必不可少的。在大型语言模型（MLMs）时代，这些模型对场景语义具有一般性的理解，为强化学习提供了可靠的**奖励函数**。此外，强化学习对于使大型模型与预期任务相匹配至关重要。在未来，经过**预训练**和**微调**之后，仍然需要强化学习来让模型与物理世界相契合，以确保其在现实世界环境中能够有效地部署。

### 集所有功能于一身的机器人
在数据层面存在的重大局限性，依然阻碍着强大的通用具身智能体的发展，尤其是在数据的标准化格式、多样性以及数据量方面。针对特定任务的数据集，不足以用于训练这些多功能的智能体。尽管像**Open X-embodiment**这样的预训练数据集看似具有统一的结构，但仍有几个关键问题尚未解决。这些问题包括
- **缺乏全面的传感模态**——目前没有任何数据集能同时整合图像、三维视觉信息、文本、触觉以及听觉输入信息。
- 此外，**多机器人数据集缺乏统一的格式，**这使得数据处理和加载变得复杂。
- 再者，在不同的机器人平台上，**对各种不同控制对象的表示方式存在不兼容的情况**。
- **数据量不足**也阻碍了大规模的预训练；
- 而且**同时包含模拟数据和真实数据的数据集十分稀缺**，而这类数据集对于解决从仿真到现实之间的差距问题至关重要。 

为了克服这些挑战，“**集所有功能于一身的机器人**”（ARIO，All Robots In One）[387] 应运而生。它是一种**新的数据集标准**，能够优化现有数据集，并助力开发出更具多功能性和通用性的具身人工智能体。ARIO以**统一的格式**记录来自不同形态机器人的控制和运动数据。通过处理机器人动作频率和传感器帧率的变化，**时间戳机制**使数据收集实现了标准化。ARIO的统一格式能够容纳来自各种不同类型机器人的可变数据，并确保精确的时间戳。这一标准使用户能够**高效地训练出高性能、可泛化的具身人工智能模型**，也让ARIO成为了具身人工智能数据集的理想格式。  
在ARIO标准的基础上，进一步开发了一个统一的大规模ARIO数据集，该数据集包含从**258个**系列和**321,064项**任务中收集的约**300万个**情节数据。这个数据集是通过多方面的方法整合而成的：
1. **从定制平台收集真实世界的数据**，涵盖了105多项任务，共产生了3662个情节数据；
2. **利用诸如Habitat、MuJoCo和SeaWave等平台生成基于模拟的数据**，来自1198项任务的情节数据共计703,088个；
3. **将现有的开源数据集转换为ARIO标准格式**，来自319,761项任务的情节数据共计2,326,438个。ARIO数据集解决了当前数据集的局限性，并有助于开发强大的、通用的具身智能体。

通过为数据收集和表示提供一个连贯的框架，**ARIO**为开发更强大、更多功能的具身人工智能体铺平了道路，这些智能体能够以日益复杂和多样的方式在物理世界中导航并与之交互。该项目可在 https://imaei.github.io/project_pages/ario 上获取。 

![图19. 集所有功能于一体的机器人。该图参考自文献[387]的研究内容。ARIO标准和数据集的主要优势可总结如下：1）全面的传感模态：ARIO融合了五种传感模态——图像、三维数据、声音、文本以及触觉信息，提供了更丰富、更多样化的数据集；2）基于时间戳的数据对齐方式：数据是基于时间戳进行记录和同步的，能够适应不同传感器的帧率；3）结构化的系列-任务-情节框架：ARIO采用了清晰的系列-任务-情节结构，对每个系列和任务都有详细的文本描述；4）统一的数据格式：该数据集遵循标准化格式，支持各种类型的机器人和控制对象，简化了数据处理过程；5）模拟数据与真实机器人数据的整合：ARIO包含了多个任务、场景以及不同类型机器人的模拟数据和真实世界数据，提高了在不同硬件平台上的泛化能力。 ](<截屏2025-03-01 20.12.24.png>)

## 挑战和未来方向

[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

---

尽管具身人工智能取得了迅速的进展，但它仍面临着一些挑战，同时也展现出了令人振奋的未来发展方向。 
**高质量的机器人数据集**：获取足够的真实世界机器人数据仍然是一个重大挑战。收集这类数据既耗时又耗费资源。仅仅依赖模拟数据会加剧从模拟到现实的差距问题。创建多样化的真实世界机器人数据集需要各个机构之间密切且广泛的合作。此外，开发更逼真、更高效的模拟器对于**提高模拟数据的质量**至关重要。目前的研究成果**RT-1** [11] 使用了基于机器人图像和自然语言指令的预训练模型。RT-1在导航和抓取任务中取得了不错的成果，但获取真实世界的机器人数据集极具挑战性。对于构建能够在机器人领域中进行跨场景和跨任务应用的可泛化具身模型而言，构建大规模数据集至关重要，可利用高质量的模拟环境数据来辅助真实世界的数据。 
**人类演示数据的高效利用**：高效利用人类演示数据意味着借助人类所展示的动作和行为来训练和改进机器人系统。这个过程包括从大规模、高质量的数据集中进行收集、处理和学习，在这些数据集中，人类执行着机器人旨在学习的任务。目前的研究成果**R3M** [388] 利用动作标签和人类演示数据来学习具有高成功率的**可泛化表征**，但在处理复杂任务时的效率仍有待提高。因此，有效地利用大量非结构化、多标签且多模态的人类演示数据，并结合动作标签数据，来训练具身模型，使其能够在相对较短的时间内学习各种任务，这一点至关重要。通过高效利用人类演示数据，机器人系统能够达到更高的性能水平和适应能力，从而使其更有能力在动态环境中执行复杂任务。 
**复杂环境的认知**：复杂环境的认知是指具身智能体在物理或虚拟环境中，**感知、理解**并在复杂的现实世界环境中**导航**的能力。基于广泛的常识知识，“**会说能做**”模型（Say-Can）[299] 利用了预训练的大语言模型（LLM）的**任务分解机制**，该机制在进行简单任务规划时严重依赖大量的常识知识，但在复杂环境中对长期任务**缺乏理解**。对于非结构化的开放环境，当前的研究工作通常依赖预训练的大语言模型的任务分解机制，运用广泛的常识知识进行简单的任务规划，却缺乏对特定场景的理解。增强在复杂环境中的**知识迁移和泛化能力**至关重要。一个真正通用的机器人系统应该能够在各种不同且未曾见过的场景中理解并执行自然语言指令。这就需要开发出具有适应性和可扩展性的具身智能体架构。 
**长期任务执行**：对于机器人来说，执行单个指令往往可能涉及到**长期任务**，比如“打扫厨房”这样的指令，它涵盖了重新摆放物品、扫地、擦桌子等一系列活动。要成功完成这类任务，机器人需要具备在**较长时间内规划并执行一系列低层次动作的能力**。尽管目前的高级任务规划器已取得了初步成果，但由于它们没有针对具身任务进行优化调整，在各种不同场景中往往显得力不从心。要应对这一挑战，就需要开发出具备强大感知能力和丰富常识知识的**高效规划器**。 
**因果关系发现**：现有的数据驱动型具身智能体是基于**数据内部的内在相关性**来做出决策的。然而，这种建模方法无法让模型真正理解知识、行为和环境之间的因果关系，从而导致策略出现偏差。这使得难以确保它们能够以可解释、稳健且可靠的方式在现实世界环境中运行。因此，对于具身智能体而言，由世界知识驱动并具备**自主因果推理能力**是很重要的。通过交互来理解世界，并通过溯因推理来学习世界的运行机制，我们可以进一步增强多模态具身智能体在复杂现实世界环境中的适应性、决策可靠性和泛化能力。对于具身任务，有必要通过交互指令和状态预测来跨模态建立**时空因果关系**[389]。此外，智能体需要理解物体的可供性，以便在动态场景中实现自适应任务规划和长距离自主导航。为了优化决策，有必要结合反事实和因果干预策略[390]，从**反事实**和**因果干预**的角度追溯因果关系，减少探索迭代次数，并优化决策。基于世界知识构建因果图，并通过主动的因果推理推动智能体从模拟到现实的迁移，这将形成一个具身人工智能的统一框架。 
**持续学习**：在机器人应用领域中，持续学习[391]对于在不同环境中部署机器人学习策略至关重要，但这在很大程度上仍是一个尚未充分探索的领域。尽管最近的一些研究探讨了持续学习的子课题，比如**增量学习**、**快速运动适应**以及**人在回路学习**等，但这些解决方案通常是为单一任务或平台而设计的，且尚未考虑基础模型。有待研究的开放性问题和可行方法包括：
- 1）在对最新数据进行微调时，**混合不同比例的先验数据分布**，以缓解灾难性遗忘问题[392]；
- 2）从**先验分布或课程中**开发出高效的原型，以便在学习新任务时进行任务推断；
- 3）提高在线学习算法的训练**稳定性和样本效率**；
- 4）确定**将大容量模型无缝融入控制框架的原则性方法**，有可能通过分层学习或快慢控制的方式来实现实时推断。 
**统一评估基准**：尽管目前存在众多用于评估低层次控制策略的基准测试，但它们在所评估的技能方面往往差异很大。此外，这些基准测试中包含的对象和场景通常受到模拟器的限制。为了**全面评估具身模型**，需要有使用逼真模拟器、涵盖各种不同技能的基准测试。对于高层次任务规划器而言，许多基准测试侧重于通过问答任务来评估规划能力。然而，**更理想的方法**是将高层次任务规划器和低层次控制策略结合起来进行评估，以执行长期任务并衡量成功率，而不是仅仅依赖于对规划器的单独评估。这种综合的评估方法能够更全面地评估具身人工智能系统的能力。 

## 总结
[⬆️](#aligning-cyber-space-with-physical-world-a-comprehensive-survey-on-embodied-ai)

具身人工智能使智能体能够感知、察觉并与来自网络空间和物理世界的各种物体进行交互，这彰显了其对于实现通用人工智能（AGI）的重要意义。本综述全面回顾了具身机器人、模拟器、四项具有代表性的具身任务：视觉主动感知、具身交互、具身智能体以及从仿真到现实的机器人控制，同时还探讨了未来的研究方向。对具身机器人、模拟器、数据集以及相关方法的对比总结，清晰地呈现了具身人工智能领域的近期发展状况，这对沿着这一新兴且前景广阔的研究方向开展未来研究大有裨益。 

